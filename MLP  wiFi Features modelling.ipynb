{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominant-species",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "liable-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# DL library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from  torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# metrics calculation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# basic plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# interactive plots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import iplot\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-solution",
   "metadata": {},
   "source": [
    "## Config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "green-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # pipeline parameters\n",
    "    SEED        = 42\n",
    "    TRAIN       = True\n",
    "    LR_FIND     = False\n",
    "    GENERATE_OOF= True\n",
    "    N_FOLDS     = 5 \n",
    "    N_EPOCHS    = 2\n",
    "    TEST_BATCH_SIZE  = 32\n",
    "    TRAIN_BATCH_SIZE = 16\n",
    "    NUM_WORKERS      = 4\n",
    "    DATA_FRAC        = 1.0\n",
    "    FOLD_TO_TRAIN    = [0, 1, 2, 3, 4] # \n",
    "\n",
    "    # model parameters\n",
    "    MODEL_ARCH  = 'MLP'\n",
    "    MODEL_NAME  = 'mlp_vtest'\n",
    "    WGT_PATH    = ''\n",
    "    WGT_MODEL   = ''\n",
    "    PRINT_N_EPOCH = 2\n",
    "    \n",
    "    # scheduler variables\n",
    "    MAX_LR    = 1e-2\n",
    "    MIN_LR    = 1e-5\n",
    "    SCHEDULER = 'CosineAnnealingWarmRestarts'  # ['ReduceLROnPlateau', 'None', OneCycleLR', ','CosineAnnealingLR']\n",
    "    T_0       = 10     # CosineAnnealingWarmRestarts\n",
    "    T_MULT    = 2      # CosineAnnealingWarmRestarts\n",
    "    T_MAX     = 5      # CosineAnnealingLR\n",
    "\n",
    "    # optimizer variables\n",
    "    OPTIMIZER     = 'Adam'\n",
    "    WEIGHT_DECAY  = 1e-6\n",
    "    GRD_ACC_STEPS = 1\n",
    "    MAX_GRD_NORM  = 1000\n",
    "    \n",
    "    # features parameters\n",
    "    USE_FREQ_FEATS = True\n",
    "    BUILDING_SITES_RANGE = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "timely-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_map = {\"B2\": -2, \"B1\": -1, \"F1\": 0, \"F2\": 1, \"F3\": 2, \"F4\": 3, \"F5\": 4, \"F6\": 5, \"F7\": 6, \"F8\": 7, \"F9\": 8,\n",
    "             \"1F\": 0, \"2F\": 1, \"3F\": 2, \"4F\": 3, \"5F\": 4, \"6F\": 5, \"7F\": 6, \"8F\": 7, \"9F\": 8}\n",
    "\n",
    "minCount = 1\n",
    "rssiFillerValue = -999.0\n",
    "dtFillerValue   = 1000.0\n",
    "freqFillerValue = 0\n",
    "featuresInputDir = 'referencePublicNotebooks/wiFiFeatures'\n",
    "modelOutputDir = 'modelSaveDir'\n",
    "sampleCsvPath = 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "parental-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buildingsList = sorted(glob.glob(f\"{outputDir}/*.csv\"))\n",
    "#buildingsList = buildingsList[CFG.BUILDING_SITES_RANGE[0]: CFG.BUILDING_SITES_RANGE[1]]\n",
    "#print(buildingsList[0].split('/')[-1])\n",
    "buildingsList = 'referencePublicNotebooks/wiFiFeatures/5a0546857ecc773753327266_npyTrain.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "metric-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingData = pd.read_csv(buildingsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-market",
   "metadata": {
    "papermill": {
     "duration": 0.030885,
     "end_time": "2021-02-15T09:18:40.779692",
     "exception": false,
     "start_time": "2021-02-15T09:18:40.748807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "identical-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBuildingName(buildingCsvPath):\n",
    "    fileName = buildingCsvPath.split('/')[-1]\n",
    "    buildingName = fileName.split('_')[0]\n",
    "    return buildingName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-theme",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-15T09:18:40.849777Z",
     "iopub.status.busy": "2021-02-15T09:18:40.847758Z",
     "iopub.status.idle": "2021-02-15T09:18:40.850551Z",
     "shell.execute_reply": "2021-02-15T09:18:40.851077Z"
    },
    "papermill": {
     "duration": 0.040621,
     "end_time": "2021-02-15T09:18:40.851226",
     "exception": false,
     "start_time": "2021-02-15T09:18:40.810605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_no_of_trainable_params(model):\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-shower",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-15T09:18:40.921617Z",
     "iopub.status.busy": "2021-02-15T09:18:40.920714Z",
     "iopub.status.idle": "2021-02-15T09:18:40.927627Z",
     "shell.execute_reply": "2021-02-15T09:18:40.926924Z"
    },
    "papermill": {
     "duration": 0.045466,
     "end_time": "2021-02-15T09:18:40.927741",
     "exception": false,
     "start_time": "2021-02-15T09:18:40.882275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(CFG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "handled-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBuilding_npyInfo(buildingCsvPath):\n",
    "    # read building data \n",
    "    data = pd.read_csv(buildingCsvPath)\n",
    "    buildingName = getBuildingName(buildingCsvPath)\n",
    "    #print(buildingName)\n",
    "        \n",
    "    # use fraction if needed\n",
    "    if CFG.DATA_FRAC < 1:\n",
    "        data = data.sample(frac=CFG.DATA_FRAC).reset_index(drop=True)\n",
    "\n",
    "    # first column is timestamp\n",
    "    timestamps = data.iloc[:,0].values   # np.expand_dims( , ,axis=1)\n",
    "    \n",
    "    # last column is pathFile name\n",
    "    groups = data.iloc[:,-1].values\n",
    "    \n",
    "    # target values are last but 3 columns\n",
    "    y = data.iloc[:,-4:-1].values\n",
    "    \n",
    "    data['filePath'] = buildingName + '_' + \\\n",
    "                       data.iloc[:,-1].astype(str) + '_' + \\\n",
    "                       data.iloc[:,0].astype(str) + '.npy'\n",
    "    X = data['filePath'].values\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return timestamps,X,y,groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBuildingData(buildingCsvPath):\n",
    "    # read building data \n",
    "    data = pd.read_csv(buildingCsvPath)\n",
    "    \n",
    "    # use fraction if needed\n",
    "    if CFG.DATA_FRAC < 1:\n",
    "        data = data.sample(frac=CFG.DATA_FRAC).reset_index(drop=True)\n",
    "\n",
    "    # first column is timestamp\n",
    "    timestamps = data.iloc[:,0].values   # np.expand_dims( , ,axis=1)\n",
    "    \n",
    "    # last column is pathFile name\n",
    "    groups = data.iloc[:,-1].values\n",
    "    \n",
    "    # target values are last but 3 columns\n",
    "    y = data.iloc[:,-4:-1].values\n",
    "    \n",
    "    # use all features\n",
    "    if CFG.USE_FREQ_FEATS == True:\n",
    "        X = data.iloc[:,1:-4].values    \n",
    "\n",
    "    else:\n",
    "        numWiFiIds = int((data.shape[1] - 5) / 3)\n",
    "        # separate into features and target variables\n",
    "        X = data.iloc[:,1:(2*numWiFiIds)+1].values    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Incase freq signal is not needed, use rssi and dt features alone    \n",
    "    # There are 5 columns for timestamp, y, pathNames values in csv, reamining are features\n",
    "    # total features = 3 * [rssi, dt, freq]\n",
    "    # hence unique wifi ids = totalFeatures / 3\n",
    "    \"\"\"\n",
    "            \n",
    "    del data\n",
    "    gc.collect()\n",
    "    return timestamps,X,y,groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def competitionMetric(preds, targets):\n",
    "    \"\"\" The metric used in this competition \"\"\"\n",
    "    # position error\n",
    "    meanPosPredictionError = torch.mean(torch.sqrt(\n",
    "                             torch.square(torch.subtract(preds[:,0], targets[:,0])) + \n",
    "                             torch.square(torch.subtract(preds[:,1], targets[:,1]))))\n",
    "    # error in floor prediction\n",
    "    meanFloorPredictionError = torch.mean(15 * torch.abs(preds[:,2] - targets[:,2]))\n",
    "    return meanPosPredictionError, meanFloorPredictionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOptimizer(model : nn.Module):    \n",
    "    if CFG.OPTIMIZER == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), weight_decay=CFG.WEIGHT_DECAY, lr=CFG.MAX_LR)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), weight_decay=CFG.WEIGHT_DECAY, lr=CFG.MAX_LR, momentum=0.9)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScheduler(optimizer, dataloader_train):\n",
    "    if CFG.SCHEDULER == 'OneCycleLR':\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr= CFG.MAX_LR, epochs = CFG.N_EPOCHS, \n",
    "                          steps_per_epoch = len(dataloader_train), pct_start=0.25, div_factor=10, anneal_strategy='cos')\n",
    "    elif CFG.SCHEDULER == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=CFG.T_MULT, eta_min=CFG.MIN_LR, last_epoch=-1)\n",
    "    elif CFG.SCHEDULER == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_MAX * len(dataloader_train), eta_min=CFG.MIN_LR, last_epoch=-1)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFoldDataLoaders(timestamps,X,y,groups,trainIndex,validIndex):\n",
    "    \n",
    "    # splitting into train and validataion sets\n",
    "    trainTimeStamps, X_train, y_train, trainGroups = timestamps[trainIndex], X[trainIndex], y[trainIndex], groups[trainIndex]\n",
    "    validTimeStamps, X_valid, y_valid, validGroups = timestamps[validIndex], X[validIndex], y[validIndex], groups[validIndex] \n",
    "    \n",
    "    # normalize input            \n",
    "    #print(f\"Before stdscaler : train_mean{X_train.mean(), X_train.std(), X_valid.mean(), X_valid.std()}\")\n",
    "    X_train = stdScaler.fit_transform(X_train)\n",
    "    X_valid = stdScaler.transform(X_valid)\n",
    "    #print(f\"After stdscaler : train_mean{X_train.mean(), X_train.std(), X_valid.mean(), X_valid.std()}\")\n",
    "    #print(f\"x,y shapes = {X_train.shape, y_train.shape, X_valid.shape, y_valid.shape}\")\n",
    "                        \n",
    "    # create torch Datasets and Dataloader for each fold's train and validation data\n",
    "    dataset_train = wiFiFeaturesDataset(trainTimeStamps, X_train, y_train, trainGroups)\n",
    "    dataset_valid = wiFiFeaturesDataset(validTimeStamps, X_valid, y_valid, validGroups)            \n",
    "    dataloader_train = DataLoader(dataset_train, batch_size= CFG.TRAIN_BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=CFG.NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "    dataloader_valid = DataLoader(dataset_valid, batch_size= CFG.TEST_BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=CFG.NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "    return dataloader_train, dataloader_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingResults(resultsDf):\n",
    "    # subplot to plot\n",
    "    fig = make_subplots(rows=1, cols=1)\n",
    "    colors = [ ('#d32f2f', '#ef5350'), ('#303f9f', '#5c6bc0'), ('#00796b', '#26a69a'),\n",
    "                ('#fbc02d', '#ffeb3b'), ('#5d4037', '#8d6e63')]\n",
    "\n",
    "    # find number of folds input df\n",
    "    numberOfFolds = resultsDf['fold'].nunique()\n",
    "    \n",
    "    # iterate through folds and plot\n",
    "    for i in range(numberOfFolds):\n",
    "        data = resultsDf[resultsDf['fold'] == i]\n",
    "        fig.add_trace(go.Scatter(x=data['epoch'].values, y=data['trainPosLoss'].values,\n",
    "                                mode='lines', visible='legendonly' if i > 0 else True,\n",
    "                                line=dict(color=colors[i][0], width=2),\n",
    "                                name='trainPossLoss -Fold{}'.format(i)),row=1, col=1)\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=data['epoch'], y=data['valPosLoss'].values,\n",
    "                                 mode='lines+markers', visible='legendonly' if i > 0 else True,\n",
    "                                 line=dict(color=colors[i][1], width=2),\n",
    "                                 name='valPosLoss -Fold{}'.format(i)),row=1, col=1)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-patrick",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wiFiFeaturesDataset(Dataset):\n",
    "    def __init__(self, timeStamps, X_data, y_data, groups):\n",
    "        self.timeStamps = timeStamps \n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.groups = groups\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x  = torch.from_numpy(self.X_data[index].astype(np.float32))\n",
    "        y  = torch.from_numpy(self.y_data[index].astype(np.float32))\n",
    "        ts = self.timeStamps[index].astype(np.int64)\n",
    "        group = self.groups[index]\n",
    "        return ts,x,y,group\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "class npyWiFiFeaturesDataset(Dataset):\n",
    "    def __init__(self, timeStamps, X_data, y_data, groups):\n",
    "        self.timeStamps = timeStamps \n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.groups = groups\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ts = self.timeStamps[index].astype(np.int64)\n",
    "        x = torch.from_numpy(np.load(self.X_data[index]))\n",
    "        y  = torch.from_numpy(self.y_data[index].astype(np.float32))\n",
    "        group = self.groups[index]\n",
    "        return ts,x,y,group\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-marriage",
   "metadata": {},
   "source": [
    "## MLP Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wiFiFeaturesMLPModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_features=n_input, out_features=512)\n",
    "        self.lin2 = nn.Linear(in_features=512,     out_features=32)\n",
    "        self.lin3 = nn.Linear(in_features=32,      out_features=n_output)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.drops = nn.Dropout(0.3)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-algeria",
   "metadata": {},
   "source": [
    "## Compute Device as CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Device as cpu or tpu\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-liver",
   "metadata": {},
   "source": [
    "## Preprocessing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cv\n",
    "folds = GroupKFold(n_splits=CFG.N_FOLDS)\n",
    "\n",
    "# for normalizing input data\n",
    "stdScaler = StandardScaler()\n",
    "\n",
    "# scaler to handle AMP\n",
    "scaler = GradScaler()   \n",
    "\n",
    "criterion = competitionMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-relaxation",
   "metadata": {},
   "source": [
    "## Lr range finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-trustee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-15T09:18:43.043751Z",
     "iopub.status.busy": "2021-02-15T09:18:43.041644Z",
     "iopub.status.idle": "2021-02-15T09:18:43.044555Z",
     "shell.execute_reply": "2021-02-15T09:18:43.045101Z"
    },
    "papermill": {
     "duration": 0.044791,
     "end_time": "2021-02-15T09:18:43.045246",
     "exception": false,
     "start_time": "2021-02-15T09:18:43.000455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_lr_finder_results(lr_finder): \n",
    "    # Create subplot grid\n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    # layout ={'title': 'Lr_finder_result'}\n",
    "    \n",
    "    # Create a line (trace) for the lr vs loss, gradient of loss\n",
    "    trace0 = go.Scatter(x=lr_finder['log_lr'], y=lr_finder['smooth_loss'],name='log_lr vs smooth_loss')\n",
    "    trace1 = go.Scatter(x=lr_finder['log_lr'], y=lr_finder['grad_loss'],name='log_lr vs loss gradient')\n",
    "\n",
    "    # Add subplot trace & assign to each grid\n",
    "    fig.add_trace(trace0, row=1, col=1);\n",
    "    fig.add_trace(trace1, row=1, col=2);\n",
    "    iplot(fig, show_link=False)\n",
    "    #fig.write_html(CFG.MODEL_NAME + '_lr_find.html');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-fiber",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-15T09:18:43.133881Z",
     "iopub.status.busy": "2021-02-15T09:18:43.132873Z",
     "iopub.status.idle": "2021-02-15T09:18:43.136469Z",
     "shell.execute_reply": "2021-02-15T09:18:43.135878Z"
    },
    "papermill": {
     "duration": 0.059229,
     "end_time": "2021-02-15T09:18:43.136586",
     "exception": false,
     "start_time": "2021-02-15T09:18:43.077357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_lr(model, optimizer, data_loader, init_value = 1e-8, final_value=100.0, beta = 0.98, num_batches = 200):\n",
    "    assert(num_batches > 0)\n",
    "    mult = (final_value / init_value) ** (1/num_batches)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    batch_num = 0\n",
    "    avg_loss = 0.0\n",
    "    best_loss = 0.0\n",
    "    smooth_losses = []\n",
    "    raw_losses = []\n",
    "    log_lrs = []\n",
    "    dataloader_it = iter(data_loader)\n",
    "    progress_bar = tqdm(range(num_batches))                \n",
    "        \n",
    "    for idx in progress_bar:\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            _, inputs, targets, _ = next(dataloader_it)\n",
    "            #print(images.shape)\n",
    "        except:\n",
    "            dataloader_it = iter(data_loader)\n",
    "            _, inputs, targets, _ = next(dataloader_it)\n",
    "\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # handle exception in criterion\n",
    "        try:\n",
    "            # Forward pass\n",
    "            y_preds = model(inputs)\n",
    "            posLoss, floorLoss = criterion(y_preds, targets)\n",
    "            loss = posLoss + floorLoss\n",
    "        except:\n",
    "            if len(smooth_losses) > 1:\n",
    "                grad_loss = np.gradient(smooth_losses)\n",
    "            else:\n",
    "                grad_loss = 0.0\n",
    "            lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                                 'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "            return lr_finder_results \n",
    "                    \n",
    "        #Compute the smoothed loss\n",
    "        avg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        \n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 50 * best_loss:\n",
    "            if len(smooth_losses) > 1:\n",
    "                grad_loss = np.gradient(smooth_losses)\n",
    "            else:\n",
    "                grad_loss = 0.0\n",
    "            lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                                 'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "            return lr_finder_results\n",
    "        \n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num==1:\n",
    "            best_loss = smoothed_loss\n",
    "        \n",
    "        #Store the values\n",
    "        raw_losses.append(loss.item())\n",
    "        smooth_losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print info\n",
    "        progress_bar.set_description(f\"loss:{loss.item()},smoothLoss: {smoothed_loss},lr:{lr}\")\n",
    "\n",
    "        #Update the lr for the next step\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "    \n",
    "    grad_loss = np.gradient(smooth_losses)\n",
    "    lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                         'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "    return lr_finder_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-humanity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-15T09:18:43.214844Z",
     "iopub.status.busy": "2021-02-15T09:18:43.213286Z",
     "iopub.status.idle": "2021-02-15T09:18:43.217169Z",
     "shell.execute_reply": "2021-02-15T09:18:43.216590Z"
    },
    "papermill": {
     "duration": 0.048229,
     "end_time": "2021-02-15T09:18:43.217286",
     "exception": false,
     "start_time": "2021-02-15T09:18:43.169057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.LR_FIND == True:\n",
    "    # create dataset instance\n",
    "    tempTs, tempX, tempY,_ = getBuildingData(buildingCsvPath=buildingsList[0])\n",
    "    tempX = stdScaler.fit_transform(tempX)\n",
    "    tempTrainDataset = wiFiFeaturesDataset(tempTs, tempX, tempY)\n",
    "    tempTrainDataloader = DataLoader(tempTrainDataset, batch_size= CFG.TRAIN_BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=CFG.NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "    \n",
    "    # create model instance   \n",
    "    model = wiFiFeaturesMLPModel(n_input=tempX.shape[1], n_output=3)\n",
    "    model.to(device);\n",
    "    \n",
    "    # optimizer function, lr schedulers and loss function\n",
    "    optimizer = getOptimizer(model)\n",
    "    lrFinderResults = find_lr(model, optimizer, tempTrainDataloader)\n",
    "    plot_lr_finder_results(lrFinderResults)\n",
    "    del tempX, tempY, tempTrainDataset, tempTrainDataloader, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-snowboard",
   "metadata": {},
   "source": [
    "## Train & Validate helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateModel(model, validationDataloader):\n",
    "    # placeholders to store output\n",
    "    val_ts = []\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    val_groups = []\n",
    "\n",
    "    # set model to Validate mode\n",
    "    model.eval()\n",
    "    dataLoaderIterator = iter(validationDataloader)\n",
    "\n",
    "    for idx in range(len(validationDataloader)):\n",
    "        try:\n",
    "            ts, inputs, targets, valGroups = next(dataLoaderIterator)\n",
    "        except StopIteration:\n",
    "            dataLoaderIterator = iter(validationDataloader)\n",
    "            ts, inputs, targets, valGroups = next(dataLoaderIterator)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device) \n",
    "\n",
    "        # forward prediction\n",
    "        with torch.no_grad():    \n",
    "            y_preds = model(inputs)\n",
    "\n",
    "        # store predictions and targets to compute metrics later\n",
    "        val_ts.append(ts)\n",
    "        val_preds.append(y_preds)\n",
    "        val_targets.append(targets)\n",
    "        val_groups.append(valGroups)\n",
    "\n",
    "    # concatenate to get as 1 2d array and find total loss  \n",
    "    val_preds = torch.cat(val_preds, 0)\n",
    "    val_targets = torch.cat(val_targets, 0)\n",
    "    valPosLoss, valFloorLoss = criterion(val_preds, val_targets)\n",
    "    valScore = valPosLoss #+ valFloorLoss\n",
    "\n",
    "    # np array concatenation\n",
    "    val_ts = np.concatenate(val_ts, axis=0)\n",
    "    val_groups = np.concatenate(val_groups, axis=0)\n",
    "    \n",
    "    # store results\n",
    "    validationResults = {'valPosLoss': valPosLoss.item() , 'valFloorLoss': valFloorLoss.item(),\\\n",
    "                         'val_ts': val_ts, 'val_groups': val_groups,\n",
    "                         'val_preds'  :val_preds.cpu().data.numpy(), \n",
    "                         'val_targets':val_targets.cpu().data.numpy(),\n",
    "                         }\n",
    "    return validationResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainValidateOneFold(buildingName, i_fold, model, optimizer, scheduler, dataloader_train, dataloader_valid):\n",
    "    trainFoldResults = []\n",
    "    bestValScore = np.inf\n",
    "    bestEpoch = 0\n",
    "\n",
    "    for epoch in range(CFG.N_EPOCHS):\n",
    "        #print('Epoch {}/{}'.format(epoch + 1, CFG.N_EPOCHS))\n",
    "        model.train()\n",
    "        trainPosLoss = 0.0\n",
    "        trainFloorLoss = 0.0\n",
    "\n",
    "        # training iterator\n",
    "        tr_iterator = iter(dataloader_train)\n",
    "\n",
    "        for idx in range(len(dataloader_train)):\n",
    "            try:\n",
    "                _, inputs, targets, _ = next(tr_iterator)\n",
    "            except StopIteration:\n",
    "                tr_iterator = iter(dataloader_train)\n",
    "                _, inputs, targets, _ = next(tr_iterator)\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)  \n",
    "\n",
    "            # builtin package to handle automatic mixed precision\n",
    "            with autocast():\n",
    "                # Forward pass\n",
    "                y_preds = model(inputs)   \n",
    "                posLoss, floorLoss = criterion(y_preds, targets)\n",
    "                loss = posLoss # + floorLoss\n",
    "\n",
    "                # Backward pass\n",
    "                scaler.scale(loss).backward()        \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad() \n",
    "\n",
    "                # log the necessary losses\n",
    "                trainPosLoss   += posLoss.item()\n",
    "                trainFloorLoss += floorLoss.item()\n",
    "\n",
    "                if scheduler is not None: \n",
    "                    if CFG.SCHEDULER == 'CosineAnnealingWarmRestarts':\n",
    "                        scheduler.step(epoch + idx / len(dataloader_train)) \n",
    "                    # onecyle lr scheduler / CosineAnnealingLR scheduler\n",
    "                    else:\n",
    "                        scheduler.step()\n",
    "                    \n",
    "        # Validate\n",
    "        foldValidationResults = validateModel(model, dataloader_valid)\n",
    "         \n",
    "        # store results\n",
    "        trainFoldResults.append({ 'fold': i_fold, 'epoch': epoch, \n",
    "                                  'trainPosLoss': trainPosLoss / len(dataloader_train), \n",
    "                                  'trainFloorLoss': trainFloorLoss / len(dataloader_train), \n",
    "                                  'valPosLoss'  : foldValidationResults['valPosLoss'] , \n",
    "                                  'valFloorLoss': foldValidationResults['valFloorLoss']})\n",
    "        \n",
    "        valScore = foldValidationResults['valPosLoss'] # + foldVal['valFloorLoss']\n",
    "        # save best models        \n",
    "        if(valScore < bestValScore):\n",
    "            # reset variables\n",
    "            bestValScore = valScore\n",
    "            bestEpoch = epoch\n",
    "\n",
    "            # save model weights\n",
    "            torch.save({'model': model.state_dict(), 'val_ts' : foldValidationResults['val_ts'], \n",
    "                        'val_preds':foldValidationResults['val_preds'], \n",
    "                        'val_targets':foldValidationResults['val_targets'],\n",
    "                        'val_groups' : foldValidationResults['val_groups']}, \n",
    "                        f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_fold{i_fold}_epoch{epoch}.pth\")\n",
    "\n",
    "    print(f\"For Fold {i_fold}, Best validation score of {bestValScore} was got at epoch {bestEpoch}\") \n",
    "    #temp = pd.DataFrame(trainFoldResults)\n",
    "    #print(temp.shape, list(temp.columns))\n",
    "    return trainFoldResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-identifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainValidateOneBuilding(buildingDataPath):\n",
    "    # placeholder to store results\n",
    "    buildingTrainResults = []\n",
    "    \n",
    "    buildingName = getBuildingName(buildingDataPath)\n",
    "    print(f\"Processing data for building - {buildingName}\")\n",
    "    timestamps, X, y, groups = getBuildingData(buildingCsvPath=buildingDataPath)\n",
    "    print(f\"Building Data shapes : {timestamps.shape, X.shape, y.shape, groups.shape}\")\n",
    "\n",
    "    for i_fold, (trainIndex, validIndex) in enumerate(folds.split(X=X, y=y[:,0],groups=groups)):\n",
    "        if i_fold in CFG.FOLD_TO_TRAIN:\n",
    "            print(\"Fold {}/{}\".format(i_fold + 1, CFG.N_FOLDS))\n",
    "            dataloader_train, dataloader_valid = getFoldDataLoaders(timestamps,X,y,groups,trainIndex,validIndex)\n",
    "\n",
    "            # supervised model instance and move to compute device\n",
    "            model = wiFiFeaturesMLPModel(n_input=X.shape[1], n_output=3)\n",
    "            model.to(device);\n",
    "            # print(f\"there are {find_no_of_trainable_params(model)} params in model\")\n",
    "\n",
    "            # optimizer function, lr schedulers and loss function\n",
    "            optimizer = getOptimizer(model)\n",
    "            scheduler = getScheduler(optimizer, dataloader_train)\n",
    "            # print(f\"optimizer={optimizer}, scheduler={scheduler}, loss_fn={criterion}\")\n",
    "\n",
    "            # train and validate single fold\n",
    "            foldResults = trainValidateOneFold(buildingName, i_fold, model, optimizer,\\\n",
    "                                               scheduler,dataloader_train, dataloader_valid)\n",
    "            buildingTrainResults = buildingTrainResults + foldResults\n",
    "            \n",
    "    buildingTrainResults = pd.DataFrame(buildingTrainResults)\n",
    "    buildingTrainResults['valTotalLoss'] = buildingTrainResults['valPosLoss'] + buildingTrainResults['valFloorLoss']\n",
    "    buildingTrainResults['trainTotalLoss'] = buildingTrainResults['trainPosLoss'] + buildingTrainResults['trainFloorLoss']\n",
    "    \n",
    "    return buildingTrainResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFoldBestResultsDf(trainResults):\n",
    "    bestResults = []\n",
    "    numFolds = trainResults['fold'].nunique()\n",
    "    \n",
    "    for fold in range(numFolds):\n",
    "        foldDf = trainResults[trainResults['fold']== fold]\n",
    "        bestResults.append(foldDf.iloc[np.argmin(foldDf['valTotalLoss'].values),:])\n",
    "    \n",
    "    bestResults =pd.DataFrame(bestResults)\n",
    "    valPosLossBest = bestResults['valPosLoss'].values\n",
    "    print(f\"Best valPosLoss for all folds = {valPosLossBest}\")\n",
    "    print(f\"Mean, std ={valPosLossBest.mean()}, {valPosLossBest.std()}\")\n",
    "    return bestResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-burton",
   "metadata": {},
   "source": [
    "## Training & Validation main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if CFG.TRAIN == True:\n",
    "    \n",
    "    #for building in buildingsList:\n",
    "    buildingTrainResults = trainValidateOneBuilding(buildingsList[0])\n",
    "    bestResults = getFoldBestResultsDf(buildingTrainResults)\n",
    "    # save results to csv\n",
    "    buildingTrainResults.to_csv(f\"{modelOutputDir}/{getBuildingName(buildingsList[0])}_{CFG.MODEL_NAME}_trainResults.csv\")\n",
    "    bestResults.to_csv(f\"{modelOutputDir}/{getBuildingName(buildingsList[0])}_{CFG.MODEL_NAME}_bestResults.csv\")\n",
    "    \n",
    "    # print and plot\n",
    "    #print(buildingTrainResults.head(3))\n",
    "    #print(bestResults.head(3))\n",
    "    plotTrainingResults(buildingTrainResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-omega",
   "metadata": {},
   "source": [
    "## Generate OOF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateModelOOF():\n",
    "    oof_ts = []\n",
    "    oof_preds = []\n",
    "    oof_targets = []\n",
    "    oof_groups = []\n",
    "    oof_folds = []\n",
    "\n",
    "    buildingName = getBuildingName(buildingCsvPath=buildingsList[0])\n",
    "    modelPaths = sorted(glob.glob(f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_fold*.pth\"))\n",
    "\n",
    "    for fold in range(len(modelPaths)):\n",
    "        # load building-model-fold checkpoint\n",
    "        checkPoint = torch.load(modelPaths[fold])\n",
    "        numRows = len(checkPoint['val_ts'])\n",
    "\n",
    "        oof_ts.append(checkPoint['val_ts'])\n",
    "        oof_preds.append(checkPoint['val_preds'])\n",
    "        oof_targets.append(checkPoint['val_targets'])\n",
    "        oof_groups.append(checkPoint['val_groups'])\n",
    "        oof_folds.append([fold] * numRows)\n",
    "    \n",
    "    oof_ts = np.concatenate(oof_ts,axis=0)\n",
    "    oof_preds = np.concatenate(oof_preds,axis=0)\n",
    "    oof_targets = np.concatenate(oof_targets,axis=0)\n",
    "    oof_groups = np.concatenate(oof_groups,axis=0)\n",
    "    oof_folds = np.concatenate(oof_folds,axis=0)\n",
    "    \n",
    "    #print(oof_ts.shape, oof_preds.shape, oof_targets.shape, oof_groups.shape, oof_folds.shape)\n",
    "    oof_df = pd.DataFrame({'ts' : oof_ts, 'x_preds': oof_preds[:,0], 'y_preds': oof_preds[:,1],\n",
    "                       'floor_preds': oof_preds[:,2], 'x_tgt': oof_targets[:,0], 'y_tgt': oof_targets[:,1],\n",
    "                       'floor_tgt': oof_targets[:,2], 'path' : oof_groups, 'fold' : oof_folds\n",
    "                      })\n",
    "    \n",
    "    return oof_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
