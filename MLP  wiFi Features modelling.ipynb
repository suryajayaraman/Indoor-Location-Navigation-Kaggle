{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "trying-collection",
   "metadata": {
    "papermill": {
     "duration": 0.024471,
     "end_time": "2021-04-27T13:11:21.163601",
     "exception": false,
     "start_time": "2021-04-27T13:11:21.139130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mounted-bishop",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:21.217937Z",
     "iopub.status.busy": "2021-04-27T13:11:21.216372Z",
     "iopub.status.idle": "2021-04-27T13:11:21.218757Z",
     "shell.execute_reply": "2021-04-27T13:11:21.219174Z"
    },
    "papermill": {
     "duration": 0.032686,
     "end_time": "2021-04-27T13:11:21.219383",
     "exception": false,
     "start_time": "2021-04-27T13:11:21.186697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## !rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "english-actress",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:21.269728Z",
     "iopub.status.busy": "2021-04-27T13:11:21.269129Z",
     "iopub.status.idle": "2021-04-27T13:11:32.858404Z",
     "shell.execute_reply": "2021-04-27T13:11:32.857920Z"
    },
    "papermill": {
     "duration": 11.616186,
     "end_time": "2021-04-27T13:11:32.858567",
     "exception": false,
     "start_time": "2021-04-27T13:11:21.242381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle5\r\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 132 kB 4.4 MB/s \r\n",
      "\u001b[?25hBuilding wheels for collected packages: pickle5\r\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=245919 sha256=759a188acf980966141cc531d9a28710214ed612055781f7dda2c7a17d089c4d\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/7e/6a/00/67136a90d6aca437d806d1d3cedf98106e840c97a3e5188198\r\n",
      "Successfully built pickle5\r\n",
      "Installing collected packages: pickle5\r\n",
      "Successfully installed pickle5-0.0.11\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "after-transfer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:32.923943Z",
     "iopub.status.busy": "2021-04-27T13:11:32.922650Z",
     "iopub.status.idle": "2021-04-27T13:11:38.003778Z",
     "shell.execute_reply": "2021-04-27T13:11:38.004296Z"
    },
    "papermill": {
     "duration": 5.118475,
     "end_time": "2021-04-27T13:11:38.004504",
     "exception": false,
     "start_time": "2021-04-27T13:11:32.886029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# DL library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from  torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# metrics calculation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# basic plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# interactive plots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import iplot\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-korean",
   "metadata": {
    "papermill": {
     "duration": 0.044975,
     "end_time": "2021-04-27T13:11:38.112689",
     "exception": false,
     "start_time": "2021-04-27T13:11:38.067714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "leading-linux",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:38.295129Z",
     "iopub.status.busy": "2021-04-27T13:11:38.293800Z",
     "iopub.status.idle": "2021-04-27T13:11:38.295786Z",
     "shell.execute_reply": "2021-04-27T13:11:38.294442Z"
    },
    "papermill": {
     "duration": 0.143912,
     "end_time": "2021-04-27T13:11:38.295942",
     "exception": false,
     "start_time": "2021-04-27T13:11:38.152030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # pipeline parameters\n",
    "    SEED        = 42\n",
    "    TRAIN       = True\n",
    "    LR_FIND     = False\n",
    "    TEST        = False\n",
    "    N_FOLDS     = 5 \n",
    "    N_EPOCHS    = 120\n",
    "    TEST_BATCH_SIZE  = 32\n",
    "    TRAIN_BATCH_SIZE = 16\n",
    "    NUM_WORKERS      = 4\n",
    "    DATA_FRAC        = 1.0\n",
    "    FOLD_TO_TRAIN    = [0, 1, 2, 3, 4] # \n",
    "\n",
    "    # model parameters\n",
    "    MODEL_ARCH  = 'MLP'\n",
    "    MODEL_NAME  = 'mlp_v7'\n",
    "    WGT_PATH    = ''\n",
    "    WGT_MODEL   = ''\n",
    "    PRINT_N_EPOCH = 2\n",
    "    \n",
    "    # scheduler variables\n",
    "    MAX_LR    = 1e-2\n",
    "    MIN_LR    = 1e-5\n",
    "    SCHEDULER = 'CosineAnnealingWarmRestarts'  # ['ReduceLROnPlateau', 'None', OneCycleLR','CosineAnnealingLR']\n",
    "    T_0       = 10     # CosineAnnealingWarmRestarts\n",
    "    T_MULT    = 2      # CosineAnnealingWarmRestarts\n",
    "    T_MAX     = 10      # CosineAnnealingLR\n",
    "\n",
    "    # optimizer variables\n",
    "    OPTIMIZER     = 'Adam'\n",
    "    WEIGHT_DECAY  = 1e-6\n",
    "    GRD_ACC_STEPS = 1\n",
    "    MAX_GRD_NORM  = 1000\n",
    "\n",
    "    # features parameters\n",
    "    USE_FREQ_FEATS = True\n",
    "    \n",
    "    BUILDING_SITES_RANGE = [20,21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smart-chile",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:38.475249Z",
     "iopub.status.busy": "2021-04-27T13:11:38.474438Z",
     "iopub.status.idle": "2021-04-27T13:11:38.478829Z",
     "shell.execute_reply": "2021-04-27T13:11:38.479444Z"
    },
    "papermill": {
     "duration": 0.05939,
     "end_time": "2021-04-27T13:11:38.479672",
     "exception": false,
     "start_time": "2021-04-27T13:11:38.420282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "floor_map = {\"B2\": -2, \"B1\": -1, \"F1\": 0, \"F2\": 1, \"F3\": 2, \"F4\": 3, \"F5\": 4, \"F6\": 5, \"F7\": 6, \"F8\": 7, \"F9\": 8,\n",
    "             \"1F\": 0, \"2F\": 1, \"3F\": 2, \"4F\": 3, \"5F\": 4, \"6F\": 5, \"7F\": 6, \"8F\": 7, \"9F\": 8}\n",
    "\n",
    "minCount = 1\n",
    "freqFillerValue = 0\n",
    "rssiFillerValue = -999.0\n",
    "dtFillerValue   = 1000.0\n",
    "modelOutputDir = '.'\n",
    "wifiFeaturesDir_train = '../input/idln-mlp-wifi-features-dataset/wiFiFeatures/train'\n",
    "wifiFeaturesDir_test  = '../input/idln-mlp-wifi-features-dataset/wiFiFeatures/test'\n",
    "sampleCsvPath = '../input/indoor-location-navigation/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-disclaimer",
   "metadata": {
    "papermill": {
     "duration": 0.041734,
     "end_time": "2021-04-27T13:11:38.564352",
     "exception": false,
     "start_time": "2021-04-27T13:11:38.522618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "organized-listening",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:38.657268Z",
     "iopub.status.busy": "2021-04-27T13:11:38.656417Z",
     "iopub.status.idle": "2021-04-27T13:11:38.658593Z",
     "shell.execute_reply": "2021-04-27T13:11:38.657959Z"
    },
    "papermill": {
     "duration": 0.051978,
     "end_time": "2021-04-27T13:11:38.658744",
     "exception": false,
     "start_time": "2021-04-27T13:11:38.606766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getBuildingName(buildingDataPath):\n",
    "    return buildingDataPath.split('/')[-1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "driven-nickel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:38.751601Z",
     "iopub.status.busy": "2021-04-27T13:11:38.750708Z",
     "iopub.status.idle": "2021-04-27T13:11:38.753190Z",
     "shell.execute_reply": "2021-04-27T13:11:38.752385Z"
    },
    "papermill": {
     "duration": 0.052561,
     "end_time": "2021-04-27T13:11:38.753364",
     "exception": false,
     "start_time": "2021-04-27T13:11:38.700803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_no_of_trainable_params(model):\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "painted-future",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:38.836380Z",
     "iopub.status.busy": "2021-04-27T13:11:38.835744Z",
     "iopub.status.idle": "2021-04-27T13:11:38.841932Z",
     "shell.execute_reply": "2021-04-27T13:11:38.841420Z"
    },
    "papermill": {
     "duration": 0.046693,
     "end_time": "2021-04-27T13:11:38.842049",
     "exception": false,
     "start_time": "2021-04-27T13:11:38.795356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(CFG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "inclusive-monday",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:38.903328Z",
     "iopub.status.busy": "2021-04-27T13:11:38.902597Z",
     "iopub.status.idle": "2021-04-27T13:11:38.906305Z",
     "shell.execute_reply": "2021-04-27T13:11:38.905866Z"
    },
    "papermill": {
     "duration": 0.038048,
     "end_time": "2021-04-27T13:11:38.906417",
     "exception": false,
     "start_time": "2021-04-27T13:11:38.868369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getBuildingData(buildingDataPath):\n",
    "    # read building data \n",
    "    #### data = pd.read_pickle(buildingDataPath)\n",
    "    with open(buildingDataPath, 'rb') as inputFile:\n",
    "        data = pickle.load(inputFile)    \n",
    "    \n",
    "    # use fraction if needed\n",
    "    if CFG.DATA_FRAC < 1:\n",
    "        data = data.sample(frac=CFG.DATA_FRAC).reset_index(drop=True)\n",
    "\n",
    "    # first column is timestamp\n",
    "    timestamps = data.iloc[:,0].values   # np.expand_dims( , ,axis=1)\n",
    "    \n",
    "    # last column is pathFile name\n",
    "    groups = data.iloc[:,-1].values\n",
    "    \n",
    "    # target values are last but 3 columns\n",
    "    y = data.iloc[:,-4:-1].values\n",
    "    \n",
    "    # use all features\n",
    "    if CFG.USE_FREQ_FEATS == True:\n",
    "        X = data.iloc[:,1:-4].values    \n",
    "\n",
    "    else:\n",
    "        numWiFiIds = int((data.shape[1] - 5) / 3)\n",
    "        # separate into features and target variables\n",
    "        X = data.iloc[:,1:(2*numWiFiIds)+1].values    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Incase freq signal is not needed, use rssi and dt features alone    \n",
    "    # There are 5 columns for timestamp, y, pathNames values in csv, reamining are features\n",
    "    # total features = 3 * [rssi, dt, freq]\n",
    "    # hence unique wifi ids = totalFeatures / 3\n",
    "    \"\"\"\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return timestamps,X,y,groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sapphire-muscle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:38.964464Z",
     "iopub.status.busy": "2021-04-27T13:11:38.963926Z",
     "iopub.status.idle": "2021-04-27T13:11:38.968157Z",
     "shell.execute_reply": "2021-04-27T13:11:38.967730Z"
    },
    "papermill": {
     "duration": 0.034996,
     "end_time": "2021-04-27T13:11:38.968266",
     "exception": false,
     "start_time": "2021-04-27T13:11:38.933270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getInputFeatureSize(featureFilesPath):\n",
    "    sampleData = np.load(f\"{npyWifiFeaturesDir}/{featureFilesPath[0]}\")\n",
    "    return len(sampleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "seeing-hughes",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:39.027425Z",
     "iopub.status.busy": "2021-04-27T13:11:39.026870Z",
     "iopub.status.idle": "2021-04-27T13:11:39.031042Z",
     "shell.execute_reply": "2021-04-27T13:11:39.030382Z"
    },
    "papermill": {
     "duration": 0.036589,
     "end_time": "2021-04-27T13:11:39.031160",
     "exception": false,
     "start_time": "2021-04-27T13:11:38.994571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def competitionMetric(preds, targets):\n",
    "    \"\"\" The metric used in this competition \"\"\"\n",
    "    # position error\n",
    "    meanPosPredictionError = torch.mean(torch.sqrt(\n",
    "                             torch.square(torch.subtract(preds[:,0], targets[:,0])) + \n",
    "                             torch.square(torch.subtract(preds[:,1], targets[:,1]))))\n",
    "    # error in floor prediction\n",
    "    meanFloorPredictionError = torch.mean(15 * torch.abs(preds[:,2] - targets[:,2]))\n",
    "    return meanPosPredictionError, meanFloorPredictionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "assigned-subject",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:39.090099Z",
     "iopub.status.busy": "2021-04-27T13:11:39.089387Z",
     "iopub.status.idle": "2021-04-27T13:11:39.092667Z",
     "shell.execute_reply": "2021-04-27T13:11:39.093038Z"
    },
    "papermill": {
     "duration": 0.035328,
     "end_time": "2021-04-27T13:11:39.093169",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.057841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getOptimizer(model : nn.Module):    \n",
    "    if CFG.OPTIMIZER == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), weight_decay=CFG.WEIGHT_DECAY, lr=CFG.MAX_LR)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), weight_decay=CFG.WEIGHT_DECAY, lr=CFG.MAX_LR, momentum=0.9)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "behind-parent",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:39.153330Z",
     "iopub.status.busy": "2021-04-27T13:11:39.152791Z",
     "iopub.status.idle": "2021-04-27T13:11:39.156469Z",
     "shell.execute_reply": "2021-04-27T13:11:39.155956Z"
    },
    "papermill": {
     "duration": 0.036591,
     "end_time": "2021-04-27T13:11:39.156628",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.120037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getScheduler(optimizer, dataloader_train):\n",
    "    if CFG.SCHEDULER == 'OneCycleLR':\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr= CFG.MAX_LR, epochs = CFG.N_EPOCHS, \n",
    "                          steps_per_epoch = len(dataloader_train), pct_start=0.25, div_factor=10, anneal_strategy='cos')\n",
    "    elif CFG.SCHEDULER == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=CFG.T_MULT, eta_min=CFG.MIN_LR, last_epoch=-1)\n",
    "    elif CFG.SCHEDULER == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_MAX * len(dataloader_train), eta_min=CFG.MIN_LR, last_epoch=-1)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vietnamese-penguin",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:39.215824Z",
     "iopub.status.busy": "2021-04-27T13:11:39.215140Z",
     "iopub.status.idle": "2021-04-27T13:11:39.219197Z",
     "shell.execute_reply": "2021-04-27T13:11:39.218779Z"
    },
    "papermill": {
     "duration": 0.035913,
     "end_time": "2021-04-27T13:11:39.219306",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.183393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDataLoader(dataset, datasetType : str):\n",
    "    if datasetType == 'train':\n",
    "        batchSize = CFG.TRAIN_BATCH_SIZE\n",
    "        shuffleDataset = True\n",
    "    else:\n",
    "        batchSize = CFG.TEST_BATCH_SIZE\n",
    "        shuffleDataset = False\n",
    "    \n",
    "    dataLoader = DataLoader(dataset, batch_size= batchSize, shuffle=shuffleDataset,\n",
    "                            num_workers=CFG.NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "    return dataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-jerusalem",
   "metadata": {
    "papermill": {
     "duration": 0.026562,
     "end_time": "2021-04-27T13:11:39.272594",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.246032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "def getFoldDataLoaders(timestamps,X,y,groups,trainIndex,validIndex):\n",
    "    \n",
    "    # splitting into train and validataion sets\n",
    "    trainTimeStamps, X_train, y_train, trainGroups = timestamps[trainIndex], X[trainIndex], y[trainIndex], groups[trainIndex]\n",
    "    validTimeStamps, X_valid, y_valid, validGroups = timestamps[validIndex], X[validIndex], y[validIndex], groups[validIndex] \n",
    "    \n",
    "    # normalize input            \n",
    "    #print(f\"Before stdscaler : train_mean{X_train.mean(), X_train.std(), X_valid.mean(), X_valid.std()}\")\n",
    "    X_train = stdScaler.fit_transform(X_train)\n",
    "    X_valid = stdScaler.transform(X_valid)\n",
    "    #print(f\"After stdscaler : train_mean{X_train.mean(), X_train.std(), X_valid.mean(), X_valid.std()}\")\n",
    "    #print(f\"x,y shapes = {X_train.shape, y_train.shape, X_valid.shape, y_valid.shape}\")\n",
    "                        \n",
    "    # create torch Datasets and Dataloader for each fold's train and validation data\n",
    "    dataset_train = wiFiFeaturesDataset(trainTimeStamps, X_train, y_train, trainGroups)\n",
    "    dataset_valid = wiFiFeaturesDataset(validTimeStamps, X_valid, y_valid, validGroups)            \n",
    "    dataloader_train = DataLoader(dataset_train, batch_size= CFG.TRAIN_BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=CFG.NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "    dataloader_valid = DataLoader(dataset_valid, batch_size= CFG.TEST_BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=CFG.NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "    return dataloader_train, dataloader_valid\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "educational-configuration",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:39.337178Z",
     "iopub.status.busy": "2021-04-27T13:11:39.335421Z",
     "iopub.status.idle": "2021-04-27T13:11:39.338032Z",
     "shell.execute_reply": "2021-04-27T13:11:39.338421Z"
    },
    "papermill": {
     "duration": 0.039023,
     "end_time": "2021-04-27T13:11:39.338585",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.299562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotTrainingResults(resultsDf, buildingName):\n",
    "    # subplot to plot\n",
    "    fig = make_subplots(rows=1, cols=1)\n",
    "    colors = [ ('#d32f2f', '#ef5350'), ('#303f9f', '#5c6bc0'), ('#00796b', '#26a69a'),\n",
    "                ('#fbc02d', '#ffeb3b'), ('#5d4037', '#8d6e63')]\n",
    "\n",
    "    # find number of folds input df\n",
    "    numberOfFolds = resultsDf['fold'].nunique()\n",
    "    \n",
    "    # iterate through folds and plot\n",
    "    for i in range(numberOfFolds):\n",
    "        data = resultsDf[resultsDf['fold'] == i]\n",
    "        fig.add_trace(go.Scatter(x=data['epoch'].values, y=data['trainPosLoss'].values,\n",
    "                                mode='lines', visible='legendonly' if i > 0 else True,\n",
    "                                line=dict(color=colors[i][0], width=2),\n",
    "                                name='{}-trainPossLoss-Fold{}'.format(buildingName, i)),row=1, col=1)\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=data['epoch'], y=data['valPosLoss'].values,\n",
    "                                 mode='lines+markers', visible='legendonly' if i > 0 else True,\n",
    "                                 line=dict(color=colors[i][1], width=2),\n",
    "                                 name='{}-valPosLoss-Fold{}'.format(buildingName,i)),row=1, col=1)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-fancy",
   "metadata": {
    "papermill": {
     "duration": 0.026691,
     "end_time": "2021-04-27T13:11:39.392050",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.365359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beginning-reduction",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:39.454279Z",
     "iopub.status.busy": "2021-04-27T13:11:39.453550Z",
     "iopub.status.idle": "2021-04-27T13:11:39.457260Z",
     "shell.execute_reply": "2021-04-27T13:11:39.456797Z"
    },
    "papermill": {
     "duration": 0.038229,
     "end_time": "2021-04-27T13:11:39.457388",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.419159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class wiFiFeaturesDataset(Dataset):\n",
    "    def __init__(self, timeStamps, X_data, y_data, groups):\n",
    "        self.timeStamps = timeStamps \n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.groups = groups\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x  = torch.from_numpy(self.X_data[index].astype(np.float32))\n",
    "        y  = torch.from_numpy(self.y_data[index].astype(np.float32))\n",
    "        ts = self.timeStamps[index].astype(np.int64)\n",
    "        group = self.groups[index]\n",
    "        return ts,x,y,group\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "regulated-harbor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:39.519596Z",
     "iopub.status.busy": "2021-04-27T13:11:39.517803Z",
     "iopub.status.idle": "2021-04-27T13:11:39.520159Z",
     "shell.execute_reply": "2021-04-27T13:11:39.520563Z"
    },
    "papermill": {
     "duration": 0.035979,
     "end_time": "2021-04-27T13:11:39.520698",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.484719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class wiFiFeaturesDataset_test(Dataset):\n",
    "    def __init__(self, timeStamps, X_data, groups):\n",
    "        self.timeStamps = timeStamps \n",
    "        self.X_data = X_data\n",
    "        self.groups = groups\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x  = torch.from_numpy(self.X_data[index].astype(np.float32))\n",
    "        ts = self.timeStamps[index].astype(np.int64)\n",
    "        group = self.groups[index]\n",
    "        return ts,x,group\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-michael",
   "metadata": {
    "papermill": {
     "duration": 0.027668,
     "end_time": "2021-04-27T13:11:39.575950",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.548282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MLP Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unique-alpha",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:39.641179Z",
     "iopub.status.busy": "2021-04-27T13:11:39.639441Z",
     "iopub.status.idle": "2021-04-27T13:11:39.641869Z",
     "shell.execute_reply": "2021-04-27T13:11:39.642277Z"
    },
    "papermill": {
     "duration": 0.039149,
     "end_time": "2021-04-27T13:11:39.642414",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.603265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class wiFiFeaturesMLPModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_features=n_input, out_features=512)\n",
    "        self.lin2 = nn.Linear(in_features=512,     out_features=32)\n",
    "        self.lin3 = nn.Linear(in_features=32,      out_features=n_output)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.drops = nn.Dropout(0.3)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        numBatches = x.shape[0]\n",
    "        \n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        \n",
    "        ## batchnorm doesnt work for batchsize of 1\n",
    "        if numBatches > 1:\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(self.lin2(x))\n",
    "            x = self.drops(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.lin3(x)\n",
    "        else:\n",
    "            x = F.relu(self.lin2(x))\n",
    "            x = self.drops(x)\n",
    "            x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-crash",
   "metadata": {
    "papermill": {
     "duration": 0.0273,
     "end_time": "2021-04-27T13:11:39.697131",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.669831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lr range finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "departmental-tampa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:39.758681Z",
     "iopub.status.busy": "2021-04-27T13:11:39.758017Z",
     "iopub.status.idle": "2021-04-27T13:11:39.762129Z",
     "shell.execute_reply": "2021-04-27T13:11:39.761706Z"
    },
    "papermill": {
     "duration": 0.0379,
     "end_time": "2021-04-27T13:11:39.762243",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.724343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_lr_finder_results(lr_finder): \n",
    "    # Create subplot grid\n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    # layout ={'title': 'Lr_finder_result'}\n",
    "    \n",
    "    # Create a line (trace) for the lr vs loss, gradient of loss\n",
    "    trace0 = go.Scatter(x=lr_finder['log_lr'], y=lr_finder['smooth_loss'],name='log_lr vs smooth_loss')\n",
    "    trace1 = go.Scatter(x=lr_finder['log_lr'], y=lr_finder['grad_loss'],name='log_lr vs loss gradient')\n",
    "\n",
    "    # Add subplot trace & assign to each grid\n",
    "    fig.add_trace(trace0, row=1, col=1);\n",
    "    fig.add_trace(trace1, row=1, col=2);\n",
    "    iplot(fig, show_link=False)\n",
    "    #fig.write_html(CFG.MODEL_NAME + '_lr_find.html');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "positive-warner",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:39.830507Z",
     "iopub.status.busy": "2021-04-27T13:11:39.829963Z",
     "iopub.status.idle": "2021-04-27T13:11:39.834176Z",
     "shell.execute_reply": "2021-04-27T13:11:39.833765Z"
    },
    "papermill": {
     "duration": 0.044698,
     "end_time": "2021-04-27T13:11:39.834289",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.789591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_lr(model, optimizer, data_loader, init_value = 1e-8, final_value=100.0, beta = 0.98, num_batches = 200):\n",
    "    assert(num_batches > 0)\n",
    "    mult = (final_value / init_value) ** (1/num_batches)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    batch_num = 0\n",
    "    avg_loss = 0.0\n",
    "    best_loss = 0.0\n",
    "    smooth_losses = []\n",
    "    raw_losses = []\n",
    "    log_lrs = []\n",
    "    dataloader_it = iter(data_loader)\n",
    "    progress_bar = tqdm(range(num_batches))                \n",
    "        \n",
    "    for idx in progress_bar:\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            _, inputs, targets = next(dataloader_it)\n",
    "            #print(images.shape)\n",
    "        except:\n",
    "            dataloader_it = iter(data_loader)\n",
    "            _, inputs, targets = next(dataloader_it)\n",
    "\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # handle exception in criterion\n",
    "        try:\n",
    "            # Forward pass\n",
    "            y_preds = model(inputs)\n",
    "            posLoss, floorLoss = criterion(y_preds, targets)\n",
    "            loss = posLoss + floorLoss\n",
    "        except:\n",
    "            if len(smooth_losses) > 1:\n",
    "                grad_loss = np.gradient(smooth_losses)\n",
    "            else:\n",
    "                grad_loss = 0.0\n",
    "            lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                                 'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "            return lr_finder_results \n",
    "                    \n",
    "        #Compute the smoothed loss\n",
    "        avg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        \n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 50 * best_loss:\n",
    "            if len(smooth_losses) > 1:\n",
    "                grad_loss = np.gradient(smooth_losses)\n",
    "            else:\n",
    "                grad_loss = 0.0\n",
    "            lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                                 'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "            return lr_finder_results\n",
    "        \n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num==1:\n",
    "            best_loss = smoothed_loss\n",
    "        \n",
    "        #Store the values\n",
    "        raw_losses.append(loss.item())\n",
    "        smooth_losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print info\n",
    "        progress_bar.set_description(f\"loss:{loss.item()},smoothLoss: {smoothed_loss},lr:{lr}\")\n",
    "\n",
    "        #Update the lr for the next step\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "    \n",
    "    grad_loss = np.gradient(smooth_losses)\n",
    "    lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                         'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "    return lr_finder_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "earlier-mentor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:39.897947Z",
     "iopub.status.busy": "2021-04-27T13:11:39.896148Z",
     "iopub.status.idle": "2021-04-27T13:11:39.898502Z",
     "shell.execute_reply": "2021-04-27T13:11:39.898907Z"
    },
    "papermill": {
     "duration": 0.03706,
     "end_time": "2021-04-27T13:11:39.899046",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.861986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.LR_FIND == True:\n",
    "    # create dataset instance\n",
    "    tempTs, tempX, tempY,_ = getBuildingData(buildingCsvPath=buildingsList[0])\n",
    "    tempX = stdScaler.fit_transform(tempX)\n",
    "    tempTrainDataset = wiFiFeaturesDataset(tempTs, tempX, tempY)\n",
    "    tempTrainDataloader = DataLoader(tempTrainDataset, batch_size= CFG.TRAIN_BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=CFG.NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "    \n",
    "    # create model instance   \n",
    "    model = wiFiFeaturesMLPModel(n_input=tempX.shape[1], n_output=3)\n",
    "    model.to(device);\n",
    "    \n",
    "    # optimizer function, lr schedulers and loss function\n",
    "    optimizer = getOptimizer(model)\n",
    "    lrFinderResults = find_lr(model, optimizer, tempTrainDataloader)\n",
    "    plot_lr_finder_results(lrFinderResults)\n",
    "    del tempX, tempY, tempTrainDataset, tempTrainDataloader, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-necessity",
   "metadata": {
    "papermill": {
     "duration": 0.027552,
     "end_time": "2021-04-27T13:11:39.953935",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.926383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train & Validate helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "working-forestry",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:40.019057Z",
     "iopub.status.busy": "2021-04-27T13:11:40.018327Z",
     "iopub.status.idle": "2021-04-27T13:11:40.022117Z",
     "shell.execute_reply": "2021-04-27T13:11:40.021693Z"
    },
    "papermill": {
     "duration": 0.040526,
     "end_time": "2021-04-27T13:11:40.022229",
     "exception": false,
     "start_time": "2021-04-27T13:11:39.981703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validateModel(model, validationDataloader):\n",
    "    # placeholders to store output\n",
    "    val_ts = []\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    val_groups = []\n",
    "\n",
    "    # set model to Validate mode\n",
    "    model.eval()\n",
    "    dataLoaderIterator = iter(validationDataloader)\n",
    "\n",
    "    for idx in range(len(validationDataloader)):\n",
    "        try:\n",
    "            ts, inputs, targets, valGroups = next(dataLoaderIterator)\n",
    "        except StopIteration:\n",
    "            dataLoaderIterator = iter(validationDataloader)\n",
    "            ts, inputs, targets, valGroups = next(dataLoaderIterator)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device) \n",
    "\n",
    "        # forward prediction\n",
    "        with torch.no_grad():    \n",
    "            y_preds = model(inputs)\n",
    "\n",
    "        # store predictions and targets to compute metrics later\n",
    "        val_ts.append(ts)\n",
    "        val_preds.append(y_preds)\n",
    "        val_targets.append(targets)\n",
    "        val_groups.append(valGroups)\n",
    "\n",
    "    # concatenate to get as 1 2d array and find total loss  \n",
    "    val_preds = torch.cat(val_preds, 0)\n",
    "    val_targets = torch.cat(val_targets, 0)\n",
    "    valPosLoss, valFloorLoss = criterion(val_preds, val_targets)\n",
    "    valScore = valPosLoss #+ valFloorLoss\n",
    "\n",
    "    # np array concatenation\n",
    "    val_ts = np.concatenate(val_ts, axis=0)\n",
    "    val_groups = np.concatenate(val_groups, axis=0)\n",
    "    \n",
    "    # store results\n",
    "    validationResults = {'valPosLoss': valPosLoss.item() , 'valFloorLoss': valFloorLoss.item(),\\\n",
    "                         'val_ts': val_ts, 'val_groups': val_groups,\n",
    "                         'val_preds'  :val_preds.cpu().data.numpy(), \n",
    "                         'val_targets':val_targets.cpu().data.numpy(),\n",
    "                         }\n",
    "    return validationResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "pressed-relay",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:40.089721Z",
     "iopub.status.busy": "2021-04-27T13:11:40.089074Z",
     "iopub.status.idle": "2021-04-27T13:11:40.093190Z",
     "shell.execute_reply": "2021-04-27T13:11:40.092768Z"
    },
    "papermill": {
     "duration": 0.043502,
     "end_time": "2021-04-27T13:11:40.093300",
     "exception": false,
     "start_time": "2021-04-27T13:11:40.049798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainValidateOneFold(buildingName, i_fold, model, optimizer, scheduler, dataloader_train, dataloader_valid):\n",
    "    trainFoldResults = []\n",
    "    bestValScore = np.inf\n",
    "    bestEpoch = 0\n",
    "\n",
    "    for epoch in range(CFG.N_EPOCHS):\n",
    "        #print('Epoch {}/{}'.format(epoch + 1, CFG.N_EPOCHS))\n",
    "        model.train()\n",
    "        trainPosLoss = 0.0\n",
    "        trainFloorLoss = 0.0\n",
    "\n",
    "        # training iterator\n",
    "        tr_iterator = iter(dataloader_train)\n",
    "\n",
    "        for idx in range(len(dataloader_train)):\n",
    "            try:\n",
    "                _, inputs, targets, _ = next(tr_iterator)\n",
    "            except StopIteration:\n",
    "                tr_iterator = iter(dataloader_train)\n",
    "                _, inputs, targets, _ = next(tr_iterator)\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)  \n",
    "\n",
    "            # builtin package to handle automatic mixed precision\n",
    "            with autocast():\n",
    "                # Forward pass\n",
    "                y_preds = model(inputs)   \n",
    "                posLoss, floorLoss = criterion(y_preds, targets)\n",
    "                loss = posLoss # + floorLoss\n",
    "\n",
    "                # Backward pass\n",
    "                scaler.scale(loss).backward()        \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad() \n",
    "\n",
    "                # log the necessary losses\n",
    "                trainPosLoss   += posLoss.item()\n",
    "                trainFloorLoss += floorLoss.item()\n",
    "\n",
    "                if scheduler is not None: \n",
    "                    if CFG.SCHEDULER == 'CosineAnnealingWarmRestarts':\n",
    "                        scheduler.step(epoch + idx / len(dataloader_train)) \n",
    "                    # onecyle lr scheduler / CosineAnnealingLR scheduler\n",
    "                    else:\n",
    "                        scheduler.step()\n",
    "                    \n",
    "        # Validate\n",
    "        foldValidationResults = validateModel(model, dataloader_valid)\n",
    "         \n",
    "        # store results\n",
    "        trainFoldResults.append({ 'fold': i_fold, 'epoch': epoch, \n",
    "                                  'trainPosLoss': trainPosLoss / len(dataloader_train), \n",
    "                                  'trainFloorLoss': trainFloorLoss / len(dataloader_train), \n",
    "                                  'valPosLoss'  : foldValidationResults['valPosLoss'] , \n",
    "                                  'valFloorLoss': foldValidationResults['valFloorLoss']})\n",
    "        \n",
    "        valScore = foldValidationResults['valPosLoss'] # + foldVal['valFloorLoss']\n",
    "        # save best models        \n",
    "        if(valScore < bestValScore):\n",
    "            # reset variables\n",
    "            bestValScore = valScore\n",
    "            bestEpoch = epoch\n",
    "\n",
    "            # save model weights\n",
    "            torch.save({'model': model.state_dict(), 'val_ts' : foldValidationResults['val_ts'], \n",
    "                        'val_preds':foldValidationResults['val_preds'], \n",
    "                        'val_targets':foldValidationResults['val_targets'],\n",
    "                        'val_groups' : foldValidationResults['val_groups']}, \n",
    "                        f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_fold{i_fold}_best.pth\")\n",
    "\n",
    "    print(f\"For Fold {i_fold}, Best validation score of {bestValScore} was got at epoch {bestEpoch}\") \n",
    "    return trainFoldResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "advance-corrections",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:40.161773Z",
     "iopub.status.busy": "2021-04-27T13:11:40.161105Z",
     "iopub.status.idle": "2021-04-27T13:11:40.164175Z",
     "shell.execute_reply": "2021-04-27T13:11:40.163758Z"
    },
    "papermill": {
     "duration": 0.043462,
     "end_time": "2021-04-27T13:11:40.164292",
     "exception": false,
     "start_time": "2021-04-27T13:11:40.120830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainValidateOneBuilding(buildingDataPath, modelToFit):\n",
    "    # placeholder to store results\n",
    "    buildingTrainResults = []\n",
    "    \n",
    "    buildingName = getBuildingName(buildingDataPath)\n",
    "    print(f\"Processing data for building - {buildingName}\")\n",
    "    timestamps, X, y, groups = getBuildingData(buildingDataPath=buildingDataPath)\n",
    "    print(f\"Building Data shapes : {timestamps.shape, X.shape, y.shape, groups.shape}\")\n",
    "\n",
    "    for i_fold, (trainIndex, validIndex) in enumerate(folds.split(X=X, y=y[:,0],groups=groups)):\n",
    "        if i_fold in CFG.FOLD_TO_TRAIN:\n",
    "            ## print(\"Fold {}/{}\".format(i_fold + 1, CFG.N_FOLDS))\n",
    "            \n",
    "            # splitting into train and validataion sets\n",
    "            trainTimeStamps, X_train, y_train, trainGroups = timestamps[trainIndex], X[trainIndex], y[trainIndex], groups[trainIndex]\n",
    "            validTimeStamps, X_valid, y_valid, validGroups = timestamps[validIndex], X[validIndex], y[validIndex], groups[validIndex] \n",
    "\n",
    "            # normalize input            \n",
    "            X_train = stdScaler.fit_transform(X_train)\n",
    "            X_valid = stdScaler.transform(X_valid)\n",
    "            \n",
    "            # save stdScaler transform\n",
    "            with open(f\"{buildingName}_stdScaler_fold{i_fold}.pkl\",'wb') as outputFile:\n",
    "                pickle.dump(stdScaler, outputFile)\n",
    "                        \n",
    "            # create torch Datasets and Dataloader for each fold's train and validation data\n",
    "            dataset_train = wiFiFeaturesDataset(trainTimeStamps, X_train, y_train, trainGroups)\n",
    "            dataset_valid = wiFiFeaturesDataset(validTimeStamps, X_valid, y_valid, validGroups)            \n",
    "            dataloader_train = getDataLoader(dataset_train, datasetType= 'train')\n",
    "            dataloader_valid = getDataLoader(dataset_valid, datasetType= 'valid')\n",
    "            \n",
    "            # supervised model instance and move to compute device\n",
    "            model = modelToFit(n_input=X.shape[1], n_output=3)\n",
    "            model.to(device);\n",
    "            ### print(f\"there are {find_no_of_trainable_params(model)} params in model\")\n",
    "\n",
    "            # optimizer function, lr schedulers and loss function\n",
    "            optimizer = getOptimizer(model)\n",
    "            scheduler = getScheduler(optimizer, dataloader_train)\n",
    "            # print(f\"optimizer={optimizer}, scheduler={scheduler}, loss_fn={criterion}\")\n",
    "\n",
    "            # train and validate single fold\n",
    "            foldResults = trainValidateOneFold(buildingName, i_fold, model, optimizer, scheduler,dataloader_train, dataloader_valid)\n",
    "            buildingTrainResults = buildingTrainResults + foldResults\n",
    "            \n",
    "            del trainTimeStamps, X_train, y_train, trainGroups\n",
    "            del validTimeStamps, X_valid, y_valid, validGroups\n",
    "            del dataloader_train, dataloader_valid, model, optimizer, scheduler\n",
    "            gc.collect()\n",
    "    \n",
    "    del timestamps, X, y, groups\n",
    "    gc.collect()\n",
    "    \n",
    "    buildingTrainResults = pd.DataFrame(buildingTrainResults)\n",
    "    buildingTrainResults['valTotalLoss'] = buildingTrainResults['valPosLoss'] + buildingTrainResults['valFloorLoss']\n",
    "    buildingTrainResults['trainTotalLoss'] = buildingTrainResults['trainPosLoss'] + buildingTrainResults['trainFloorLoss']\n",
    "    return buildingTrainResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bright-motivation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:40.227256Z",
     "iopub.status.busy": "2021-04-27T13:11:40.226722Z",
     "iopub.status.idle": "2021-04-27T13:11:40.230914Z",
     "shell.execute_reply": "2021-04-27T13:11:40.230197Z"
    },
    "papermill": {
     "duration": 0.038316,
     "end_time": "2021-04-27T13:11:40.231057",
     "exception": false,
     "start_time": "2021-04-27T13:11:40.192741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getFoldBestResultsDf(trainResults):\n",
    "    bestResults = []\n",
    "    numFolds = trainResults['fold'].nunique()\n",
    "    \n",
    "    for fold in range(numFolds):\n",
    "        foldDf = trainResults[trainResults['fold']== fold]\n",
    "        bestResults.append(foldDf.iloc[np.argmin(foldDf['valTotalLoss'].values),:])\n",
    "    \n",
    "    bestResults =pd.DataFrame(bestResults)\n",
    "    valPosLossBest = bestResults['valPosLoss'].values\n",
    "    print(f\"Best valPosLoss for all folds = {valPosLossBest}\")\n",
    "    print(f\"Mean, std ={valPosLossBest.mean()}, {valPosLossBest.std()}\")\n",
    "    return bestResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-privilege",
   "metadata": {
    "papermill": {
     "duration": 0.028415,
     "end_time": "2021-04-27T13:11:40.287842",
     "exception": false,
     "start_time": "2021-04-27T13:11:40.259427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate OOF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sexual-flash",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:40.354475Z",
     "iopub.status.busy": "2021-04-27T13:11:40.353862Z",
     "iopub.status.idle": "2021-04-27T13:11:40.357257Z",
     "shell.execute_reply": "2021-04-27T13:11:40.356824Z"
    },
    "papermill": {
     "duration": 0.041203,
     "end_time": "2021-04-27T13:11:40.357392",
     "exception": false,
     "start_time": "2021-04-27T13:11:40.316189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateOOF(modelSaveDir:str, buildingName:str, modelName:str):\n",
    "    oof_ts = []\n",
    "    oof_preds = []\n",
    "    oof_targets = []\n",
    "    oof_groups = []\n",
    "    oof_folds = []\n",
    "\n",
    "    modelPaths = sorted(glob.glob(f\"{modelSaveDir}/{buildingName}_{modelName}_fold*.pth\"))\n",
    "\n",
    "    for fold in range(len(modelPaths)):\n",
    "        # load building-model-fold checkpoint\n",
    "        checkPoint = torch.load(modelPaths[fold], map_location=torch.device(device))\n",
    "        numRows = len(checkPoint['val_ts'])\n",
    "\n",
    "        oof_ts.append(checkPoint['val_ts'])\n",
    "        oof_preds.append(checkPoint['val_preds'])\n",
    "        oof_targets.append(checkPoint['val_targets'])\n",
    "        oof_groups.append(checkPoint['val_groups'])\n",
    "        oof_folds.append([fold] * numRows)\n",
    "    \n",
    "    oof_ts = np.concatenate(oof_ts,axis=0)\n",
    "    oof_preds = np.concatenate(oof_preds,axis=0)\n",
    "    oof_targets = np.concatenate(oof_targets,axis=0)\n",
    "    oof_groups = np.concatenate(oof_groups,axis=0)\n",
    "    oof_folds = np.concatenate(oof_folds,axis=0)\n",
    "    \n",
    "    #print(oof_ts.shape, oof_preds.shape, oof_targets.shape, oof_groups.shape, oof_folds.shape)\n",
    "    oof_df = pd.DataFrame({'timestamp' : oof_ts, 'x_preds': oof_preds[:,0], 'y_preds': oof_preds[:,1],\n",
    "                       'floor_preds': oof_preds[:,2], 'x_tgt': oof_targets[:,0], 'y_tgt': oof_targets[:,1],\n",
    "                       'floor_tgt': oof_targets[:,2], 'path' : oof_groups, 'fold' : oof_folds\n",
    "                      })\n",
    "    print(f\"OOF prediction for {buildingName} site generated\")\n",
    "    return oof_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-fisher",
   "metadata": {
    "papermill": {
     "duration": 0.027694,
     "end_time": "2021-04-27T13:11:40.413612",
     "exception": false,
     "start_time": "2021-04-27T13:11:40.385918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test set prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "killing-departure",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:40.486240Z",
     "iopub.status.busy": "2021-04-27T13:11:40.484443Z",
     "iopub.status.idle": "2021-04-27T13:11:40.487102Z",
     "shell.execute_reply": "2021-04-27T13:11:40.487501Z"
    },
    "papermill": {
     "duration": 0.04584,
     "end_time": "2021-04-27T13:11:40.487656",
     "exception": false,
     "start_time": "2021-04-27T13:11:40.441816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateWiFiSubmission(modelToFit, modelSaveDir:str, buildingName:str, modelName:str):\n",
    "    modelPaths = sorted(glob.glob(f\"{modelSaveDir}/{buildingName}_{modelName}_fold*.pth\"))\n",
    "    buildingTestData = f\"{wifiFeaturesDir_test}/{buildingName}_test.pickle\"\n",
    "    with open(buildingTestData, 'rb') as inputFile:\n",
    "        testData = pickle.load(inputFile)    \n",
    "    \n",
    "    test_ts = []\n",
    "    test_fold = []\n",
    "    test_preds = []\n",
    "    test_groups = []\n",
    "\n",
    "    for fold in range(CFG.N_FOLDS):\n",
    "        ## print(f\"Fold {fold} processing\")\n",
    "        with open(f\"{modelOutputDir}/{buildingName}_stdScaler_fold{fold}.pkl\",'rb') as inputFile:\n",
    "            foldStdScalerTransform = pickle.load(inputFile)\n",
    "\n",
    "        #print(f\"Before stdscaler : testX mean = {testX.mean()}, testData std = {testX.std()}\")\n",
    "        testX = testData.iloc[:,1:-1].values  \n",
    "        testGroups = testData.iloc[:,-1].values    \n",
    "        testTimestamps = testData.iloc[:,0].values\n",
    "        \n",
    "        # transform using train Data \n",
    "        testX = foldStdScalerTransform.transform(testX)\n",
    "        ## print(f\"For fold {fold}, After stdscaler : testData mean = {testX.mean()}, testX std = {testX.std()}\")    \n",
    "\n",
    "        checkPoint = torch.load(modelPaths[fold], map_location=torch.device(device))\n",
    "        model = modelToFit(n_input=testX.shape[1], n_output=3)\n",
    "        model.to(device);\n",
    "        model.load_state_dict(checkPoint['model'])\n",
    "\n",
    "        # set model to Validate mode\n",
    "        model.eval()\n",
    "        ## test Dataset and data loaders\n",
    "        testDataset = wiFiFeaturesDataset_test(testTimestamps, testX, testGroups)\n",
    "        testDataloader = getDataLoader(testDataset, datasetType= 'test')\n",
    "\n",
    "        dataLoaderIterator = iter(testDataloader)\n",
    "        for idx in range(len(testDataloader)):\n",
    "            try:\n",
    "                ts, inputs, testGroups = next(dataLoaderIterator)\n",
    "            except StopIteration:\n",
    "                dataLoaderIterator = iter(testDataloader)\n",
    "                ts, inputs, testGroups = next(dataLoaderIterator)\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            # forward prediction\n",
    "            with torch.no_grad():    \n",
    "                y_preds = model(inputs)\n",
    "\n",
    "            # store predictions and targets to compute metrics later\n",
    "            test_ts.append(ts)\n",
    "            test_preds.append(y_preds)\n",
    "            test_groups.append(testGroups)\n",
    "        \n",
    "        test_fold.append([fold] * len(testX))\n",
    "\n",
    "        del testDataloader\n",
    "        ## torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    # concatenate to get as 1 2d array \n",
    "    test_preds = torch.cat(test_preds, 0).cpu().data.numpy() \n",
    "    test_ts = np.concatenate(test_ts, axis=0)\n",
    "    test_fold = np.concatenate(test_fold, axis=0)\n",
    "    test_groups = np.concatenate(test_groups, axis=0)\n",
    "    subm_wifi_df = pd.DataFrame({'timestamp' : test_ts, 'x_preds': test_preds[:,0], 'y_preds': test_preds[:,1],\n",
    "                                 'floor_preds': test_preds[:,2], 'path' : test_groups, 'fold' : test_fold})\n",
    "    subm_wifi_df.to_pickle(f\"{buildingName}_wifi_subm.pickle\")  \n",
    "    print(f\"Test data prediction for {buildingName} site generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-penny",
   "metadata": {
    "papermill": {
     "duration": 0.027973,
     "end_time": "2021-04-27T13:11:40.543977",
     "exception": false,
     "start_time": "2021-04-27T13:11:40.516004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compute Device as CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "latter-bidding",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:41.116246Z",
     "iopub.status.busy": "2021-04-27T13:11:41.115461Z",
     "iopub.status.idle": "2021-04-27T13:11:41.120095Z",
     "shell.execute_reply": "2021-04-27T13:11:41.120545Z"
    },
    "papermill": {
     "duration": 0.548072,
     "end_time": "2021-04-27T13:11:41.120716",
     "exception": false,
     "start_time": "2021-04-27T13:11:40.572644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Device as cpu or tpu\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-absorption",
   "metadata": {
    "papermill": {
     "duration": 0.028855,
     "end_time": "2021-04-27T13:11:41.178999",
     "exception": false,
     "start_time": "2021-04-27T13:11:41.150144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "unauthorized-insertion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:41.243073Z",
     "iopub.status.busy": "2021-04-27T13:11:41.242468Z",
     "iopub.status.idle": "2021-04-27T13:11:41.246323Z",
     "shell.execute_reply": "2021-04-27T13:11:41.246735Z"
    },
    "papermill": {
     "duration": 0.0382,
     "end_time": "2021-04-27T13:11:41.246880",
     "exception": false,
     "start_time": "2021-04-27T13:11:41.208680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for cv\n",
    "folds = GroupKFold(n_splits=CFG.N_FOLDS)\n",
    "\n",
    "# for normalizing input data\n",
    "stdScaler = StandardScaler()\n",
    "\n",
    "# scaler to handle AMP\n",
    "scaler = GradScaler()   \n",
    "\n",
    "criterion = competitionMetric\n",
    "modelToFit = wiFiFeaturesMLPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-saudi",
   "metadata": {
    "papermill": {
     "duration": 0.028547,
     "end_time": "2021-04-27T13:11:41.304210",
     "exception": false,
     "start_time": "2021-04-27T13:11:41.275663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training & Validation main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "given-dance",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-27T13:11:41.368438Z",
     "iopub.status.busy": "2021-04-27T13:11:41.367878Z",
     "iopub.status.idle": "2021-04-27T14:28:09.175232Z",
     "shell.execute_reply": "2021-04-27T14:28:09.175930Z"
    },
    "papermill": {
     "duration": 4587.842873,
     "end_time": "2021-04-27T14:28:09.176116",
     "exception": false,
     "start_time": "2021-04-27T13:11:41.333243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 sites are to be trained\n",
      "----------------------------------\n",
      "Processing data for building - 5da138b74db8ce0c98bd4774\n",
      "Building Data shapes : ((17382,), (17382, 10587), (17382, 3), (17382,))\n",
      "For Fold 0, Best validation score of 14.256637573242188 was got at epoch 7\n",
      "For Fold 1, Best validation score of 14.17503833770752 was got at epoch 15\n",
      "For Fold 2, Best validation score of 14.188374519348145 was got at epoch 9\n",
      "For Fold 3, Best validation score of 12.643767356872559 was got at epoch 117\n",
      "For Fold 4, Best validation score of 15.950362205505371 was got at epoch 115\n",
      "Best valPosLoss for all folds = [14.25663757 14.17503834 14.18837452 12.64376736 15.95036221]\n",
      "Mean, std =14.242835998535156, 1.0469402396729324\n",
      "OOF prediction for 5da138b74db8ce0c98bd4774 site generated\n",
      "Test data prediction for 5da138b74db8ce0c98bd4774 site generated\n",
      "CPU times: user 1h 32s, sys: 8min 54s, total: 1h 9min 26s\n",
      "Wall time: 1h 16min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if CFG.TRAIN == True:\n",
    "    \n",
    "    buildingPathList_train = sorted(glob.glob(f\"{wifiFeaturesDir_train}/*.pickle\"))\n",
    "    buildingPathList_train = buildingPathList_train[CFG.BUILDING_SITES_RANGE[0]: CFG.BUILDING_SITES_RANGE[1]]\n",
    "    print(f\"{len(buildingPathList_train)} sites are to be trained\")\n",
    "\n",
    "    for buildingPath_train in buildingPathList_train:\n",
    "        \n",
    "        print('----------------------------------')\n",
    "        ## get building name\n",
    "        buildingName = getBuildingName(buildingPath_train)\n",
    "        \n",
    "        ## train and validate for building data\n",
    "        buildingTrainResults = trainValidateOneBuilding(buildingPath_train, modelToFit)\n",
    "        bestResults = getFoldBestResultsDf(buildingTrainResults)\n",
    "        \n",
    "        ## generate OOF prediction for building-model combination\n",
    "        buildingOOF = generateOOF(modelOutputDir, buildingName, CFG.MODEL_NAME)\n",
    "        \n",
    "        ## prediction for test data too\n",
    "        generateWiFiSubmission(modelToFit, modelOutputDir, buildingName, CFG.MODEL_NAME)\n",
    "\n",
    "        ## save results to file\n",
    "        buildingOOF.to_pickle(f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_OOF.pickle\")\n",
    "        bestResults.to_pickle(f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_bestResults.pickle\")\n",
    "        buildingTrainResults.to_pickle(f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_trainResults.pickle\")\n",
    "        \n",
    "        ## plot building results\n",
    "        ## plotTrainingResults(buildingTrainResults, buildingName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4615.379278,
   "end_time": "2021-04-27T14:28:11.531595",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-27T13:11:16.152317",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
