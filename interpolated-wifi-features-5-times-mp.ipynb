{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wifi features\n",
    "Huge thanks to the great [wifi fearures](https://www.kaggle.com/devinanzelmo/wifi-features) notebook by [Devin Anzelmo](https://www.kaggle.com/devinanzelmo). I learned a lot from the notebook.\n",
    "I've made a small chage to the notebook and now it runs faster ~40min instead of 2-4 hours. Hope this helps some kagglers!\n",
    "\n",
    "In case you find a bug please leave comments here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_map = {\"B2\": -2, \"B1\": -1, \"F1\": 0, \"F2\": 1, \"F3\": 2, \"F4\": 3, \"F5\": 4, \"F6\": 5, \"F7\": 6, \"F8\": 7, \"F9\": 8,\n",
    "             \"1F\": 0, \"2F\": 1, \"3F\": 2, \"4F\": 3, \"5F\": 4, \"6F\": 5, \"7F\": 6, \"8F\": 7, \"9F\": 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "minCount = 1\n",
    "rssiFillerValue = -999.0\n",
    "dtFillerValue   = 1000.0\n",
    "freqFillerValue = 0\n",
    "outputDir = 'referencePublicNotebooks/wiFiFeatures'\n",
    "sampleCsvPath = 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "c09e4adb-e879-4e9f-b45f-0be2a49319b0",
    "_uuid": "864cdaf7-bc14-47d2-afd8-672ebacab5f9"
   },
   "outputs": [],
   "source": [
    "def input_dir() -> Path:\n",
    "    #return Path('/kaggle/input/indoor-location-navigation/')\n",
    "    return Path('.')\n",
    "\n",
    "def extract_wps_wifis(file: Path) -> Tuple[List[str], List[str]]:\n",
    "    wps = []\n",
    "    wifis = []\n",
    "    with open(file) as f:\n",
    "        for row in csv.reader(f, delimiter=\"\\t\", doublequote=True):\n",
    "            if row[1] == \"TYPE_WAYPOINT\":\n",
    "                # x\n",
    "                row[2] = float(row[2])  # type: ignore\n",
    "                # y\n",
    "                row[3] = float(row[3])  # type: ignore\n",
    "                wps.append(row)\n",
    "            elif row[1] == \"TYPE_WIFI\":\n",
    "                # wifi signal value\n",
    "                row[4] = int(row[4])  # type: ignore\n",
    "                wifis.append(row)\n",
    "    wps = sorted(wps, key=lambda x: x[0])  # timestamp\n",
    "    wifis = sorted(wifis, key=lambda x: x[0])  # timestamp\n",
    "    return wps, wifis\n",
    "\n",
    "\n",
    "def top_bssids(bssids: List[str], n: int) -> List[str]:\n",
    "    df = pd.DataFrame(bssids)\n",
    "    value_counts = df[0].value_counts() # type: ignore\n",
    "    return sorted(value_counts[value_counts > n].index.tolist())\n",
    "\n",
    "\n",
    "def top_bssids_for_building(input_dir: Path, building: str, n: int) -> List[str]:\n",
    "    folders = sorted(glob.glob(os.path.join(\n",
    "        input_dir, 'train/' + building+'/*')))\n",
    "    bssids = []\n",
    "    for folder in folders:\n",
    "        files = glob.glob(os.path.join(folder, \"*.txt\"))\n",
    "        for file in files:\n",
    "            _, wifis = extract_wps_wifis(Path(file))\n",
    "            bssids.extend([wifi[3] for wifi in wifis])\n",
    "\n",
    "    return top_bssids(bssids, n)\n",
    "\n",
    "\n",
    "def nearest_waypoint(timestamp: int, wps: np.ndarray) -> List[str]:\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    for wp in wps:\n",
    "        # timestamp delta\n",
    "        dist = abs(timestamp - int(wp[0]))\n",
    "        dists.append(dist)\n",
    "    nearest_index = np.argmin(dists)\n",
    "    return wps[nearest_index]\n",
    "    \"\"\"\n",
    "    wayPtTimestamps = wps[:,0].astype(np.int64)\n",
    "    waypointx = wps[:,2].astype(float)\n",
    "    waypointy = wps[:,3].astype(float)\n",
    "        \n",
    "    interpolatedWiFiAPx = np.interp(timestamp, wayPtTimestamps, waypointx)\n",
    "    interpolatedWiFiAPy = np.interp(timestamp, wayPtTimestamps, waypointy)\n",
    "    return [interpolatedWiFiAPx, interpolatedWiFiAPy]\n",
    "    \n",
    "\n",
    "\n",
    "# Note: This can have exact same rows in train. Because both wifi_group_a and \n",
    "# wifi_group_b can be nearest to a certain waypoint and wifi_group_a and wifi_group_b are the same.\n",
    "def generate_train_for_building(building_path: Path, bssids: List[str]) -> pd.Series:\n",
    "    dfs = []\n",
    "    folders = sorted(building_path.glob('*'))\n",
    "    for folder in folders:\n",
    "        files = folder.glob(\"*.txt\")\n",
    "        for file in files:\n",
    "            rows = generate_train_for_path(file, bssids)\n",
    "            dfs.extend(rows)\n",
    "    print(len(dfs))\n",
    "    \"\"\"\n",
    "    building_df = pd.concat(dfs)\n",
    "    building_df.reset_index(drop=True, inplace=True)\n",
    "    type_map = {column: int for column in bssids}\n",
    "    building_df = building_df.astype(type_map) # type: ignore\n",
    "    \"\"\"\n",
    "    building_df = pd.DataFrame(dfs)\n",
    "    return building_df\n",
    "\n",
    "\n",
    "def generate_train_for_path(path_file: Path, bssids: List[str]) -> List[Any]:\n",
    "    floor = str(path_file.parent.name)\n",
    "    wps, wifis = extract_wps_wifis(path_file)\n",
    "    wps = np.array(wps)\n",
    "    wifis_df = pd.DataFrame(wifis, columns=[\n",
    "                            'timestamp', 'type', 'ssid', 'bssid', 'rssi', 'channel', 'last_timestamp'])\n",
    "\n",
    "    # adding timestamp feature\n",
    "    wifis_df['dt'] = (wifis_df['timestamp'].astype(float) - wifis_df['last_timestamp'].astype(float)) / 1000.0\n",
    "    \n",
    "    rows = []\n",
    "    for timestamp, wifi_group in wifis_df.groupby('timestamp'):\n",
    "        timestamp = int(timestamp)\n",
    "        path = path_file.stem\n",
    "        row = generate_train_for_timestamp(\n",
    "            timestamp, wifi_group, wps, floor, path, bssids)\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def generate_train_for_timestamp(timestamp: int, wifi_group: pd.DataFrame, wps: np.ndarray, floor: str, path: str, bssids: List[str]) -> pd.DataFrame:\n",
    "    waypoint = nearest_waypoint(timestamp, wps)\n",
    "    wifi_group = wifi_group.drop_duplicates(subset='bssid')\n",
    "    \n",
    "    # feature extraction\n",
    "    tmp = wifi_group.loc[:,['bssid', 'rssi', 'dt','channel']]\n",
    "    \n",
    "    # reindex, one-hot encoding and fill nan with default value\n",
    "    row = tmp.set_index('bssid').reindex(bssids)\n",
    "    row.fillna({'rssi':rssiFillerValue, 'dt':dtFillerValue, 'channel':freqFillerValue}, inplace=True)\n",
    "    row = row.values.flatten('F').tolist()\n",
    "    \n",
    "    # append target values - x,y,floor and path\n",
    "    row.extend([waypoint[0], waypoint[1], floor_map[floor], path])\n",
    "    \n",
    "    \"\"\"\n",
    "    # generate rssi, dt and frequency features\n",
    "    tmp = wifi_group.loc[:, ['bssid','value']]  \n",
    "    dtTmp = wifi_group.loc[:, ['dt_bssid','dt']] \n",
    "    channelTmp = wifi_group.loc[:, ['channel_bssid','channel']] \n",
    "    \n",
    "    # reindex each, replace nan with default values\n",
    "    row = tmp.set_index('bssid').reindex(bssids).replace(np.nan, -999).T\n",
    "    dtRow = dtTmp.set_index('dt_bssid').reindex(dtBssids).replace(np.nan, 1000.0).T\n",
    "    channelRow = channelTmp.set_index('channel_bssid').reindex(channelBssids).replace(np.nan, 0).T\n",
    "    \n",
    "    # check if features order is correct\n",
    "    #print(np.array_equal(np.where(row.values > -500)[1], np.where(dtRow.values < 500.0)[1]), np.array_equal(np.where(row.values > -500)[1], np.where(channelRow.astype(int).values > 100)[1]) ) \n",
    "        \n",
    "    # append features horizontally\n",
    "    row[dtRow.columns] = dtRow.values\n",
    "    row[channelRow.columns] = channelRow.values\n",
    "    \n",
    "    # fill target values\n",
    "    row[\"x\"] = waypoint[0]\n",
    "    row[\"y\"] = waypoint[1]\n",
    "    row[\"f\"] = floor_map[floor]\n",
    "    row[\"path\"] = path\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return row\n",
    "\n",
    "\n",
    "def generate_target_buildings() -> List[str]:\n",
    "    ssubm = pd.read_csv(sampleCsvPath)\n",
    "    ssubm_df = ssubm[\"site_path_timestamp\"].apply(\n",
    "        lambda x: pd.Series(x.split(\"_\")))\n",
    "    buildingsList = sorted(ssubm_df[0].value_counts().index.tolist()) # type: ignore\n",
    "    return buildingsList\n",
    "\n",
    "\n",
    "def generate_one(building: str):\n",
    "    print(f\"start:{building}\")\n",
    "    building_path = input_dir() / 'train' / building\n",
    "    bssids = top_bssids_for_building(input_dir(), building, minCount)\n",
    "    train_df = generate_train_for_building(building_path, bssids)\n",
    "    print(train_df.shape)\n",
    "    train_df.to_csv(f'{outputDir}/{building}_train.csv', index=False)\n",
    "    print(f\"end:{building}\")\n",
    "\n",
    "\n",
    "def generate_train():\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    print(f\"num_cores={num_cores}\")\n",
    "    pool = Pool(num_cores)\n",
    "    pool.map(generate_one, generate_target_buildings()[0:1])\n",
    "\n",
    "\n",
    "def generate_test_one(building_df: pd.DataFrame):\n",
    "    building = building_df.iloc[0, 0]\n",
    "    print(f\"start: {building}\")\n",
    "    bssids = top_bssids_for_building(input_dir(), building, minCount) # type: ignore\n",
    "    feats = []\n",
    "    # group by path\n",
    "    for path, path_df in building_df.groupby('path'):\n",
    "        _, wifis = extract_wps_wifis(input_dir() / 'test' / f'{path}.txt')\n",
    "\n",
    "        wifi_df = pd.DataFrame(wifis)\n",
    "        wifi_points = pd.DataFrame(wifi_df.groupby(0).count().index.tolist())\n",
    "        for timepoint in path_df.iloc[:, 2].tolist():\n",
    "            deltas = (wifi_points.astype(int) - int(timepoint)).abs()\n",
    "            min_delta_idx = deltas.values.argmin()\n",
    "            wifi_block_timestamp = wifi_points.iloc[min_delta_idx].values[0]\n",
    "\n",
    "            wifi_block = wifi_df[wifi_df[0] ==\n",
    "                                 wifi_block_timestamp].drop_duplicates(subset=3)\n",
    "            feat = wifi_block.set_index(3)[4].reindex(bssids).fillna(-999)\n",
    "\n",
    "            feat['site_path_timestamp'] = f'{building}_{path}_{timepoint}'\n",
    "            feats.append(feat)\n",
    "    feature_df = pd.concat(feats, axis=1).T\n",
    "    feature_df.to_csv(f\"{building}_test.csv\", index=False)\n",
    "    print(f'end: {building}')\n",
    "\n",
    "\n",
    "def generate_test():\n",
    "    sub_df = pd.read_csv(sampleCsvPath)\n",
    "    sub_df = sub_df[\"site_path_timestamp\"].apply(\n",
    "        lambda x: pd.Series(x.split(\"_\")))\n",
    "    sub_df.columns = ['site', 'path', 'timestamp']\n",
    "\n",
    "    building_dfs = [building_df for _, building_df in sub_df.groupby('site')]\n",
    "\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    print(f\"num_cores={num_cores}\")\n",
    "    pool = Pool(num_cores)\n",
    "    pool.map(generate_test_one, building_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "buildingsList = generate_target_buildings()\n",
    "building = buildingsList[0]\n",
    "folders = sorted(Path(f\"{input_dir()}/train/{building}\").glob('*'))\n",
    "folder = folders[0]\n",
    "\n",
    "print(f\"building = {building}, floor = {folder.stem}\")\n",
    "\n",
    "bssids = top_bssids_for_building(input_dir(), building, 1) \n",
    "dtBssids = [\"dt_\"+x for x in bssids]\n",
    "channelBssids = [\"channel_\"+x for x in bssids]\n",
    "print(len(bssids))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#for path_file in sorted(files):\n",
    "files = folder.glob(\"*.txt\")\n",
    "path_file = sorted(files)[0]\n",
    "print(path_file.stem)\n",
    "floor = str(path_file.parent.name)\n",
    "wps, wifis = extract_wps_wifis(path_file)\n",
    "wps = np.array(wps)\n",
    "wifis_df = pd.DataFrame(wifis, columns=[\n",
    "                        'timestamp', 'type', 'ssid', 'bssid', 'value', 'channel', 'last_timestamp'])\n",
    "\n",
    "#wifis_df['dt_bssid'] = 'dt_' + wifis_df['bssid'].astype(str)\n",
    "#wifis_df['channel_bssid'] = 'channel_' + wifis_df['bssid'].astype(str)\n",
    "\n",
    "# adding timestamp feature\n",
    "wifis_df['dt'] = (wifis_df['timestamp'].astype(float) - wifis_df['last_timestamp'].astype(float)) / 1000.0\n",
    "\n",
    "    \n",
    "for timestamp, wifi_group in wifis_df.groupby('timestamp'):\n",
    "    timestamp = int(timestamp)\n",
    "    path = path_file.stem\n",
    "    #row = generate_train_for_timestamp(timestamp, wifi_group, wps, floor, path, bssids)\n",
    "    waypoint = nearest_waypoint(timestamp, wps)\n",
    "    wifi_group = wifi_group.drop_duplicates(subset='bssid')\n",
    "    break\n",
    "    #tmp = wifi_group.iloc[:, 3:5]  # bssid and value\n",
    "    #row = tmp.set_index('bssid').reindex(bssids).replace(np.nan, -999).T\n",
    "\n",
    "#break\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "wifi_group.head(3)\n",
    "tmp = wifi_group.loc[:,['bssid', 'value', 'dt','channel']]\n",
    "tmp.head(3)\n",
    "\n",
    "row = tmp.set_index('bssid').reindex(bssids)\n",
    "row.head(3)\n",
    "\n",
    "row.fillna({'value':rssiFillerValue, 'dt':dtFillerValue, 'channel':freqFillerValue}, inplace=True)\n",
    "row.head(3)\n",
    "\n",
    "features = row.values.flatten('F').tolist()\n",
    "print(len(features))\n",
    "\n",
    "# f1 = row['value'].notnull().index.values\n",
    "# f2 = row['dt'].notnull().index.values\n",
    "# f3 = row['channel'].notnull().index.values\n",
    "\n",
    "# np.array_equal(f1,f2)\n",
    "# np.array_equal(f1,f3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tmp = wifi_group.loc[:, ['bssid','value']]  # bssid and value\n",
    "print(tmp.shape)\n",
    "print(tmp.head(2))\n",
    "\n",
    "dtTmp = wifi_group.loc[:, ['dt_bssid','dt']]  # bssid and value\n",
    "print(dtTmp.shape)\n",
    "print(dtTmp.head(3))\n",
    "\n",
    "channelTmp = wifi_group.loc[:, ['channel_bssid','channel']]  # bssid and value\n",
    "print(channelTmp.shape)\n",
    "print(channelTmp.head(3))\n",
    "\n",
    "\n",
    "row = tmp.set_index('bssid').reindex(bssids).replace(np.nan, -999).T\n",
    "dtRow = dtTmp.set_index('dt_bssid').reindex(dtBssids).replace(np.nan, 1000.0).T\n",
    "channelRow = channelTmp.set_index('channel_bssid').reindex(channelBssids).replace(np.nan, 0).T\n",
    "\n",
    "row[dtRow.columns] = dtRow.values\n",
    "row[channelRow.columns] = channelRow.values\n",
    "\n",
    "np.array_equal(np.where(row.values > -500)[1], np.where(dtRow.values < 500.0)[1]) \n",
    "np.array_equal(np.where(row.values > -500)[1], np.where(channelRow.astype(int).values > 100)[1]) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_cores=4\n",
      "start:5a0546857ecc773753327266\n",
      "9296\n",
      "(9296, 10174)\n",
      "end:5a0546857ecc773753327266\n",
      "CPU times: user 2.18 s, sys: 65.6 ms, total: 2.24 s\n",
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
