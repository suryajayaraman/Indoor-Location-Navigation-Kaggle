{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029515,
     "end_time": "2021-05-04T00:08:40.639665",
     "exception": false,
     "start_time": "2021-05-04T00:08:40.61015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 12.382641,
     "end_time": "2021-05-04T00:08:53.050236",
     "exception": false,
     "start_time": "2021-05-04T00:08:40.667595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 4.812882,
     "end_time": "2021-05-04T00:08:57.895251",
     "exception": false,
     "start_time": "2021-05-04T00:08:53.082369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "## from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# DL library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "## from  torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# metrics calculation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# basic plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# interactive plots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import iplot\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031754,
     "end_time": "2021-05-04T00:08:58.031815",
     "exception": false,
     "start_time": "2021-05-04T00:08:58.000061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.043059,
     "end_time": "2021-05-04T00:08:58.107606",
     "exception": false,
     "start_time": "2021-05-04T00:08:58.064547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # pipeline parameters\n",
    "    SEED        = 42\n",
    "    TRAIN       = True\n",
    "    LR_FIND     = False\n",
    "    TEST        = False\n",
    "    N_FOLDS     = 2 \n",
    "    N_EPOCHS    = 2\n",
    "    TEST_BATCH_SIZE  = 128\n",
    "    TRAIN_BATCH_SIZE = 16\n",
    "    NUM_WORKERS      = 4\n",
    "    DATA_FRAC        = 1.0\n",
    "    FOLD_TO_TRAIN    = [0,1,2,3, 4] \n",
    "\n",
    "    # model parameters\n",
    "    MODEL_ARCH  = 'seq2seq'\n",
    "    MODEL_NAME  = 'ConvSeq_v1'\n",
    "    WGT_PATH    = ''\n",
    "    WGT_MODEL   = ''\n",
    "    PRINT_N_EPOCH = 2\n",
    "    RNN_TYPE = 'LSTM'\n",
    "    \n",
    "    # scheduler variables\n",
    "    MAX_LR    = 1e-1\n",
    "    MIN_LR    = 1e-6\n",
    "    SCHEDULER = 'CosineAnnealingWarmRestarts'  # ['ReduceLROnPlateau', 'None', 'OneCycleLR','CosineAnnealingLR']\n",
    "    T_0       = 5      # CosineAnnealingWarmRestarts\n",
    "    T_MULT    = 2      # CosineAnnealingWarmRestarts\n",
    "    T_MAX     = 10     # CosineAnnealingLR\n",
    "\n",
    "    # optimizer variables\n",
    "    OPTIMIZER     = 'Adam'\n",
    "    WEIGHT_DECAY  = 1e-6\n",
    "    GRD_ACC_STEPS = 1\n",
    "    MAX_GRD_NORM  = 1\n",
    "\n",
    "    BUILDING_SITES_RANGE = [0,24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.040208,
     "end_time": "2021-05-04T00:08:58.180327",
     "exception": false,
     "start_time": "2021-05-04T00:08:58.140119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "floor_map = {\"B2\": -2, \"B1\": -1, \"F1\": 0, \"F2\": 1, \"F3\": 2, \"F4\": 3, \"F5\": 4, \"F6\": 5, \"F7\": 6, \"F8\": 7, \"F9\": 8,\n",
    "             \"1F\": 0, \"2F\": 1, \"3F\": 2, \"4F\": 3, \"5F\": 4, \"6F\": 5, \"7F\": 6, \"8F\": 7, \"9F\": 8}\n",
    "\n",
    "modelOutputDir = '.'\n",
    "imuFeatures_trainPath = 'imuSeq2SeqData.pickle'\n",
    "## imuFeatures_testPath  = ''\n",
    "sampleCsvPath = 'sample_submission.csv'\n",
    "ssubm = pd.read_csv(sampleCsvPath)\n",
    "ssubm_df = ssubm[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\n",
    "\n",
    "## number of time sequences to give as input to encoder\n",
    "imuInputSequenceLength = 100\n",
    "\n",
    "## max number of time sequences in decoder\n",
    "wayPointMaxSequenceLength = 107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031519,
     "end_time": "2021-05-04T00:08:58.243421",
     "exception": false,
     "start_time": "2021-05-04T00:08:58.211902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.038906,
     "end_time": "2021-05-04T00:08:58.314336",
     "exception": false,
     "start_time": "2021-05-04T00:08:58.27543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getBuildingName(buildingDataPath):\n",
    "    return buildingDataPath.split('/')[-1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.039158,
     "end_time": "2021-05-04T00:08:58.385788",
     "exception": false,
     "start_time": "2021-05-04T00:08:58.34663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_no_of_trainable_params(model):\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.043922,
     "end_time": "2021-05-04T00:08:58.461775",
     "exception": false,
     "start_time": "2021-05-04T00:08:58.417853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(CFG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.044629,
     "end_time": "2021-05-04T00:08:58.539389",
     "exception": false,
     "start_time": "2021-05-04T00:08:58.49476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getSeq2SeqFeatures(imuDataPath):\n",
    "    \"\"\" \"\"\"\n",
    "    # read features from file \n",
    "    with open(imuDataPath, 'rb') as inputFile:\n",
    "        imuData = pickle.load(inputFile)        \n",
    "    return imuData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:08:58.755959Z",
     "iopub.status.busy": "2021-05-04T00:08:58.754803Z",
     "iopub.status.idle": "2021-05-04T00:08:58.757802Z",
     "shell.execute_reply": "2021-05-04T00:08:58.7573Z"
    },
    "papermill": {
     "duration": 0.042066,
     "end_time": "2021-05-04T00:08:58.757921",
     "exception": false,
     "start_time": "2021-05-04T00:08:58.715855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def competitionMetric(preds, targets):\n",
    "    \"\"\" The metric used in this competition \"\"\"\n",
    "    # position error\n",
    "    meanPosPredictionError = torch.mean(torch.sqrt(\n",
    "                             torch.pow((preds[:,0] - targets[:,0]), 2) + \n",
    "                             torch.pow((preds[:,1] - targets[:,1]), 2)))\n",
    "    \n",
    "    ## floor prediction error\n",
    "    if((preds.shape[1] == 3) and (targets.shape[1] ==3)):\n",
    "        meanFloorPredictionError = torch.mean(15 * torch.abs(preds[:,2] - targets[:,2]))\n",
    "    else:\n",
    "        meanFloorPredictionError = 0.0\n",
    "    return meanPosPredictionError, meanFloorPredictionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:08:58.831538Z",
     "iopub.status.busy": "2021-05-04T00:08:58.830596Z",
     "iopub.status.idle": "2021-05-04T00:08:58.833546Z",
     "shell.execute_reply": "2021-05-04T00:08:58.83298Z"
    },
    "papermill": {
     "duration": 0.042186,
     "end_time": "2021-05-04T00:08:58.833683",
     "exception": false,
     "start_time": "2021-05-04T00:08:58.791497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getOptimizer(model : nn.Module):    \n",
    "    if CFG.OPTIMIZER == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), weight_decay=CFG.WEIGHT_DECAY, lr=CFG.MAX_LR)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), weight_decay=CFG.WEIGHT_DECAY, lr=CFG.MAX_LR, momentum=0.9)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:08:58.909606Z",
     "iopub.status.busy": "2021-05-04T00:08:58.908627Z",
     "iopub.status.idle": "2021-05-04T00:08:58.911344Z",
     "shell.execute_reply": "2021-05-04T00:08:58.91091Z"
    },
    "papermill": {
     "duration": 0.043278,
     "end_time": "2021-05-04T00:08:58.911468",
     "exception": false,
     "start_time": "2021-05-04T00:08:58.86819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getScheduler(optimizer, dataloader_train):\n",
    "    if CFG.SCHEDULER == 'OneCycleLR':\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr= CFG.MAX_LR, epochs = CFG.N_EPOCHS, \n",
    "                          steps_per_epoch = len(dataloader_train), pct_start=0.25, div_factor=10, anneal_strategy='cos')\n",
    "    elif CFG.SCHEDULER == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=CFG.T_MULT, eta_min=CFG.MIN_LR, last_epoch=-1)\n",
    "    elif CFG.SCHEDULER == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_MAX * len(dataloader_train), eta_min=CFG.MIN_LR, last_epoch=-1)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:08:58.983966Z",
     "iopub.status.busy": "2021-05-04T00:08:58.983028Z",
     "iopub.status.idle": "2021-05-04T00:08:58.985854Z",
     "shell.execute_reply": "2021-05-04T00:08:58.985366Z"
    },
    "papermill": {
     "duration": 0.041548,
     "end_time": "2021-05-04T00:08:58.985971",
     "exception": false,
     "start_time": "2021-05-04T00:08:58.944423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDataLoader(dataset, datasetType : str):\n",
    "    if datasetType == 'train':\n",
    "        batchSize = CFG.TRAIN_BATCH_SIZE\n",
    "        shuffleDataset = True\n",
    "    else:\n",
    "        batchSize = CFG.TEST_BATCH_SIZE\n",
    "        shuffleDataset = False\n",
    "    \n",
    "    dataLoader = DataLoader(dataset, batch_size= batchSize, shuffle=shuffleDataset,\n",
    "                            num_workers=CFG.NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "    return dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:08:59.060996Z",
     "iopub.status.busy": "2021-05-04T00:08:59.060258Z",
     "iopub.status.idle": "2021-05-04T00:08:59.062558Z",
     "shell.execute_reply": "2021-05-04T00:08:59.063Z"
    },
    "papermill": {
     "duration": 0.043986,
     "end_time": "2021-05-04T00:08:59.063149",
     "exception": false,
     "start_time": "2021-05-04T00:08:59.019163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotTrainingResults(resultsDf, buildingName):\n",
    "    # subplot to plot\n",
    "    fig = make_subplots(rows=1, cols=1)\n",
    "    colors = [ ('#d32f2f', '#ef5350'), ('#303f9f', '#5c6bc0'), ('#00796b', '#26a69a'),\n",
    "                ('#fbc02d', '#ffeb3b'), ('#5d4037', '#8d6e63')]\n",
    "\n",
    "    # find number of folds input df\n",
    "    numberOfFolds = resultsDf['fold'].nunique()\n",
    "    \n",
    "    # iterate through folds and plot\n",
    "    for i in range(numberOfFolds):\n",
    "        data = resultsDf[resultsDf['fold'] == i]\n",
    "        fig.add_trace(go.Scatter(x=data['epoch'].values, y=data['trainPosLoss'].values,\n",
    "                                mode='lines', visible='legendonly' if i > 0 else True,\n",
    "                                line=dict(color=colors[i][0], width=2),\n",
    "                                name='{}-trainPossLoss-Fold{}'.format(buildingName, i)),row=1, col=1)\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=data['epoch'], y=data['valPosLoss'].values,\n",
    "                                 mode='lines+markers', visible='legendonly' if i > 0 else True,\n",
    "                                 line=dict(color=colors[i][1], width=2),\n",
    "                                 name='{}-valPosLoss-Fold{}'.format(buildingName,i)),row=1, col=1)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032789,
     "end_time": "2021-05-04T00:08:59.128673",
     "exception": false,
     "start_time": "2021-05-04T00:08:59.095884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.041938,
     "end_time": "2021-05-04T00:08:59.203748",
     "exception": false,
     "start_time": "2021-05-04T00:08:59.16181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class imuFeaturesDataset_train(Dataset):\n",
    "    def __init__(self, imuData):\n",
    "        self.imuData = imuData \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ## output shape is (imuInputSequenceLength, 7)\n",
    "        ## where 7 indicates numbre of features - [ts, lin_ax, lin_ay, gz, roll, pitch, yaw]\n",
    "        ## imuInputSequenceLength is max input sequence length\n",
    "        encoderData  = self.imuData['encoderData'][index].transpose()\n",
    "        encoderData  = torch.from_numpy(encoderData)\n",
    "\n",
    "        ## output shape is (wayPointMaxSequenceLength, 3)\n",
    "        ## where 3 indicates numbere of targets - [x,y,floor]\n",
    "        decoderData  = self.imuData['decoderData'][index]  ## .transpose()\n",
    "        decoderData  = torch.from_numpy(decoderData)\n",
    "\n",
    "        ## output shape is (1, wayPointMaxSequenceLength)\n",
    "        ## where 1 indicates inference Ts - decoder input\n",
    "        inferenceTs  = self.imuData['inferenceTsList'][index]\n",
    "        inferenceTs  = torch.from_numpy(inferenceTs)\n",
    "        \n",
    "        ## path name\n",
    "        pathName = self.imuData['path'][index]\n",
    "        ## number of waypoints\n",
    "        numWayPoints = self.imuData['numWayPoints'][index].astype(np.int16)\n",
    "\n",
    "        return encoderData, decoderData, inferenceTs, pathName, numWayPoints\n",
    "     \n",
    "    def __len__ (self):\n",
    "        return len(self.imuData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034297,
     "end_time": "2021-05-04T00:08:59.346779",
     "exception": false,
     "start_time": "2021-05-04T00:08:59.312482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convolution seq2seq Model classes - Encoder, Decoder and Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, inputSize, embSize, hidSize, nLayers, \\\n",
    "                 kernelSize, dropout, device, maxSeqLen = 100):\n",
    "        \n",
    "        super().__init__()\n",
    "        assert kernelSize % 2 == 1, \"Kernel size must be odd!\"\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        ## store variables\n",
    "        self.encFeatSize = inputSize\n",
    "        self.encEmbSize  = embSize\n",
    "        self.encHidSize  = hidSize\n",
    "        self.nLayers     = nLayers\n",
    "        self.kernelSize  = kernelSize\n",
    "        self.device      = device\n",
    "        self.encSeqLen   = maxSeqLen\n",
    "        \n",
    "        ## encoder layers\n",
    "        self.tokEmbedding = nn.Linear(self.encFeatSize, self.encEmbSize)\n",
    "        self.posEmbedding = nn.Embedding(self.encSeqLen, self.encEmbSize)\n",
    "        self.emb2hid = nn.Linear(self.encEmbSize, self.encHidSize)\n",
    "        self.hid2emb = nn.Linear(self.encHidSize, self.encEmbSize)\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = self.encHidSize, out_channels = 2 * self.encHidSize, \\\n",
    "                                    kernel_size = self.kernelSize,  padding = (self.kernelSize - 1) // 2)\n",
    "                                    for _ in range(self.nLayers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        ## src = [batchSize, encSeqLen, encFeatSize]\n",
    "        batchSize = src.shape[0]\n",
    "        \n",
    "        ## 1. create position tensor\n",
    "        ## pos = [0, 1, 2, 3, ..., encSeqLen - 1], repeated for batchSize, along rows\n",
    "        ## pos = [batchSize, encSeqLen]\n",
    "        pos = torch.arange(0, self.encSeqLen).unsqueeze(0).repeat(batchSize, 1).to(self.device)\n",
    "              \n",
    "        ## 2. embed tokens and positions\n",
    "        ## tokEmbedded = posEmbedded = [batchSize, encSeqLen, encEmbSize]        \n",
    "        tokEmbedded = self.tokEmbedding(src)\n",
    "        posEmbedded = self.posEmbedding(pos)\n",
    "        \n",
    "        ## 3. combine embeddings by elementwise summing\n",
    "        ## embedded = [batchSize, encSeqLen, encEmbSize]\n",
    "        embedded = self.dropout(tokEmbedded + posEmbedded)\n",
    "        \n",
    "        ## 4. pass embedded through linear layer to convert from emb dim to hid dim\n",
    "        ## convInput = [batchSize, encSeqLen, encHidSize]\n",
    "        convInput = self.emb2hid(embedded)\n",
    "        \n",
    "        ## 5. permute for convolutional layer\n",
    "        ## convInput = [batchSize, encHidSize, encSeqLen]\n",
    "        convInput = convInput.permute(0, 2, 1) \n",
    "        \n",
    "        ## 6. begin convolutional blocks..., repeate for nLayers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            ## 7. pass through convolutional layer\n",
    "            ## conved = [batchSize, 2*encHidSize, encSeqLen]\n",
    "            conved = conv(self.dropout(convInput))\n",
    "\n",
    "            ## 8. pass through GLU activation function\n",
    "            ## conved = [batchSize, encHidSize, encSeqLen]\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            ## 9. apply residual connection\n",
    "            ## conved = [batchSize, encHidSize, encSeqLen]\n",
    "            conved = (conved + convInput) * self.scale\n",
    "\n",
    "            ## 10. set convInput to conved for next loop iteration\n",
    "            convInput = conved\n",
    "        \n",
    "        ## ...end convolutional blocks\n",
    "        \n",
    "        ## 11. permute and convert back to emb dim\n",
    "        ## conved = [batchSize, encSeqLen, encEmbSize]\n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        ## 12. elementwise sum output (conved) and input (embedded) to be used for attention\n",
    "        ## combined = [batchSize, encSeqLen, encEmbSize]\n",
    "        combined = (conved + embedded) * self.scale\n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, inputSize, embSize, hidSize, outputSize, nLayers, \\\n",
    "                 kernelSize, dropout, fillerValue, device, maxSeqLen = 107):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        ## store vairables\n",
    "        self.decFeatSize = inputSize\n",
    "        self.decEmbSize  = embSize\n",
    "        self.decHidSize  = hidSize\n",
    "        self.decOutSize  = outputSize        \n",
    "        self.nLayers     = nLayers\n",
    "        self.kernelSize  = kernelSize\n",
    "        self.fillerValue = fillerValue\n",
    "        self.device      = device        \n",
    "        self.decSeqLen   = maxSeqLen\n",
    "        \n",
    "        \n",
    "        ## decoder layers\n",
    "        self.tokEmbedding = nn.Linear(self.decFeatSize, self.decEmbSize)\n",
    "        self.posEmbedding = nn.Embedding(self.decSeqLen, self.decEmbSize)\n",
    "        self.emb2hid = nn.Linear(self.decEmbSize, self.decHidSize)\n",
    "        self.hid2emb = nn.Linear(self.decHidSize, self.decEmbSize)\n",
    "        \n",
    "        ## attention specific layers\n",
    "        self.attnHid2emb = nn.Linear(self.decHidSize, self.decEmbSize)\n",
    "        self.attnEmb2hid = nn.Linear(self.decEmbSize, self.decHidSize)\n",
    "        self.fcOut = nn.Linear(self.decEmbSize, self.decOutSize)\n",
    "\n",
    "        ## convolution block layers\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = self.decHidSize, out_channels = 2 * self.decHidSize, \\\n",
    "                                    kernel_size = self.kernelSize) for _ in range(self.nLayers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "      \n",
    "    def calculateAttention(self, embedded, conved, encoderConved, encoderCombined):\n",
    "        \n",
    "        ## embedded = [batchSize, decSeqLen, decEmbSize]\n",
    "        ## conved = [batchSize, decHidSize, decSeqLen]\n",
    "        ## encoderConved = encoderCombined = [batchSize, encSeqLen, decEmbSize]\n",
    "        \n",
    "        ## a. permute and convert back to emb dim\n",
    "        ## convedEmb = [batchSize, decSeqLen, decEmbSize]\n",
    "        convedEmb = self.attnHid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        ## b. residual connection\n",
    "        ## combined = [batchSize, decSeqLen, decEmbSize]\n",
    "        combined = (convedEmb + embedded) * self.scale\n",
    "        \n",
    "        ## c. Energy matrix calculation\n",
    "        ## energy = [batchSize, decSeqLen, encSeqLen]\n",
    "        energy = torch.matmul(combined, encoderConved.permute(0, 2, 1))\n",
    "        \n",
    "        ## d. attention is softmax of energy\n",
    "        ## attention = [batchSize, decSeqLen, encSeqLen]\n",
    "        attention = F.softmax(energy, dim=2)\n",
    "        \n",
    "        ## e. attention over encoded states \n",
    "        ## attendedEncoding = [batchSize, decSeqLen, decEmbSize]\n",
    "        attendedEncoding = torch.matmul(attention, encoderCombined)\n",
    "        \n",
    "        ## f. convert from emb dim -> hid dim\n",
    "        ## attendedEncoding = [batchSize, decSeqLen, decHidSize]\n",
    "        attendedEncoding = self.attnEmb2hid(attendedEncoding)\n",
    "        \n",
    "        ## g. apply residual connection with decoder token \n",
    "        ## attendedCombined = [batch size, decHidSize, decSeqLen]\n",
    "        attendedCombined = (conved + attendedEncoding.permute(0, 2, 1)) * self.scale\n",
    "        return attention, attendedCombined\n",
    "        \n",
    "    \n",
    "    def forward(self, trg, encoderConved, encoderCombined):\n",
    "        ## trg = [batchSize, decSeqLen]\n",
    "        ## encoderConved = encoderCombined = [batchSize, encSeqLen, decEmbSize]\n",
    "        batchSize = trg.shape[0]\n",
    "            \n",
    "        ## 1. create position tensor\n",
    "        ## pos = [0, 1, 2, 3, ..., decSeqLen - 1], repeated for batchSize, along rows\n",
    "        ## pos = [batchSize, decSeqLen]        \n",
    "        pos = torch.arange(0, self.decSeqLen).unsqueeze(0).repeat(batchSize, 1).to(self.device)\n",
    "        \n",
    "        ## 2. embed tokens and positions\n",
    "        ## tokEmbedded = posEmbedded = [batchSize, decSeqLen, decEmbSize] \n",
    "        trg = trg.unsqueeze(2)\n",
    "        tokEmbedded = self.tokEmbedding(trg)\n",
    "        posEmbedded = self.posEmbedding(pos)\n",
    "        \n",
    "        ## 3. combine embeddings by elementwise summing\n",
    "        ## embedded = [batchSize, decSeqLen, decEmbSize]\n",
    "        embedded = self.dropout(tokEmbedded + posEmbedded)\n",
    "        \n",
    "        ## 4. pass embedded through linear layer to convert from emb dim to hid dim\n",
    "        ## convInput = [batchSize, decSeqLen, decHidSize]\n",
    "        convInput = self.emb2hid(embedded)\n",
    "        \n",
    "        ## 5. permute for convolutional layer\n",
    "        ## convInput = [batchSize, decHidSize, decSeqLen]\n",
    "        convInput = convInput.permute(0, 2, 1)         \n",
    "        padding = torch.zeros(batchSize, self.decHidSize, self.kernelSize - 1).fill_(\\\n",
    "                       self.fillerValue).to(self.device)\n",
    "        \n",
    "        ## 6. begin of convolution blocks \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            ## 7. apply dropout\n",
    "            convInput = self.dropout(convInput)\n",
    "        \n",
    "            ## 8. need to pad so decoder can't \"cheat\"\n",
    "            ## paddedConvInput = [batchSize, decHidSize, decSeqLen + kernelSize - 1]\n",
    "            paddedConvInput = torch.cat((padding, convInput), dim = 2)\n",
    "        \n",
    "            ## 9. pass through convolutional layer\n",
    "            ## conved = [batchSize, 2 * decHidSize, decSeqLen]\n",
    "            conved = conv(paddedConvInput)\n",
    "\n",
    "            ## 10. pass through GLU activation function\n",
    "            ## conved = [batchSize, decHidSize, decSeqLen]\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            ## 11. calculate attention\n",
    "            ## attention = [batchSize, decSeqLen, encSeqLen]\n",
    "            ## conved = [batch size, decHidSize, decSeqLen]\n",
    "            attention, conved = self.calculateAttention(embedded, conved, \n",
    "                                encoderConved, encoderCombined)\n",
    "            \n",
    "            ## 12. apply residual connection\n",
    "            ## conved = [batchSize, decHidSize, decSeqLen]\n",
    "            conved = (conved + convInput) * self.scale\n",
    "                        \n",
    "            ## 13. set convInput to conved for next loop iteration\n",
    "            convInput = conved\n",
    "            \n",
    "        ## 14. convert to decEmbSize\n",
    "        #conved = [batchSize, decSeqLen, decEmbSize]\n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "         \n",
    "        ## 15. linear layer to output dimension\n",
    "        ## output = [batchSize, decSeqLen, decOutSize]\n",
    "        output = self.fcOut(self.dropout(conved))\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        ## src = [batchSize, encSeqLen, encFeatSize]\n",
    "        ## trg = [batchSize, decSeqLen]\n",
    "        \n",
    "        ## 1. calculate z^u (encoderConved) and (z^u + e) (encoderCombined)\n",
    "        ## encoderConved is output from final encoder conv. block\n",
    "        ## encoderCombined = encoderConved + src embedding + positional embeddings (all elementwise)\n",
    "        ## encoderConved = encoderCombined = [batchSize, encSeqLen, decEmbSize]\n",
    "        encoderConved, encoderCombined = self.encoder(src)\n",
    "                    \n",
    "        ## 2. calculate predictions of next words\n",
    "        ## output is a batch of predictions for each input in the decoder\n",
    "        ## attention a batch of attention scores across the encoder sequence for \n",
    "        ## each timestamp in the decoder\n",
    "        ## output = [batchSize, decSeqLen, decOutSize]\n",
    "        ## attention = [batchSize, decSeqLen, encSeqLen]\n",
    "        output, attention = self.decoder(trg, encoderConved, encoderCombined)        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Device as cpu or tpu\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10877, 6)\n"
     ]
    }
   ],
   "source": [
    "data = getSeq2SeqFeatures(imuFeatures_trainPath)\n",
    "print(data.shape)\n",
    "## data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = imuFeaturesDataset_train(data)\n",
    "trainDataLoader = getDataLoader(trainDataset, datasetType='train')\n",
    "batch = next(iter(trainDataLoader))\n",
    "encoderInput, decoderOutput, decoderInput, pathName, _ = batch\n",
    "encoderInput  = encoderInput.to(device)\n",
    "decoderOutput = decoderOutput.to(device)\n",
    "decoderInput  = decoderInput.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 100, 7]), torch.Size([16, 107, 3]), torch.Size([16, 107]), 16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoderInput.shape, decoderOutput.shape, decoderInput.shape, len(pathName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 7\n",
    "DECODER_INPUT_DIM = 1\n",
    "OUTPUT_DIM = 3\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "HID_DIM = 64\n",
    "N_LAYERS = 5\n",
    "KERNEL_SIZE = 3\n",
    "FILLER_VALUE = 0.0\n",
    "ENC_DROPOUT = 0.4\n",
    "DEC_DROPOUT = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, KERNEL_SIZE, \\\n",
    "              ENC_DROPOUT, device, imuInputSequenceLength).double()\n",
    "dec = Decoder(DECODER_INPUT_DIM, DEC_EMB_DIM, HID_DIM, OUTPUT_DIM, \\\n",
    "              N_LAYERS, KERNEL_SIZE, DEC_DROPOUT, FILLER_VALUE, device, wayPointMaxSequenceLength).double()\n",
    "model = Seq2Seq(enc, dec)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, _ = model(encoderInput, decoderInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 107, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 107, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoderOutput.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034217,
     "end_time": "2021-05-04T00:08:59.741822",
     "exception": false,
     "start_time": "2021-05-04T00:08:59.707605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lr range finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:08:59.81694Z",
     "iopub.status.busy": "2021-05-04T00:08:59.816043Z",
     "iopub.status.idle": "2021-05-04T00:08:59.818676Z",
     "shell.execute_reply": "2021-05-04T00:08:59.818173Z"
    },
    "papermill": {
     "duration": 0.042515,
     "end_time": "2021-05-04T00:08:59.818806",
     "exception": false,
     "start_time": "2021-05-04T00:08:59.776291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_lr_finder_results(lr_finder): \n",
    "    # Create subplot grid\n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    # layout ={'title': 'Lr_finder_result'}\n",
    "    \n",
    "    # Create a line (trace) for the lr vs loss, gradient of loss\n",
    "    trace0 = go.Scatter(x=lr_finder['log_lr'], y=lr_finder['smooth_loss'],name='log_lr vs smooth_loss')\n",
    "    trace1 = go.Scatter(x=lr_finder['log_lr'], y=lr_finder['grad_loss'],name='log_lr vs loss gradient')\n",
    "\n",
    "    # Add subplot trace & assign to each grid\n",
    "    fig.add_trace(trace0, row=1, col=1);\n",
    "    fig.add_trace(trace1, row=1, col=2);\n",
    "    iplot(fig, show_link=False)\n",
    "    #fig.write_html(CFG.MODEL_NAME + '_lr_find.html');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:08:59.901965Z",
     "iopub.status.busy": "2021-05-04T00:08:59.901025Z",
     "iopub.status.idle": "2021-05-04T00:08:59.90342Z",
     "shell.execute_reply": "2021-05-04T00:08:59.903853Z"
    },
    "papermill": {
     "duration": 0.051403,
     "end_time": "2021-05-04T00:08:59.904019",
     "exception": false,
     "start_time": "2021-05-04T00:08:59.852616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_lr(model, optimizer, data_loader, init_value = 1e-8, final_value=100.0, beta = 0.98, num_batches = 200):\n",
    "    assert(num_batches > 0)\n",
    "    mult = (final_value / init_value) ** (1/num_batches)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    batch_num = 0\n",
    "    avg_loss = 0.0\n",
    "    best_loss = 0.0\n",
    "    smooth_losses = []\n",
    "    raw_losses = []\n",
    "    log_lrs = []\n",
    "    dataloader_it = iter(data_loader)\n",
    "    progress_bar = tqdm(range(num_batches))                \n",
    "        \n",
    "    for idx in progress_bar:\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            _, inputs, targets = next(dataloader_it)\n",
    "            #print(images.shape)\n",
    "        except:\n",
    "            dataloader_it = iter(data_loader)\n",
    "            _, inputs, targets = next(dataloader_it)\n",
    "\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # handle exception in criterion\n",
    "        try:\n",
    "            # Forward pass\n",
    "            y_preds = model(inputs)\n",
    "            posLoss, floorLoss = criterion(y_preds, targets)\n",
    "            loss = posLoss + floorLoss\n",
    "        except:\n",
    "            if len(smooth_losses) > 1:\n",
    "                grad_loss = np.gradient(smooth_losses)\n",
    "            else:\n",
    "                grad_loss = 0.0\n",
    "            lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                                 'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "            return lr_finder_results \n",
    "                    \n",
    "        #Compute the smoothed loss\n",
    "        avg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        \n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 50 * best_loss:\n",
    "            if len(smooth_losses) > 1:\n",
    "                grad_loss = np.gradient(smooth_losses)\n",
    "            else:\n",
    "                grad_loss = 0.0\n",
    "            lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                                 'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "            return lr_finder_results\n",
    "        \n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num==1:\n",
    "            best_loss = smoothed_loss\n",
    "        \n",
    "        #Store the values\n",
    "        raw_losses.append(loss.item())\n",
    "        smooth_losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print info\n",
    "        progress_bar.set_description(f\"loss:{loss.item()},smoothLoss: {smoothed_loss},lr:{lr}\")\n",
    "\n",
    "        #Update the lr for the next step\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "    \n",
    "    grad_loss = np.gradient(smooth_losses)\n",
    "    lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                         'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "    return lr_finder_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:08:59.979395Z",
     "iopub.status.busy": "2021-05-04T00:08:59.978495Z",
     "iopub.status.idle": "2021-05-04T00:08:59.980841Z",
     "shell.execute_reply": "2021-05-04T00:08:59.981217Z"
    },
    "papermill": {
     "duration": 0.043117,
     "end_time": "2021-05-04T00:08:59.98136",
     "exception": false,
     "start_time": "2021-05-04T00:08:59.938243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.LR_FIND == True:\n",
    "    # create dataset instance\n",
    "    tempTs, tempX, tempY,_ = getBuildingData(buildingCsvPath=buildingsList[0])\n",
    "    tempX = stdScaler.fit_transform(tempX)\n",
    "    tempTrainDataset = wiFiFeaturesDataset(tempTs, tempX, tempY)\n",
    "    tempTrainDataloader = DataLoader(tempTrainDataset, batch_size= CFG.TRAIN_BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=CFG.NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "    \n",
    "    # create model instance   \n",
    "    model = wiFiFeaturesMLPModel(n_input=tempX.shape[1], n_output=3)\n",
    "    model.to(device);\n",
    "    \n",
    "    # optimizer function, lr schedulers and loss function\n",
    "    optimizer = getOptimizer(model)\n",
    "    lrFinderResults = find_lr(model, optimizer, tempTrainDataloader)\n",
    "    plot_lr_finder_results(lrFinderResults)\n",
    "    del tempX, tempY, tempTrainDataset, tempTrainDataloader, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034429,
     "end_time": "2021-05-04T00:09:00.051211",
     "exception": false,
     "start_time": "2021-05-04T00:09:00.016782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train & Validate helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:09:00.130842Z",
     "iopub.status.busy": "2021-05-04T00:09:00.129891Z",
     "iopub.status.idle": "2021-05-04T00:09:00.132801Z",
     "shell.execute_reply": "2021-05-04T00:09:00.132231Z"
    },
    "papermill": {
     "duration": 0.047314,
     "end_time": "2021-05-04T00:09:00.132939",
     "exception": false,
     "start_time": "2021-05-04T00:09:00.085625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validateModel(model, validationDataloader):\n",
    "    # placeholders to store output\n",
    "    val_ts = []\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    val_groups = []\n",
    "\n",
    "    # set model to Validate mode\n",
    "    model.eval()\n",
    "    dataLoaderIterator = iter(validationDataloader)\n",
    "\n",
    "    for idx in range(len(validationDataloader)):\n",
    "        try:\n",
    "            ts, inputs, targets, valGroups = next(dataLoaderIterator)\n",
    "        except StopIteration:\n",
    "            dataLoaderIterator = iter(validationDataloader)\n",
    "            ts, inputs, targets, valGroups = next(dataLoaderIterator)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device) \n",
    "\n",
    "        # forward prediction\n",
    "        with torch.no_grad():    \n",
    "            y_preds = model(inputs)\n",
    "\n",
    "        # store predictions and targets to compute metrics later\n",
    "        val_ts.append(ts)\n",
    "        val_preds.append(y_preds)\n",
    "        val_targets.append(targets)\n",
    "        val_groups.append(valGroups)\n",
    "\n",
    "    # concatenate to get as 1 2d array and find total loss  \n",
    "    val_preds = torch.cat(val_preds, 0)\n",
    "    val_targets = torch.cat(val_targets, 0)\n",
    "    valPosLoss, valFloorLoss = criterion(val_preds, val_targets)\n",
    "    valScore = valPosLoss + valFloorLoss\n",
    "\n",
    "    # np array concatenation\n",
    "    val_ts = np.concatenate(val_ts, axis=0)\n",
    "    val_groups = np.concatenate(val_groups, axis=0)\n",
    "    \n",
    "    # store results\n",
    "    validationResults = {'valPosLoss': valPosLoss.item() , 'valFloorLoss': valFloorLoss.item(),\\\n",
    "                         'val_ts': val_ts, 'val_groups': val_groups,\n",
    "                         'val_preds'  :val_preds.cpu().data.numpy(), \n",
    "                         'val_targets':val_targets.cpu().data.numpy(),\n",
    "                         }\n",
    "    return validationResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:09:00.215572Z",
     "iopub.status.busy": "2021-05-04T00:09:00.214502Z",
     "iopub.status.idle": "2021-05-04T00:09:00.217045Z",
     "shell.execute_reply": "2021-05-04T00:09:00.217538Z"
    },
    "papermill": {
     "duration": 0.050399,
     "end_time": "2021-05-04T00:09:00.217682",
     "exception": false,
     "start_time": "2021-05-04T00:09:00.167283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainValidateOneFold(buildingName, i_fold, model, optimizer, scheduler, dataloader_train, dataloader_valid):\n",
    "    trainFoldResults = []\n",
    "    bestValScore = np.inf\n",
    "    bestEpoch = 0\n",
    "\n",
    "    ## for epoch in tqdm(range(CFG.N_EPOCHS)):\n",
    "    for epoch in range(CFG.N_EPOCHS):\n",
    "        #print('Epoch {}/{}'.format(epoch + 1, CFG.N_EPOCHS))\n",
    "        model.train()\n",
    "        trainPosLoss = 0.0\n",
    "        trainFloorLoss = 0.0\n",
    "\n",
    "        # training iterator\n",
    "        tr_iterator = iter(dataloader_train)\n",
    "\n",
    "        for idx in range(len(dataloader_train)):\n",
    "            try:\n",
    "                _, inputs, targets, _ = next(tr_iterator)\n",
    "            except StopIteration:\n",
    "                tr_iterator = iter(dataloader_train)\n",
    "                _, inputs, targets, _ = next(tr_iterator)\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)  \n",
    "\n",
    "            # builtin package to handle automatic mixed precision\n",
    "            ## with autocast():\n",
    "            # Forward pass\n",
    "            y_preds = model(inputs)   \n",
    "            posLoss, floorLoss = criterion(y_preds, targets)\n",
    "            loss = posLoss + floorLoss\n",
    "\n",
    "            # Backward pass\n",
    "            ## scaler.scale(loss).backward()        \n",
    "            ## scaler.step(optimizer)\n",
    "            ## scaler.update()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            # log the necessary losses\n",
    "            trainPosLoss   += posLoss.item()\n",
    "            trainFloorLoss += floorLoss.item()\n",
    "\n",
    "            if scheduler is not None: \n",
    "                if CFG.SCHEDULER == 'CosineAnnealingWarmRestarts':\n",
    "                    scheduler.step(epoch + idx / len(dataloader_train)) \n",
    "                # onecyle lr scheduler / CosineAnnealingLR scheduler\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "                    \n",
    "        # Validate\n",
    "        foldValidationResults = validateModel(model, dataloader_valid)\n",
    "         \n",
    "        # store results\n",
    "        trainFoldResults.append({ 'fold': i_fold, 'epoch': epoch, \n",
    "                                  'trainPosLoss': trainPosLoss / len(dataloader_train), \n",
    "                                  'trainFloorLoss': trainFloorLoss / len(dataloader_train), \n",
    "                                  'valPosLoss'  : foldValidationResults['valPosLoss'] , \n",
    "                                  'valFloorLoss': foldValidationResults['valFloorLoss']})\n",
    "        \n",
    "        valScore = foldValidationResults['valPosLoss'] # + foldVal['valFloorLoss']\n",
    "        ## print(f'fold = {i_fold}, epoch = {epoch}, valscore = {valScore}')\n",
    "        # save best models        \n",
    "        if(valScore < bestValScore):\n",
    "            # reset variables\n",
    "            bestValScore = valScore\n",
    "            bestEpoch = epoch\n",
    "\n",
    "            # save model weights\n",
    "            torch.save({'model': model.state_dict(), 'val_ts' : foldValidationResults['val_ts'], \n",
    "                        'val_preds':foldValidationResults['val_preds'], \n",
    "                        'val_targets':foldValidationResults['val_targets'],\n",
    "                        'val_groups' : foldValidationResults['val_groups']}, \n",
    "                        f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_fold{i_fold}_best.pth\")\n",
    "\n",
    "    print(f\"For Fold {i_fold}, Best position validation score of {bestValScore} was got at epoch {bestEpoch}\") \n",
    "    return trainFoldResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:09:00.309395Z",
     "iopub.status.busy": "2021-05-04T00:09:00.308451Z",
     "iopub.status.idle": "2021-05-04T00:09:00.311276Z",
     "shell.execute_reply": "2021-05-04T00:09:00.31078Z"
    },
    "papermill": {
     "duration": 0.049422,
     "end_time": "2021-05-04T00:09:00.31139",
     "exception": false,
     "start_time": "2021-05-04T00:09:00.261968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainValidateOneBuilding(buildingDataPath, modelToFit):\n",
    "    # placeholder to store results\n",
    "    buildingTrainResults = []\n",
    "    \n",
    "    buildingName = getBuildingName(buildingDataPath)\n",
    "    print(f\"Processing data for building - {buildingName}\")\n",
    "    timestamps, X, y, groups = getBuildingData(buildingDataPath)\n",
    "    print(f\"Building Data shapes : {timestamps.shape, X.shape, y.shape, groups.shape}\")\n",
    "\n",
    "    for i_fold, (trainIndex, validIndex) in enumerate(folds.split(X=X, y=y[:,0],groups=groups)):\n",
    "        if i_fold in CFG.FOLD_TO_TRAIN:\n",
    "            ## print(\"Fold {}/{}\".format(i_fold + 1, CFG.N_FOLDS))\n",
    "            \n",
    "            # splitting into train and validataion sets\n",
    "            trainTimeStamps, X_train, y_train, trainGroups = timestamps[trainIndex], X[trainIndex], y[trainIndex], groups[trainIndex]\n",
    "            validTimeStamps, X_valid, y_valid, validGroups = timestamps[validIndex], X[validIndex], y[validIndex], groups[validIndex] \n",
    "                        \n",
    "            # create torch Datasets and Dataloader for each fold's train and validation data\n",
    "            dataset_train = wiFiFeaturesDataset(trainTimeStamps, X_train, y_train, trainGroups)\n",
    "            dataset_valid = wiFiFeaturesDataset(validTimeStamps, X_valid, y_valid, validGroups)            \n",
    "            dataloader_train = getDataLoader(dataset_train, datasetType= 'train')\n",
    "            dataloader_valid = getDataLoader(dataset_valid, datasetType= 'valid')\n",
    "            \n",
    "            # supervised model instance and move to compute device\n",
    "            model = modelToFit(X.shape[1], 3, CFG.FILTERLIST, CFG.STRIDELIST)\n",
    "            model.to(device);\n",
    "            ## print(f\"there are {find_no_of_trainable_params(model)} params in model\")\n",
    "\n",
    "            # optimizer function, lr schedulers and loss function\n",
    "            optimizer = getOptimizer(model)\n",
    "            scheduler = getScheduler(optimizer, dataloader_train)\n",
    "            # print(f\"optimizer={optimizer}, scheduler={scheduler}, loss_fn={criterion}\")\n",
    "\n",
    "            # train and validate single fold\n",
    "            foldResults = trainValidateOneFold(buildingName, i_fold, model, optimizer, scheduler,dataloader_train, dataloader_valid)\n",
    "            buildingTrainResults = buildingTrainResults + foldResults\n",
    "            \n",
    "            ## del trainTimeStamps, X_train, y_train, trainGroups\n",
    "            ## del validTimeStamps, X_valid, y_valid, validGroups\n",
    "            del dataloader_train, dataloader_valid, model, optimizer, scheduler\n",
    "            gc.collect()\n",
    "    \n",
    "    ## del timestamps, X, y, groups\n",
    "    ## gc.collect()\n",
    "    \n",
    "    buildingTrainResults = pd.DataFrame(buildingTrainResults)\n",
    "    buildingTrainResults['valTotalLoss'] = buildingTrainResults['valPosLoss'] + buildingTrainResults['valFloorLoss']\n",
    "    buildingTrainResults['trainTotalLoss'] = buildingTrainResults['trainPosLoss'] + buildingTrainResults['trainFloorLoss']\n",
    "    return buildingTrainResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:09:00.387644Z",
     "iopub.status.busy": "2021-05-04T00:09:00.386692Z",
     "iopub.status.idle": "2021-05-04T00:09:00.389488Z",
     "shell.execute_reply": "2021-05-04T00:09:00.389023Z"
    },
    "papermill": {
     "duration": 0.043555,
     "end_time": "2021-05-04T00:09:00.389604",
     "exception": false,
     "start_time": "2021-05-04T00:09:00.346049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getFoldBestResultsDf(trainResults):\n",
    "    bestResults = []\n",
    "    numFolds = trainResults['fold'].nunique()\n",
    "    \n",
    "    for fold in range(numFolds):\n",
    "        if fold in CFG.FOLD_TO_TRAIN:\n",
    "            foldDf = trainResults[trainResults['fold']== fold]\n",
    "            bestResults.append(foldDf.iloc[np.argmin(foldDf['valTotalLoss'].values),:])\n",
    "    \n",
    "    bestResults =pd.DataFrame(bestResults)\n",
    "    valPosLossBest = bestResults['valPosLoss'].values\n",
    "    print(f\"Best valPosLoss for all folds = {valPosLossBest}\")\n",
    "    print(f\"Mean, std ={valPosLossBest.mean()}, {valPosLossBest.std()}\")\n",
    "    return bestResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034901,
     "end_time": "2021-05-04T00:09:00.459484",
     "exception": false,
     "start_time": "2021-05-04T00:09:00.424583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate OOF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:09:00.539622Z",
     "iopub.status.busy": "2021-05-04T00:09:00.538769Z",
     "iopub.status.idle": "2021-05-04T00:09:00.541384Z",
     "shell.execute_reply": "2021-05-04T00:09:00.540946Z"
    },
    "papermill": {
     "duration": 0.047281,
     "end_time": "2021-05-04T00:09:00.5415",
     "exception": false,
     "start_time": "2021-05-04T00:09:00.494219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateOOF(modelSaveDir:str, buildingName:str, modelName:str):\n",
    "    oof_ts = []\n",
    "    oof_preds = []\n",
    "    oof_targets = []\n",
    "    oof_groups = []\n",
    "    oof_folds = []\n",
    "\n",
    "    modelPaths = sorted(glob.glob(f\"{modelSaveDir}/{buildingName}_{modelName}_fold*.pth\"))\n",
    "\n",
    "    for fold in range(len(modelPaths)):\n",
    "        # load building-model-fold checkpoint\n",
    "        checkPoint = torch.load(modelPaths[fold], map_location=torch.device(device))\n",
    "        numRows = len(checkPoint['val_ts'])\n",
    "\n",
    "        oof_ts.append(checkPoint['val_ts'])\n",
    "        oof_preds.append(checkPoint['val_preds'])\n",
    "        oof_targets.append(checkPoint['val_targets'])\n",
    "        oof_groups.append(checkPoint['val_groups'])\n",
    "        oof_folds.append([fold] * numRows)\n",
    "    \n",
    "    oof_ts = np.concatenate(oof_ts,axis=0)\n",
    "    oof_preds = np.concatenate(oof_preds,axis=0)\n",
    "    oof_targets = np.concatenate(oof_targets,axis=0)\n",
    "    oof_groups = np.concatenate(oof_groups,axis=0)\n",
    "    oof_folds = np.concatenate(oof_folds,axis=0)\n",
    "    \n",
    "    #print(oof_ts.shape, oof_preds.shape, oof_targets.shape, oof_groups.shape, oof_folds.shape)\n",
    "    oof_df = pd.DataFrame({'timestamp' : oof_ts, 'x_preds': oof_preds[:,0], 'y_preds': oof_preds[:,1],\n",
    "                       'floor_preds': oof_preds[:,2], 'x_tgt': oof_targets[:,0], 'y_tgt': oof_targets[:,1],\n",
    "                       'floor_tgt': oof_targets[:,2], 'path' : oof_groups, 'fold' : oof_folds\n",
    "                      })\n",
    "    print(f\"OOF prediction for {buildingName} site generated\")\n",
    "    return oof_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034781,
     "end_time": "2021-05-04T00:09:00.610741",
     "exception": false,
     "start_time": "2021-05-04T00:09:00.57596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test set prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:09:00.695741Z",
     "iopub.status.busy": "2021-05-04T00:09:00.694652Z",
     "iopub.status.idle": "2021-05-04T00:09:00.697458Z",
     "shell.execute_reply": "2021-05-04T00:09:00.697011Z"
    },
    "papermill": {
     "duration": 0.051003,
     "end_time": "2021-05-04T00:09:00.697571",
     "exception": false,
     "start_time": "2021-05-04T00:09:00.646568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateWiFiSubmission(modelToFit, modelSaveDir:str, buildingName:str, modelName:str):\n",
    "    modelPaths = sorted(glob.glob(f\"{modelSaveDir}/{buildingName}_{modelName}_fold*.pth\"))\n",
    "    buildingTestData = f\"{wifiFeaturesDir_test}/{buildingName}_test.pickle\"\n",
    "    with open(buildingTestData, 'rb') as inputFile:\n",
    "        testData = pickle.load(inputFile)    \n",
    "    \n",
    "    test_ts = []\n",
    "    test_fold = []\n",
    "    test_preds = []\n",
    "    test_groups = []\n",
    "\n",
    "    for fold in range(CFG.N_FOLDS):\n",
    "        ## print(f\"Fold {fold} processing\")\n",
    "        #print(f\"Before stdscaler : testX mean = {testX.mean()}, testData std = {testX.std()}\")\n",
    "        testGroups = testData.iloc[:,-1].values    \n",
    "        testTimestamps = testData.iloc[:,0].values        \n",
    "        testX = getBuildingFeatures(testData, 'test')  \n",
    "        \n",
    "        checkPoint = torch.load(modelPaths[fold], map_location=torch.device(device))\n",
    "        model = modelToFit(testX.shape[1], 3, CFG.FILTERLIST, CFG.STRIDELIST)\n",
    "        model.to(device);\n",
    "        model.load_state_dict(checkPoint['model'])\n",
    "\n",
    "        # set model to Validate mode\n",
    "        model.eval()\n",
    "        ## test Dataset and data loaders\n",
    "        testDataset = wiFiFeaturesDataset_test(testTimestamps, testX, testGroups)\n",
    "        testDataloader = getDataLoader(testDataset, datasetType= 'test')\n",
    "\n",
    "        dataLoaderIterator = iter(testDataloader)\n",
    "        for idx in range(len(testDataloader)):\n",
    "            try:\n",
    "                ts, inputs, testGroups = next(dataLoaderIterator)\n",
    "            except StopIteration:\n",
    "                dataLoaderIterator = iter(testDataloader)\n",
    "                ts, inputs, testGroups = next(dataLoaderIterator)\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            # forward prediction\n",
    "            with torch.no_grad():    \n",
    "                y_preds = model(inputs)\n",
    "\n",
    "            # store predictions and targets to compute metrics later\n",
    "            test_ts.append(ts)\n",
    "            test_preds.append(y_preds)\n",
    "            test_groups.append(testGroups)\n",
    "        \n",
    "        test_fold.append([fold] * len(testX))\n",
    "        ## del testDataloader\n",
    "        ## torch.cuda.empty_cache()\n",
    "        ## gc.collect()\n",
    "        \n",
    "    # concatenate to get as 1 2d array \n",
    "    test_preds = torch.cat(test_preds, 0).cpu().data.numpy() \n",
    "    test_ts = np.concatenate(test_ts, axis=0)\n",
    "    test_fold = np.concatenate(test_fold, axis=0)\n",
    "    test_groups = np.concatenate(test_groups, axis=0)\n",
    "    subm_wifi_df = pd.DataFrame({'timestamp' : test_ts, 'x_preds': test_preds[:,0], 'y_preds': test_preds[:,1],\n",
    "                                 'floor_preds': test_preds[:,2], 'path' : test_groups, 'fold' : test_fold})\n",
    "    subm_wifi_df.to_pickle(f\"{modelSaveDir}/{buildingName}_wifi_subm.pickle\")  \n",
    "    print(f\"Test data prediction for {buildingName} site generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034591,
     "end_time": "2021-05-04T00:09:00.766202",
     "exception": false,
     "start_time": "2021-05-04T00:09:00.731611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compute Device as CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:09:01.20578Z",
     "iopub.status.busy": "2021-05-04T00:09:01.202991Z",
     "iopub.status.idle": "2021-05-04T00:09:01.208397Z",
     "shell.execute_reply": "2021-05-04T00:09:01.208862Z"
    },
    "papermill": {
     "duration": 0.407549,
     "end_time": "2021-05-04T00:09:01.209012",
     "exception": false,
     "start_time": "2021-05-04T00:09:00.801463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Device as cpu or tpu\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03519,
     "end_time": "2021-05-04T00:09:01.279733",
     "exception": false,
     "start_time": "2021-05-04T00:09:01.244543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:09:01.356724Z",
     "iopub.status.busy": "2021-05-04T00:09:01.355834Z",
     "iopub.status.idle": "2021-05-04T00:09:01.35859Z",
     "shell.execute_reply": "2021-05-04T00:09:01.358172Z"
    },
    "papermill": {
     "duration": 0.04289,
     "end_time": "2021-05-04T00:09:01.358729",
     "exception": false,
     "start_time": "2021-05-04T00:09:01.315839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for cv\n",
    "folds = GroupKFold(n_splits=CFG.N_FOLDS)\n",
    "\n",
    "# scaler to handle AMP\n",
    "## scaler = GradScaler()   \n",
    "\n",
    "criterion = competitionMetric\n",
    "modelToFit = CNNWiFiFeaturesModel   ## CNNWiFiFeaturesModel_variant2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03604,
     "end_time": "2021-05-04T00:09:01.430575",
     "exception": false,
     "start_time": "2021-05-04T00:09:01.394535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training & Validation main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:09:01.514104Z",
     "iopub.status.busy": "2021-05-04T00:09:01.513332Z",
     "iopub.status.idle": "2021-05-04T01:46:17.774024Z",
     "shell.execute_reply": "2021-05-04T01:46:17.773452Z"
    },
    "papermill": {
     "duration": 5836.305896,
     "end_time": "2021-05-04T01:46:17.774222",
     "exception": false,
     "start_time": "2021-05-04T00:09:01.468326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if CFG.TRAIN == True:\n",
    "    \n",
    "    buildingPathList_train = sorted(glob.glob(f\"{wifiFeaturesDir_train}/*.pickle\"))\n",
    "    buildingPathList_train = buildingPathList_train[CFG.BUILDING_SITES_RANGE[0]: CFG.BUILDING_SITES_RANGE[1]]\n",
    "    print(f\"{len(buildingPathList_train)} sites are to be trained\")\n",
    "    ## print(buildingPathList_train)\n",
    "\n",
    "    for buildingPath_train in buildingPathList_train:\n",
    "        \n",
    "        print('----------------------------------')\n",
    "        ## get building name\n",
    "        buildingName = getBuildingName(buildingPath_train)\n",
    "        \n",
    "        ## train and validate for building data\n",
    "        ## buildingTrainResults = trainValidateOneBuilding(buildingPath_train, modelToFit)\n",
    "        ## bestResults = getFoldBestResultsDf(buildingTrainResults)\n",
    "        \n",
    "    \n",
    "        ## generate OOF prediction for building-model combination\n",
    "        buildingOOF = generateOOF(modelOutputDir, buildingName, CFG.MODEL_NAME)\n",
    "        \n",
    "        ## prediction for test data too\n",
    "        generateWiFiSubmission(modelToFit, modelOutputDir, buildingName, CFG.MODEL_NAME)\n",
    "\n",
    "        ## save results to file\n",
    "        buildingOOF.to_pickle(f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_OOF.pickle\")\n",
    "        \n",
    "        ## bestResults.to_pickle(f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_bestResults.pickle\")\n",
    "        ## buildingTrainResults.to_pickle(f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_trainResults.pickle\")\n",
    "        \n",
    "        ## plot building results\n",
    "        ## plotTrainingResults(buildingTrainResults, buildingName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T01:46:17.934206Z",
     "iopub.status.busy": "2021-05-04T01:46:17.933077Z",
     "iopub.status.idle": "2021-05-04T01:46:17.944866Z",
     "shell.execute_reply": "2021-05-04T01:46:17.945423Z"
    },
    "papermill": {
     "duration": 0.096342,
     "end_time": "2021-05-04T01:46:17.945635",
     "exception": false,
     "start_time": "2021-05-04T01:46:17.849293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## bestResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.072617,
     "end_time": "2021-05-04T01:46:18.10244",
     "exception": false,
     "start_time": "2021-05-04T01:46:18.029823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "buildingPathList_train = sorted(glob.glob(f\"{wifiFeaturesDir_train}/*.pickle\"))\n",
    "buildingPathList_test = sorted(glob.glob(f\"{wifiFeaturesDir_test}/*.pickle\"))\n",
    "\n",
    "for idx in range(len(buildingPathList_train)):\n",
    "    print('-----------------------------')\n",
    "    trainFilePath = buildingPathList_train[idx]\n",
    "    testFilePath  = buildingPathList_test[idx]\n",
    "    print(f\"{idx}. {getBuildingName(trainFilePath)}\")\n",
    "    with open(trainFilePath, 'rb') as inputTrainFile:\n",
    "        trainData = pickle.load(inputTrainFile)\n",
    "    with open(testFilePath, 'rb') as inputTestFile:\n",
    "        testData = pickle.load(inputTestFile)\n",
    "    \n",
    "    with pd.option_context('display.max_rows', 1, 'display.max_columns', 12,\n",
    "                           'display.width', 500, 'display.precision', 3,\n",
    "                           'display.colheader_justify', 'left'):\n",
    "        display(trainData)\n",
    "        display(testData)\n",
    "```        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.071437,
     "end_time": "2021-05-04T01:46:18.244181",
     "exception": false,
     "start_time": "2021-05-04T01:46:18.172744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
