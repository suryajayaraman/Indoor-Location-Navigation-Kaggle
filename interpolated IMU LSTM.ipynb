{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reduced-master",
   "metadata": {
    "papermill": {
     "duration": 0.036151,
     "end_time": "2021-05-02T08:27:00.577292",
     "exception": false,
     "start_time": "2021-05-02T08:27:00.541141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-composite",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:00.725871Z",
     "iopub.status.busy": "2021-05-02T08:27:00.725017Z",
     "iopub.status.idle": "2021-05-02T08:27:17.794634Z",
     "shell.execute_reply": "2021-05-02T08:27:17.793841Z"
    },
    "papermill": {
     "duration": 17.106883,
     "end_time": "2021-05-02T08:27:17.794797",
     "exception": false,
     "start_time": "2021-05-02T08:27:00.687914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sixth-label",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:17.892386Z",
     "iopub.status.busy": "2021-05-02T08:27:17.887183Z",
     "iopub.status.idle": "2021-05-02T08:27:23.142682Z",
     "shell.execute_reply": "2021-05-02T08:27:23.141370Z"
    },
    "papermill": {
     "duration": 5.306148,
     "end_time": "2021-05-02T08:27:23.142864",
     "exception": false,
     "start_time": "2021-05-02T08:27:17.836716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# DL library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from  torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# metrics calculation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# basic plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# interactive plots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import iplot\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-checklist",
   "metadata": {
    "papermill": {
     "duration": 0.036584,
     "end_time": "2021-05-02T08:27:23.216694",
     "exception": false,
     "start_time": "2021-05-02T08:27:23.180110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offensive-spyware",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:23.300148Z",
     "iopub.status.busy": "2021-05-02T08:27:23.299195Z",
     "iopub.status.idle": "2021-05-02T08:27:23.302604Z",
     "shell.execute_reply": "2021-05-02T08:27:23.303250Z"
    },
    "papermill": {
     "duration": 0.05021,
     "end_time": "2021-05-02T08:27:23.303416",
     "exception": false,
     "start_time": "2021-05-02T08:27:23.253206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # pipeline parameters\n",
    "    SEED        = 42\n",
    "    TRAIN       = True\n",
    "    LR_FIND     = False\n",
    "    TEST        = False\n",
    "    N_FOLDS     = 5 \n",
    "    N_EPOCHS    = 40\n",
    "    TEST_BATCH_SIZE  = 128\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    NUM_WORKERS      = 4\n",
    "    DATA_FRAC        = 1.0\n",
    "    FOLD_TO_TRAIN    = [0, 1, 2, 3, 4] # \n",
    "\n",
    "    # model parameters\n",
    "    MODEL_ARCH  = 'CNN'\n",
    "    MODEL_NAME  = 'CNN_v1'\n",
    "    WGT_PATH    = ''\n",
    "    WGT_MODEL   = ''\n",
    "    PRINT_N_EPOCH = 2\n",
    "    \n",
    "    # scheduler variables\n",
    "    MAX_LR    = 1e-2\n",
    "    MIN_LR    = 1e-5\n",
    "    SCHEDULER = 'CosineAnnealingWarmRestarts'  # ['ReduceLROnPlateau', 'None', 'OneCycleLR','CosineAnnealingLR']\n",
    "    T_0       = 5      # CosineAnnealingWarmRestarts\n",
    "    T_MULT    = 2      # CosineAnnealingWarmRestarts\n",
    "    T_MAX     = 10     # CosineAnnealingLR\n",
    "\n",
    "    # optimizer variables\n",
    "    OPTIMIZER     = 'Adam'\n",
    "    WEIGHT_DECAY  = 1e-6\n",
    "    GRD_ACC_STEPS = 1\n",
    "    MAX_GRD_NORM  = 2\n",
    "\n",
    "    # features parameters\n",
    "    USE_FREQ_FEATS = True\n",
    "    USE_DT_FEATS   = True\n",
    "    \n",
    "    BUILDING_SITES_RANGE = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sexual-arrangement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:23.384237Z",
     "iopub.status.busy": "2021-05-02T08:27:23.383289Z",
     "iopub.status.idle": "2021-05-02T08:27:23.387034Z",
     "shell.execute_reply": "2021-05-02T08:27:23.386467Z"
    },
    "papermill": {
     "duration": 0.047098,
     "end_time": "2021-05-02T08:27:23.387159",
     "exception": false,
     "start_time": "2021-05-02T08:27:23.340061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "floor_map = {\"B2\": -2, \"B1\": -1, \"F1\": 0, \"F2\": 1, \"F3\": 2, \"F4\": 3, \"F5\": 4, \"F6\": 5, \"F7\": 6, \"F8\": 7, \"F9\": 8,\n",
    "             \"1F\": 0, \"2F\": 1, \"3F\": 2, \"4F\": 3, \"5F\": 4, \"6F\": 5, \"7F\": 6, \"8F\": 7, \"9F\": 8}\n",
    "\n",
    "minCount = 1\n",
    "freqFillerValue = 0\n",
    "rssiFillerValue = -999.0\n",
    "dtFillerValue   = 1000.0\n",
    "modelOutputDir = 'modelSaveDir/CNN_v1_Results'\n",
    "wifiFeaturesDir_train = 'referencePublicNotebooks/wiFiFeatures/train'\n",
    "wifiFeaturesDir_test  = 'referencePublicNotebooks/wiFiFeatures/test'\n",
    "sampleCsvPath = 'sample_submission.csv'\n",
    "waypointData_trainPath = 'wayPointData_train.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-basis",
   "metadata": {
    "papermill": {
     "duration": 0.03561,
     "end_time": "2021-05-02T08:27:23.458864",
     "exception": false,
     "start_time": "2021-05-02T08:27:23.423254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "characteristic-behavior",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:23.703169Z",
     "iopub.status.busy": "2021-05-02T08:27:23.702357Z",
     "iopub.status.idle": "2021-05-02T08:27:23.710236Z",
     "shell.execute_reply": "2021-05-02T08:27:23.709513Z"
    },
    "papermill": {
     "duration": 0.051019,
     "end_time": "2021-05-02T08:27:23.710369",
     "exception": false,
     "start_time": "2021-05-02T08:27:23.659350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(CFG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "funded-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeAngle(angle): #input angle [-pi, pi] mapped to [0, 2* pi]\n",
    "    if angle < 0.0:\n",
    "        return (angle + 2*np.pi)\n",
    "    if angle > (2*np.pi):\n",
    "        return (angle - 2*np.pi)\n",
    "    else:\n",
    "        return angle\n",
    "\n",
    "def compass_to_cart(compass_orientation):\n",
    "    #input  : compass heading in radians\n",
    "    #output : cartesian heading in radians\n",
    "\n",
    "    if compass_orientation == 0.0:\n",
    "        compass_orientation = 2*np.pi\n",
    "    \n",
    "    t1 = NormalizeAngle(compass_orientation - (0.5*np.pi))\n",
    "    cartesian_orientation = NormalizeAngle(2*np.pi - t1)\n",
    "    return cartesian_orientation\n",
    "\n",
    "def cart_to_compass(cartesian_orientation):\n",
    "    #input  : cartesian heading in radians\n",
    "    #output : compass heading in radians\n",
    "\n",
    "    if cartesian_orientation == 0.0:\n",
    "        cartesian_orientation = 2*np.pi\n",
    "    \n",
    "    t1 = NormalizeAngle(2*np.pi - cartesian_orientation)\n",
    "    compass_orientation = NormalizeAngle(t1 + (0.5*np.pi))\n",
    "    return compass_orientation\n",
    "\n",
    "def getYawFromEuler(qx, qy, qz):\n",
    "    qw = np.sqrt(1 - (qx**2 + qy**2 + qz**2))\n",
    "    roll  = np.arctan2(2 * (qw * qx + qy * qz), 1 - 2 * (qx**2 + qy**2))\n",
    "    pitch = np.arcsin( 2 * (qw * qy - qz * qx))\n",
    "    yaw   = np.arctan2(2 * (qw * qz + qx * qy), 1 - 2 * (qy**2 + qz**2))\n",
    "    \n",
    "    ## convert from compass to cartesian coordinates\n",
    "    yawDegrees = np.rad2deg(compass_to_cart((-yaw) + np.pi))\n",
    "    return yawDegrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "persistent-career",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:23.537528Z",
     "iopub.status.busy": "2021-05-02T08:27:23.536500Z",
     "iopub.status.idle": "2021-05-02T08:27:23.540218Z",
     "shell.execute_reply": "2021-05-02T08:27:23.539699Z"
    },
    "papermill": {
     "duration": 0.044985,
     "end_time": "2021-05-02T08:27:23.540369",
     "exception": false,
     "start_time": "2021-05-02T08:27:23.495384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getBuildingName(buildingDataPath):\n",
    "    return buildingDataPath.split('/')[-1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "congressional-awareness",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:23.619691Z",
     "iopub.status.busy": "2021-05-02T08:27:23.618789Z",
     "iopub.status.idle": "2021-05-02T08:27:23.622643Z",
     "shell.execute_reply": "2021-05-02T08:27:23.622096Z"
    },
    "papermill": {
     "duration": 0.045501,
     "end_time": "2021-05-02T08:27:23.622781",
     "exception": false,
     "start_time": "2021-05-02T08:27:23.577280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_no_of_trainable_params(model):\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unlimited-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPathTimeList():\n",
    "    pathTimeList = []\n",
    "    for path in tqdm(pathFilesList):\n",
    "        pathImuData = imuData[imuData.path == path].sort_values(by='timestamp')\n",
    "        pathDataTimestamp = pathImuData.timestamp.values\n",
    "        pathTimeList.append(len(pathDataTimestamp))\n",
    "        \n",
    "    ## plt.figure(figsize=(12,6));\n",
    "    ## sns.histplot(pathTimeList);\n",
    "    return pathTimeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "miniature-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWayPointData_train():\n",
    "    with open(waypointData_trainPath,'rb') as inputFile:\n",
    "        waypointData_train = pickle.load(inputFile)\n",
    "    return waypointData_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "natural-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPathWaypointData(pathFile):\n",
    "    waypointData_train = getWayPointData_train()\n",
    "    pathWayPointData = waypointData_train[waypointData_train.path == pathFile]\n",
    "    return pathWayPointData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "forty-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('referencePublicNotebooks/imuFeatures/train/5a0546857ecc773753327266_imuData_train.pickle','rb') as inputFile:\n",
    "    imuData = pickle.load(inputFile)\n",
    "imuData.sort_values(by=['path', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "authorized-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathFilesList = sorted(imuData.path.unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-stone",
   "metadata": {},
   "source": [
    "```python\n",
    "numPaths = 3\n",
    "fig = make_subplots(rows=numPaths, cols=1)\n",
    "\n",
    "\n",
    "for index in range(numPaths):\n",
    "    pathFile = pathFilesList[20 + index] ## 10 + \n",
    "    ##print(pathFile)\n",
    "\n",
    "    pathWayPointData = getPathWaypointData(pathFile)\n",
    "    pathWayPointData['dt'] = (pathWayPointData['timestamp'] - pathWayPointData.iloc[0,0]) / 1000.0\n",
    "    pathWaypointYaw = np.rad2deg(np.arctan2(np.diff(pathWayPointData.y), np.diff(pathWayPointData.x))) + 180.0\n",
    "\n",
    "    ## path waypoints\n",
    "    waypoints = pathWayPointData.loc[:,['x', 'y']].values\n",
    "    localWayPoints = waypoints - waypoints[0]\n",
    "    ## plt.plot(waypoints[:,0], waypoints[:,1])\n",
    "    \n",
    "    pathImuData = imuData[imuData.path == pathFile]\n",
    "    pathImuData['dt'] = (pathImuData['timestamp'] - pathImuData['timestamp'][0]) / 1000.0\n",
    "    pathImuData['yaw'] = pathImuData.apply(lambda row: getYawFromEuler(row['ahrs_q1'], row['ahrs_q2'], row['ahrs_q3']),axis=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=pathWayPointData.dt, y=pathWaypointYaw, name =f'{index+1}_waypoint'), row=index+1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=pathImuData.dt, y=pathImuData.yaw, name =f'{index+1}_azimuth'), row=index+1, col=1)\n",
    "\n",
    "fig.update_layout(height=600, width=600, title_text=f\"{numPaths}_comparison\")    \n",
    "fig.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-cambodia",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "wayPointData_train = getWayPointData_train()\n",
    "buildingList = sorted(wayPointData_train.building.unique().tolist())\n",
    "pathList     = sorted(wayPointData_train.path.unique().tolist())\n",
    "\n",
    "output = []\n",
    "wayPointBins = [0,5,10,20,84,110]\n",
    "for building, buildingData in wayPointData_train.groupby(by='building'):\n",
    "    for path, pathData in buildingData.groupby(by='path'):\n",
    "        output.append([building, path, pathData.shape[0]])\n",
    "        \n",
    "output = pd.DataFrame(output, columns =['building', 'path', 'count'])  \n",
    "output['countBin'] = pd.cut(output['count'], bins=wayPointBins)\n",
    "\n",
    "## for building, buildingData in output.groupby(by='building'):\n",
    "##    print(f\"{building}, numPaths = {len(buildingData['path'].unique())}, minCount = {buildingData['count'].min()}, maxCount = {buildingData['count'].max()}\")\n",
    "\n",
    "for building, buildingData in output.groupby(by='building'):\n",
    "    print('---------------------')\n",
    "    print(building)\n",
    "    print(buildingData['countBin'].value_counts(normalize=False))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-rainbow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-palace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-threshold",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-processing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-disorder",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:23.794791Z",
     "iopub.status.busy": "2021-05-02T08:27:23.793811Z",
     "iopub.status.idle": "2021-05-02T08:27:23.797655Z",
     "shell.execute_reply": "2021-05-02T08:27:23.797083Z"
    },
    "papermill": {
     "duration": 0.050998,
     "end_time": "2021-05-02T08:27:23.797830",
     "exception": false,
     "start_time": "2021-05-02T08:27:23.746832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### rssi features is fixed, we can choose to use dt and freq features optionally\n",
    "# Incase freq signal is not needed, use rssi and dt features alone    \n",
    "# There are 5 columns for timestamp, y, pathNames values in csv, reamining are features\n",
    "# total features = 3 * [rssi, dt, freq]\n",
    "# hence unique wifi ids = totalFeatures / 3\n",
    "\"\"\"\n",
    "def getBuildingFeatures(buildingData, dataType):\n",
    "    if dataType == 'train':\n",
    "        buildingData = buildingData.iloc[:,1:-4].values.astype(np.float16)\n",
    "    else:\n",
    "        buildingData = buildingData.iloc[:,1:-1].values.astype(np.float16)\n",
    "    \n",
    "    numBssids = int(buildingData.shape[1] / 3)\n",
    "    ## replace -999 with 99, 1000.0 with 50.0 and scale accordindly\n",
    "    buildingData[buildingData == -999.0] = -99.0\n",
    "    buildingData[buildingData == 1000.0] = 50.0\n",
    "    buildingData[:,0:numBssids]              = buildingData[:,0:numBssids] / 100.0\n",
    "    buildingData[:,numBssids: 2*numBssids]   = buildingData[:,numBssids: 2*numBssids] / 50.0\n",
    "    buildingData[:,2*numBssids: 3*numBssids] = buildingData[:,2*numBssids: 3*numBssids] / 1000.0\n",
    "\n",
    "    if CFG.USE_FREQ_FEATS == True:    \n",
    "        ## use all features\n",
    "        if CFG.USE_DT_FEATS == True:\n",
    "            X = buildingData\n",
    "        ## use rssi and freq features alone\n",
    "        else:\n",
    "            desiredFeatures = list(range(0, numBssids)) + list(range(2*numBssids, 3*numBssids))\n",
    "            X = buildingData[:, desiredFeatures]\n",
    "    else:\n",
    "        ## use only rssi features alone\n",
    "        if CFG.USE_DT_FEATS == True:\n",
    "            X = buildingData[:,0:2*numBssids]\n",
    "        ## use only rssi features alone\n",
    "        else:\n",
    "            X = buildingData[:,0:numBssids]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-christianity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:23.881059Z",
     "iopub.status.busy": "2021-05-02T08:27:23.880175Z",
     "iopub.status.idle": "2021-05-02T08:27:23.884383Z",
     "shell.execute_reply": "2021-05-02T08:27:23.883789Z"
    },
    "papermill": {
     "duration": 0.049159,
     "end_time": "2021-05-02T08:27:23.884574",
     "exception": false,
     "start_time": "2021-05-02T08:27:23.835415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getBuildingData(buildingDataPath):\n",
    "    # read building data \n",
    "    #### data = pd.read_pickle(buildingDataPath)\n",
    "    with open(buildingDataPath, 'rb') as inputFile:\n",
    "        data = pickle.load(inputFile)    \n",
    "    \n",
    "    # use fraction if needed\n",
    "    if CFG.DATA_FRAC < 1:\n",
    "        data = data.sample(frac=CFG.DATA_FRAC).reset_index(drop=True)\n",
    "\n",
    "    # first column is timestamp\n",
    "    timestamps = data.iloc[:,0].values   # np.expand_dims( , ,axis=1)\n",
    "    \n",
    "    # last column is pathFile name\n",
    "    groups = data.iloc[:,-1].values\n",
    "    \n",
    "    # target values are last but 3 columns\n",
    "    y = data.iloc[:,-4:-1].values\n",
    "    X = getBuildingFeatures(data,'train')\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return timestamps,X,y,groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-eight",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:23.964318Z",
     "iopub.status.busy": "2021-05-02T08:27:23.963413Z",
     "iopub.status.idle": "2021-05-02T08:27:23.967309Z",
     "shell.execute_reply": "2021-05-02T08:27:23.966787Z"
    },
    "papermill": {
     "duration": 0.045493,
     "end_time": "2021-05-02T08:27:23.967436",
     "exception": false,
     "start_time": "2021-05-02T08:27:23.921943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getInputFeatureSize(featureFilesPath):\n",
    "    sampleData = np.load(f\"{npyWifiFeaturesDir}/{featureFilesPath[0]}\")\n",
    "    return len(sampleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-harvard",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:24.047821Z",
     "iopub.status.busy": "2021-05-02T08:27:24.046928Z",
     "iopub.status.idle": "2021-05-02T08:27:24.050717Z",
     "shell.execute_reply": "2021-05-02T08:27:24.050144Z"
    },
    "papermill": {
     "duration": 0.047183,
     "end_time": "2021-05-02T08:27:24.050879",
     "exception": false,
     "start_time": "2021-05-02T08:27:24.003696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def competitionMetric(preds, targets):\n",
    "    \"\"\" The metric used in this competition \"\"\"\n",
    "    # position error\n",
    "    meanPosPredictionError = torch.mean(torch.sqrt(\n",
    "                             torch.square(torch.subtract(preds[:,0], targets[:,0])) + \n",
    "                             torch.square(torch.subtract(preds[:,1], targets[:,1]))))\n",
    "    # error in floor prediction\n",
    "    meanFloorPredictionError = torch.mean(15 * torch.abs(preds[:,2] - targets[:,2]))\n",
    "    return meanPosPredictionError, meanFloorPredictionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-theta",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:24.132439Z",
     "iopub.status.busy": "2021-05-02T08:27:24.131567Z",
     "iopub.status.idle": "2021-05-02T08:27:24.134658Z",
     "shell.execute_reply": "2021-05-02T08:27:24.135455Z"
    },
    "papermill": {
     "duration": 0.046768,
     "end_time": "2021-05-02T08:27:24.135711",
     "exception": false,
     "start_time": "2021-05-02T08:27:24.088943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getOptimizer(model : nn.Module):    \n",
    "    if CFG.OPTIMIZER == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), weight_decay=CFG.WEIGHT_DECAY, lr=CFG.MAX_LR)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), weight_decay=CFG.WEIGHT_DECAY, lr=CFG.MAX_LR, momentum=0.9)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-terrorism",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:24.219412Z",
     "iopub.status.busy": "2021-05-02T08:27:24.218503Z",
     "iopub.status.idle": "2021-05-02T08:27:24.222265Z",
     "shell.execute_reply": "2021-05-02T08:27:24.221732Z"
    },
    "papermill": {
     "duration": 0.048901,
     "end_time": "2021-05-02T08:27:24.222415",
     "exception": false,
     "start_time": "2021-05-02T08:27:24.173514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getScheduler(optimizer, dataloader_train):\n",
    "    if CFG.SCHEDULER == 'OneCycleLR':\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr= CFG.MAX_LR, epochs = CFG.N_EPOCHS, \n",
    "                          steps_per_epoch = len(dataloader_train), pct_start=0.25, div_factor=10, anneal_strategy='cos')\n",
    "    elif CFG.SCHEDULER == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=CFG.T_MULT, eta_min=CFG.MIN_LR, last_epoch=-1)\n",
    "    elif CFG.SCHEDULER == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_MAX * len(dataloader_train), eta_min=CFG.MIN_LR, last_epoch=-1)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-mixer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:24.305470Z",
     "iopub.status.busy": "2021-05-02T08:27:24.304516Z",
     "iopub.status.idle": "2021-05-02T08:27:24.308069Z",
     "shell.execute_reply": "2021-05-02T08:27:24.308706Z"
    },
    "papermill": {
     "duration": 0.048714,
     "end_time": "2021-05-02T08:27:24.308880",
     "exception": false,
     "start_time": "2021-05-02T08:27:24.260166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDataLoader(dataset, datasetType : str):\n",
    "    if datasetType == 'train':\n",
    "        batchSize = CFG.TRAIN_BATCH_SIZE\n",
    "        shuffleDataset = True\n",
    "    else:\n",
    "        batchSize = CFG.TEST_BATCH_SIZE\n",
    "        shuffleDataset = False\n",
    "    \n",
    "    dataLoader = DataLoader(dataset, batch_size= batchSize, shuffle=shuffleDataset,\n",
    "                            num_workers=CFG.NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "    return dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-praise",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:24.394813Z",
     "iopub.status.busy": "2021-05-02T08:27:24.393757Z",
     "iopub.status.idle": "2021-05-02T08:27:24.397891Z",
     "shell.execute_reply": "2021-05-02T08:27:24.397167Z"
    },
    "papermill": {
     "duration": 0.051238,
     "end_time": "2021-05-02T08:27:24.398100",
     "exception": false,
     "start_time": "2021-05-02T08:27:24.346862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotTrainingResults(resultsDf, buildingName):\n",
    "    # subplot to plot\n",
    "    fig = make_subplots(rows=1, cols=1)\n",
    "    colors = [ ('#d32f2f', '#ef5350'), ('#303f9f', '#5c6bc0'), ('#00796b', '#26a69a'),\n",
    "                ('#fbc02d', '#ffeb3b'), ('#5d4037', '#8d6e63')]\n",
    "\n",
    "    # find number of folds input df\n",
    "    numberOfFolds = resultsDf['fold'].nunique()\n",
    "    \n",
    "    # iterate through folds and plot\n",
    "    for i in range(numberOfFolds):\n",
    "        data = resultsDf[resultsDf['fold'] == i]\n",
    "        fig.add_trace(go.Scatter(x=data['epoch'].values, y=data['trainPosLoss'].values,\n",
    "                                mode='lines', visible='legendonly' if i > 0 else True,\n",
    "                                line=dict(color=colors[i][0], width=2),\n",
    "                                name='{}-trainPossLoss-Fold{}'.format(buildingName, i)),row=1, col=1)\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=data['epoch'], y=data['valPosLoss'].values,\n",
    "                                 mode='lines+markers', visible='legendonly' if i > 0 else True,\n",
    "                                 line=dict(color=colors[i][1], width=2),\n",
    "                                 name='{}-valPosLoss-Fold{}'.format(buildingName,i)),row=1, col=1)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-herald",
   "metadata": {
    "papermill": {
     "duration": 0.037917,
     "end_time": "2021-05-02T08:27:24.473615",
     "exception": false,
     "start_time": "2021-05-02T08:27:24.435698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-preparation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:24.560608Z",
     "iopub.status.busy": "2021-05-02T08:27:24.559623Z",
     "iopub.status.idle": "2021-05-02T08:27:24.563655Z",
     "shell.execute_reply": "2021-05-02T08:27:24.563020Z"
    },
    "papermill": {
     "duration": 0.051901,
     "end_time": "2021-05-02T08:27:24.563791",
     "exception": false,
     "start_time": "2021-05-02T08:27:24.511890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class wiFiFeaturesDataset(Dataset):\n",
    "    def __init__(self, timeStamps, X_data, y_data, groups):\n",
    "        self.timeStamps = timeStamps \n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.groups = groups\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x  = torch.from_numpy(self.X_data[index].astype(np.float32))\n",
    "        y  = torch.from_numpy(self.y_data[index].astype(np.float32))\n",
    "        ts = self.timeStamps[index].astype(np.int64)\n",
    "        group = self.groups[index]\n",
    "        return ts,x,y,group\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-accessory",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:24.651827Z",
     "iopub.status.busy": "2021-05-02T08:27:24.650651Z",
     "iopub.status.idle": "2021-05-02T08:27:24.654601Z",
     "shell.execute_reply": "2021-05-02T08:27:24.653877Z"
    },
    "papermill": {
     "duration": 0.048838,
     "end_time": "2021-05-02T08:27:24.654760",
     "exception": false,
     "start_time": "2021-05-02T08:27:24.605922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class wiFiFeaturesDataset_test(Dataset):\n",
    "    def __init__(self, timeStamps, X_data, groups):\n",
    "        self.timeStamps = timeStamps \n",
    "        self.X_data = X_data\n",
    "        self.groups = groups\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x  = torch.from_numpy(self.X_data[index].astype(np.float32))\n",
    "        ts = self.timeStamps[index].astype(np.int64)\n",
    "        group = self.groups[index]\n",
    "        return ts,x,group\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-tampa",
   "metadata": {
    "papermill": {
     "duration": 0.038078,
     "end_time": "2021-05-02T08:27:24.732052",
     "exception": false,
     "start_time": "2021-05-02T08:27:24.693974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MLP Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-democrat",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:24.819257Z",
     "iopub.status.busy": "2021-05-02T08:27:24.818334Z",
     "iopub.status.idle": "2021-05-02T08:27:24.822571Z",
     "shell.execute_reply": "2021-05-02T08:27:24.822038Z"
    },
    "papermill": {
     "duration": 0.051432,
     "end_time": "2021-05-02T08:27:24.822708",
     "exception": false,
     "start_time": "2021-05-02T08:27:24.771276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class wiFiFeaturesMLPModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_features=n_input, out_features=512)\n",
    "        self.lin2 = nn.Linear(in_features=512,     out_features=32)\n",
    "        self.lin3 = nn.Linear(in_features=32,      out_features=n_output)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.drops = nn.Dropout(0.3)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        numBatches = x.shape[0]\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        \n",
    "        ## batchnorm doesnt work for batchsize of 1\n",
    "        if numBatches > 1:\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(self.lin2(x))\n",
    "            x = self.drops(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.lin3(x)\n",
    "        else:\n",
    "            x = F.relu(self.lin2(x))\n",
    "            x = self.drops(x)\n",
    "            x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-infection",
   "metadata": {},
   "source": [
    "## CNN WiFi features Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNWiFiFeaturesModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.numBssids = int(n_input/3)\n",
    "        self.conv1 = nn.Conv2d(1, 16, (3,64), stride=8)\n",
    "        self.conv2 = nn.Conv2d(1,32, (3,64), stride=4)\n",
    "        self.drops = nn.Dropout(p=0.3)\n",
    "        self.fc_inputSize = self.getFC_InputSize(self.numBssids, [16,32])\n",
    "        self.bn1 = nn.BatchNorm1d(self.fc_inputSize)        \n",
    "        self.fc1 = nn.Linear(self.fc_inputSize, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128,16)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(16,n_output)\n",
    "        \n",
    "    def getConvOutputSize(self, h_in, w_in, kernel_height, kernel_width, paddSize, stride):\n",
    "        h_out = int((h_in + 2*paddSize - kernel_height) / stride) + 1\n",
    "        w_out = int((w_in + 2*paddSize - kernel_width)  / stride) + 1\n",
    "        return (h_out,w_out)        \n",
    "        \n",
    "    def getFC_InputSize(self, numFeatures, filtersList):\n",
    "        conv1_outputSize = self.getConvOutputSize(3,numFeatures, 3,64,0,8)\n",
    "        conv2_outputSize = self.getConvOutputSize(filtersList[0], conv1_outputSize[1], 3,64,0,4)\n",
    "        fc_inputSize = conv2_outputSize[0] * conv2_outputSize[1]\n",
    "        return fc_inputSize\n",
    "\n",
    "    def forward(self, x):\n",
    "        numBatches = x.shape[0]\n",
    "        x = x.view(numBatches,1,3,-1)        \n",
    "        \n",
    "        ## first conv layer\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        ## second conv layer\n",
    "        x = x.view(numBatches, 1, 16, -1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        ## max pooling across channels, then dropout on features \n",
    "        x = torch.mean(x,1)\n",
    "        x = x.view(numBatches,-1)\n",
    "        ## x = self.drops(x)\n",
    "        ## print(x.shape)\n",
    "        \n",
    "        ## fc layer starts\n",
    "        ## batchnorm doesnt work for batchsize of 1\n",
    "        if numBatches > 1:   \n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.drops(x)\n",
    "            x = self.bn2(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.bn3(x)\n",
    "            x = self.fc3(x)\n",
    "        else:\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.drops(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNWiFiFeaturesModel_variant2(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.numBssids = int(n_input/3)\n",
    "\n",
    "        ## CNN layer definitions\n",
    "        self.conv1 = nn.Conv2d(1,16,(3,64),stride=2)\n",
    "        self.conv1_bn = nn.BatchNorm2d(16)\n",
    "        self.maxPool1 = nn.MaxPool2d(3, stride=2)  \n",
    "        self.conv2 = nn.Conv2d(1,16,(3,64),stride=2)\n",
    "        self.conv2_bn = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16,32,(3,64),stride=2)\n",
    "        self.conv3_bn = nn.BatchNorm2d(32)\n",
    "\n",
    "        ## calculate the fc layer input size\n",
    "        self.fc_inputSize = self.getFC_InputSize(self.numBssids, [16,16,32])\n",
    "        \n",
    "        ## dropout for regularisation\n",
    "        self.drops = nn.Dropout(p=0.3)\n",
    "        \n",
    "        ## fc layers definition\n",
    "        self.fc1 = nn.Linear(self.fc_inputSize, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(16, n_output)\n",
    "        \n",
    "        \n",
    "    def getConvOutputSize(self, h_in, w_in, kernel_height, kernel_width, paddSize, stride):\n",
    "        h_out = int((h_in + 2*paddSize - kernel_height) / stride) + 1\n",
    "        w_out = int((w_in + 2*paddSize - kernel_width)  / stride) + 1\n",
    "        return (h_out,w_out)        \n",
    "        \n",
    "    def getFC_InputSize(self, numFeatures, filtersList):\n",
    "        conv1_outputSize = self.getConvOutputSize(3,numFeatures,3,64,0,2)\n",
    "        inputHead_outputSize = self.getConvOutputSize(filtersList[0], conv1_outputSize[1], 3,3,0,2)\n",
    "        conv2_outputSize = self.getConvOutputSize(inputHead_outputSize[0], inputHead_outputSize[1], 3,64,0,2)\n",
    "        featureExtractor_outputSize = self.getConvOutputSize(conv2_outputSize[0], conv2_outputSize[1],3,63,0,2)\n",
    "        fc_inputSize = featureExtractor_outputSize[0] * featureExtractor_outputSize[1]\n",
    "        return fc_inputSize\n",
    "\n",
    "    def forward(self, x):\n",
    "        numBatches = x.shape[0]\n",
    "        x = x.view(numBatches,1,3,-1)        \n",
    "        \n",
    "        ## first conv layer\n",
    "        x = F.relu(self.conv1_bn(self.conv1(x)).view(numBatches, 1, 16, -1))\n",
    "        \n",
    "        ## inputHead + featureExtractor conv layers\n",
    "        x = self.maxPool1(x)\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = self.conv3_bn(self.conv3(x))\n",
    "\n",
    "        ## avg pooling across channels, then dropout on features \n",
    "        x = torch.mean(x,1)\n",
    "        x = x.view(numBatches,-1)\n",
    "        ## x = self.drops(x)\n",
    "        ## print(x.shape)\n",
    "        \n",
    "        \"\"\"\n",
    "        fc layer starts\n",
    "        ---------------\n",
    "        Conv/fc -> batchnorm -> relu activation -> dropout -> conv/fc\n",
    "        ---------------\n",
    "        batchnorm doesnt work for batchsize of 1\n",
    "        \"\"\"\n",
    "        if numBatches > 1:\n",
    "            x = F.relu(self.bn1(self.fc1(x)))\n",
    "            x = self.drops(x)\n",
    "            x = F.relu(self.bn2(self.fc2(x)))\n",
    "            x = self.fc3(x)\n",
    "        else:\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.drops(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-peoples",
   "metadata": {
    "papermill": {
     "duration": 0.039008,
     "end_time": "2021-05-02T08:27:24.901315",
     "exception": false,
     "start_time": "2021-05-02T08:27:24.862307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lr range finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-metadata",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:24.986390Z",
     "iopub.status.busy": "2021-05-02T08:27:24.985523Z",
     "iopub.status.idle": "2021-05-02T08:27:24.989511Z",
     "shell.execute_reply": "2021-05-02T08:27:24.988927Z"
    },
    "papermill": {
     "duration": 0.049473,
     "end_time": "2021-05-02T08:27:24.989647",
     "exception": false,
     "start_time": "2021-05-02T08:27:24.940174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_lr_finder_results(lr_finder): \n",
    "    # Create subplot grid\n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    # layout ={'title': 'Lr_finder_result'}\n",
    "    \n",
    "    # Create a line (trace) for the lr vs loss, gradient of loss\n",
    "    trace0 = go.Scatter(x=lr_finder['log_lr'], y=lr_finder['smooth_loss'],name='log_lr vs smooth_loss')\n",
    "    trace1 = go.Scatter(x=lr_finder['log_lr'], y=lr_finder['grad_loss'],name='log_lr vs loss gradient')\n",
    "\n",
    "    # Add subplot trace & assign to each grid\n",
    "    fig.add_trace(trace0, row=1, col=1);\n",
    "    fig.add_trace(trace1, row=1, col=2);\n",
    "    iplot(fig, show_link=False)\n",
    "    #fig.write_html(CFG.MODEL_NAME + '_lr_find.html');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-bedroom",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:25.083530Z",
     "iopub.status.busy": "2021-05-02T08:27:25.082577Z",
     "iopub.status.idle": "2021-05-02T08:27:25.086681Z",
     "shell.execute_reply": "2021-05-02T08:27:25.086097Z"
    },
    "papermill": {
     "duration": 0.058432,
     "end_time": "2021-05-02T08:27:25.086821",
     "exception": false,
     "start_time": "2021-05-02T08:27:25.028389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_lr(model, optimizer, data_loader, init_value = 1e-8, final_value=100.0, beta = 0.98, num_batches = 200):\n",
    "    assert(num_batches > 0)\n",
    "    mult = (final_value / init_value) ** (1/num_batches)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    batch_num = 0\n",
    "    avg_loss = 0.0\n",
    "    best_loss = 0.0\n",
    "    smooth_losses = []\n",
    "    raw_losses = []\n",
    "    log_lrs = []\n",
    "    dataloader_it = iter(data_loader)\n",
    "    progress_bar = tqdm(range(num_batches))                \n",
    "        \n",
    "    for idx in progress_bar:\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            _, inputs, targets = next(dataloader_it)\n",
    "            #print(images.shape)\n",
    "        except:\n",
    "            dataloader_it = iter(data_loader)\n",
    "            _, inputs, targets = next(dataloader_it)\n",
    "\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # handle exception in criterion\n",
    "        try:\n",
    "            # Forward pass\n",
    "            y_preds = model(inputs)\n",
    "            posLoss, floorLoss = criterion(y_preds, targets)\n",
    "            loss = posLoss + floorLoss\n",
    "        except:\n",
    "            if len(smooth_losses) > 1:\n",
    "                grad_loss = np.gradient(smooth_losses)\n",
    "            else:\n",
    "                grad_loss = 0.0\n",
    "            lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                                 'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "            return lr_finder_results \n",
    "                    \n",
    "        #Compute the smoothed loss\n",
    "        avg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        \n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 50 * best_loss:\n",
    "            if len(smooth_losses) > 1:\n",
    "                grad_loss = np.gradient(smooth_losses)\n",
    "            else:\n",
    "                grad_loss = 0.0\n",
    "            lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                                 'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "            return lr_finder_results\n",
    "        \n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num==1:\n",
    "            best_loss = smoothed_loss\n",
    "        \n",
    "        #Store the values\n",
    "        raw_losses.append(loss.item())\n",
    "        smooth_losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print info\n",
    "        progress_bar.set_description(f\"loss:{loss.item()},smoothLoss: {smoothed_loss},lr:{lr}\")\n",
    "\n",
    "        #Update the lr for the next step\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "    \n",
    "    grad_loss = np.gradient(smooth_losses)\n",
    "    lr_finder_results = {'log_lr':log_lrs, 'raw_loss':raw_losses, \n",
    "                         'smooth_loss':smooth_losses, 'grad_loss': grad_loss}\n",
    "    return lr_finder_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-encounter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:25.173870Z",
     "iopub.status.busy": "2021-05-02T08:27:25.172872Z",
     "iopub.status.idle": "2021-05-02T08:27:25.176794Z",
     "shell.execute_reply": "2021-05-02T08:27:25.176197Z"
    },
    "papermill": {
     "duration": 0.050482,
     "end_time": "2021-05-02T08:27:25.176927",
     "exception": false,
     "start_time": "2021-05-02T08:27:25.126445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.LR_FIND == True:\n",
    "    # create dataset instance\n",
    "    tempTs, tempX, tempY,_ = getBuildingData(buildingCsvPath=buildingsList[0])\n",
    "    tempX = stdScaler.fit_transform(tempX)\n",
    "    tempTrainDataset = wiFiFeaturesDataset(tempTs, tempX, tempY)\n",
    "    tempTrainDataloader = DataLoader(tempTrainDataset, batch_size= CFG.TRAIN_BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=CFG.NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "    \n",
    "    # create model instance   \n",
    "    model = wiFiFeaturesMLPModel(n_input=tempX.shape[1], n_output=3)\n",
    "    model.to(device);\n",
    "    \n",
    "    # optimizer function, lr schedulers and loss function\n",
    "    optimizer = getOptimizer(model)\n",
    "    lrFinderResults = find_lr(model, optimizer, tempTrainDataloader)\n",
    "    plot_lr_finder_results(lrFinderResults)\n",
    "    del tempX, tempY, tempTrainDataset, tempTrainDataloader, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-listening",
   "metadata": {
    "papermill": {
     "duration": 0.039875,
     "end_time": "2021-05-02T08:27:25.256008",
     "exception": false,
     "start_time": "2021-05-02T08:27:25.216133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train & Validate helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-worst",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:25.345628Z",
     "iopub.status.busy": "2021-05-02T08:27:25.344651Z",
     "iopub.status.idle": "2021-05-02T08:27:25.348672Z",
     "shell.execute_reply": "2021-05-02T08:27:25.348148Z"
    },
    "papermill": {
     "duration": 0.053134,
     "end_time": "2021-05-02T08:27:25.348809",
     "exception": false,
     "start_time": "2021-05-02T08:27:25.295675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validateModel(model, validationDataloader):\n",
    "    # placeholders to store output\n",
    "    val_ts = []\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    val_groups = []\n",
    "\n",
    "    # set model to Validate mode\n",
    "    model.eval()\n",
    "    dataLoaderIterator = iter(validationDataloader)\n",
    "\n",
    "    for idx in range(len(validationDataloader)):\n",
    "        try:\n",
    "            ts, inputs, targets, valGroups = next(dataLoaderIterator)\n",
    "        except StopIteration:\n",
    "            dataLoaderIterator = iter(validationDataloader)\n",
    "            ts, inputs, targets, valGroups = next(dataLoaderIterator)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device) \n",
    "\n",
    "        # forward prediction\n",
    "        with torch.no_grad():    \n",
    "            y_preds = model(inputs)\n",
    "\n",
    "        # store predictions and targets to compute metrics later\n",
    "        val_ts.append(ts)\n",
    "        val_preds.append(y_preds)\n",
    "        val_targets.append(targets)\n",
    "        val_groups.append(valGroups)\n",
    "\n",
    "    # concatenate to get as 1 2d array and find total loss  \n",
    "    val_preds = torch.cat(val_preds, 0)\n",
    "    val_targets = torch.cat(val_targets, 0)\n",
    "    valPosLoss, valFloorLoss = criterion(val_preds, val_targets)\n",
    "    valScore = valPosLoss + valFloorLoss\n",
    "\n",
    "    # np array concatenation\n",
    "    val_ts = np.concatenate(val_ts, axis=0)\n",
    "    val_groups = np.concatenate(val_groups, axis=0)\n",
    "    \n",
    "    # store results\n",
    "    validationResults = {'valPosLoss': valPosLoss.item() , 'valFloorLoss': valFloorLoss.item(),\\\n",
    "                         'val_ts': val_ts, 'val_groups': val_groups,\n",
    "                         'val_preds'  :val_preds.cpu().data.numpy(), \n",
    "                         'val_targets':val_targets.cpu().data.numpy(),\n",
    "                         }\n",
    "    return validationResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-greensboro",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:25.441351Z",
     "iopub.status.busy": "2021-05-02T08:27:25.440341Z",
     "iopub.status.idle": "2021-05-02T08:27:25.444350Z",
     "shell.execute_reply": "2021-05-02T08:27:25.443665Z"
    },
    "papermill": {
     "duration": 0.057102,
     "end_time": "2021-05-02T08:27:25.444510",
     "exception": false,
     "start_time": "2021-05-02T08:27:25.387408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainValidateOneFold(buildingName, i_fold, model, optimizer, scheduler, dataloader_train, dataloader_valid):\n",
    "    trainFoldResults = []\n",
    "    bestValScore = np.inf\n",
    "    bestEpoch = 0\n",
    "\n",
    "    for epoch in range(CFG.N_EPOCHS):\n",
    "        #print('Epoch {}/{}'.format(epoch + 1, CFG.N_EPOCHS))\n",
    "        model.train()\n",
    "        trainPosLoss = 0.0\n",
    "        trainFloorLoss = 0.0\n",
    "\n",
    "        # training iterator\n",
    "        tr_iterator = iter(dataloader_train)\n",
    "\n",
    "        for idx in range(len(dataloader_train)):\n",
    "            try:\n",
    "                _, inputs, targets, _ = next(tr_iterator)\n",
    "            except StopIteration:\n",
    "                tr_iterator = iter(dataloader_train)\n",
    "                _, inputs, targets, _ = next(tr_iterator)\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)  \n",
    "\n",
    "            # builtin package to handle automatic mixed precision\n",
    "            with autocast():\n",
    "                # Forward pass\n",
    "                y_preds = model(inputs)   \n",
    "                posLoss, floorLoss = criterion(y_preds, targets)\n",
    "                loss = posLoss + floorLoss\n",
    "\n",
    "                # Backward pass\n",
    "                scaler.scale(loss).backward()        \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad() \n",
    "\n",
    "                # log the necessary losses\n",
    "                trainPosLoss   += posLoss.item()\n",
    "                trainFloorLoss += floorLoss.item()\n",
    "\n",
    "                if scheduler is not None: \n",
    "                    if CFG.SCHEDULER == 'CosineAnnealingWarmRestarts':\n",
    "                        scheduler.step(epoch + idx / len(dataloader_train)) \n",
    "                    # onecyle lr scheduler / CosineAnnealingLR scheduler\n",
    "                    else:\n",
    "                        scheduler.step()\n",
    "                    \n",
    "        # Validate\n",
    "        foldValidationResults = validateModel(model, dataloader_valid)\n",
    "         \n",
    "        # store results\n",
    "        trainFoldResults.append({ 'fold': i_fold, 'epoch': epoch, \n",
    "                                  'trainPosLoss': trainPosLoss / len(dataloader_train), \n",
    "                                  'trainFloorLoss': trainFloorLoss / len(dataloader_train), \n",
    "                                  'valPosLoss'  : foldValidationResults['valPosLoss'] , \n",
    "                                  'valFloorLoss': foldValidationResults['valFloorLoss']})\n",
    "        \n",
    "        valScore = foldValidationResults['valPosLoss'] # + foldVal['valFloorLoss']\n",
    "        # save best models        \n",
    "        if(valScore < bestValScore):\n",
    "            # reset variables\n",
    "            bestValScore = valScore\n",
    "            bestEpoch = epoch\n",
    "\n",
    "            # save model weights\n",
    "            torch.save({'model': model.state_dict(), 'val_ts' : foldValidationResults['val_ts'], \n",
    "                        'val_preds':foldValidationResults['val_preds'], \n",
    "                        'val_targets':foldValidationResults['val_targets'],\n",
    "                        'val_groups' : foldValidationResults['val_groups']}, \n",
    "                        f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_fold{i_fold}_best.pth\")\n",
    "\n",
    "    print(f\"For Fold {i_fold}, Best position validation score of {bestValScore} was got at epoch {bestEpoch}\") \n",
    "    return trainFoldResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-hardwood",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:25.538180Z",
     "iopub.status.busy": "2021-05-02T08:27:25.537232Z",
     "iopub.status.idle": "2021-05-02T08:27:25.541356Z",
     "shell.execute_reply": "2021-05-02T08:27:25.540766Z"
    },
    "papermill": {
     "duration": 0.056464,
     "end_time": "2021-05-02T08:27:25.541530",
     "exception": false,
     "start_time": "2021-05-02T08:27:25.485066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainValidateOneBuilding(buildingDataPath, modelToFit):\n",
    "    # placeholder to store results\n",
    "    buildingTrainResults = []\n",
    "    \n",
    "    buildingName = getBuildingName(buildingDataPath)\n",
    "    print(f\"Processing data for building - {buildingName}\")\n",
    "    timestamps, X, y, groups = getBuildingData(buildingDataPath)\n",
    "    print(f\"Building Data shapes : {timestamps.shape, X.shape, y.shape, groups.shape}\")\n",
    "\n",
    "    for i_fold, (trainIndex, validIndex) in enumerate(folds.split(X=X, y=y[:,0],groups=groups)):\n",
    "        if i_fold in CFG.FOLD_TO_TRAIN:\n",
    "            ## print(\"Fold {}/{}\".format(i_fold + 1, CFG.N_FOLDS))\n",
    "            \n",
    "            # splitting into train and validataion sets\n",
    "            trainTimeStamps, X_train, y_train, trainGroups = timestamps[trainIndex], X[trainIndex], y[trainIndex], groups[trainIndex]\n",
    "            validTimeStamps, X_valid, y_valid, validGroups = timestamps[validIndex], X[validIndex], y[validIndex], groups[validIndex] \n",
    "                        \n",
    "            # create torch Datasets and Dataloader for each fold's train and validation data\n",
    "            dataset_train = wiFiFeaturesDataset(trainTimeStamps, X_train, y_train, trainGroups)\n",
    "            dataset_valid = wiFiFeaturesDataset(validTimeStamps, X_valid, y_valid, validGroups)            \n",
    "            dataloader_train = getDataLoader(dataset_train, datasetType= 'train')\n",
    "            dataloader_valid = getDataLoader(dataset_valid, datasetType= 'valid')\n",
    "            \n",
    "            # supervised model instance and move to compute device\n",
    "            model = modelToFit(n_input=X.shape[1], n_output=3)\n",
    "            model.to(device);\n",
    "            ### print(f\"there are {find_no_of_trainable_params(model)} params in model\")\n",
    "\n",
    "            # optimizer function, lr schedulers and loss function\n",
    "            optimizer = getOptimizer(model)\n",
    "            scheduler = getScheduler(optimizer, dataloader_train)\n",
    "            # print(f\"optimizer={optimizer}, scheduler={scheduler}, loss_fn={criterion}\")\n",
    "\n",
    "            # train and validate single fold\n",
    "            foldResults = trainValidateOneFold(buildingName, i_fold, model, optimizer, scheduler,dataloader_train, dataloader_valid)\n",
    "            buildingTrainResults = buildingTrainResults + foldResults\n",
    "            \n",
    "            del trainTimeStamps, X_train, y_train, trainGroups\n",
    "            del validTimeStamps, X_valid, y_valid, validGroups\n",
    "            del dataloader_train, dataloader_valid, model, optimizer, scheduler\n",
    "            gc.collect()\n",
    "    \n",
    "    del timestamps, X, y, groups\n",
    "    gc.collect()\n",
    "    \n",
    "    buildingTrainResults = pd.DataFrame(buildingTrainResults)\n",
    "    buildingTrainResults['valTotalLoss'] = buildingTrainResults['valPosLoss'] + buildingTrainResults['valFloorLoss']\n",
    "    buildingTrainResults['trainTotalLoss'] = buildingTrainResults['trainPosLoss'] + buildingTrainResults['trainFloorLoss']\n",
    "    return buildingTrainResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-health",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:25.660228Z",
     "iopub.status.busy": "2021-05-02T08:27:25.658909Z",
     "iopub.status.idle": "2021-05-02T08:27:25.676714Z",
     "shell.execute_reply": "2021-05-02T08:27:25.674730Z"
    },
    "papermill": {
     "duration": 0.096554,
     "end_time": "2021-05-02T08:27:25.677282",
     "exception": false,
     "start_time": "2021-05-02T08:27:25.580728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getFoldBestResultsDf(trainResults):\n",
    "    bestResults = []\n",
    "    numFolds = trainResults['fold'].nunique()\n",
    "    \n",
    "    for fold in range(numFolds):\n",
    "        foldDf = trainResults[trainResults['fold']== fold]\n",
    "        bestResults.append(foldDf.iloc[np.argmin(foldDf['valTotalLoss'].values),:])\n",
    "    \n",
    "    bestResults =pd.DataFrame(bestResults)\n",
    "    valPosLossBest = bestResults['valPosLoss'].values\n",
    "    print(f\"Best valPosLoss for all folds = {valPosLossBest}\")\n",
    "    print(f\"Mean, std ={valPosLossBest.mean()}, {valPosLossBest.std()}\")\n",
    "    return bestResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-freeze",
   "metadata": {
    "papermill": {
     "duration": 0.072485,
     "end_time": "2021-05-02T08:27:25.835367",
     "exception": false,
     "start_time": "2021-05-02T08:27:25.762882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate OOF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-offense",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:25.983438Z",
     "iopub.status.busy": "2021-05-02T08:27:25.982356Z",
     "iopub.status.idle": "2021-05-02T08:27:25.992795Z",
     "shell.execute_reply": "2021-05-02T08:27:25.993438Z"
    },
    "papermill": {
     "duration": 0.089299,
     "end_time": "2021-05-02T08:27:25.993736",
     "exception": false,
     "start_time": "2021-05-02T08:27:25.904437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateOOF(modelSaveDir:str, buildingName:str, modelName:str):\n",
    "    oof_ts = []\n",
    "    oof_preds = []\n",
    "    oof_targets = []\n",
    "    oof_groups = []\n",
    "    oof_folds = []\n",
    "\n",
    "    modelPaths = sorted(glob.glob(f\"{modelSaveDir}/{buildingName}_{modelName}_fold*.pth\"))\n",
    "\n",
    "    for fold in range(len(modelPaths)):\n",
    "        # load building-model-fold checkpoint\n",
    "        checkPoint = torch.load(modelPaths[fold], map_location=torch.device(device))\n",
    "        numRows = len(checkPoint['val_ts'])\n",
    "\n",
    "        oof_ts.append(checkPoint['val_ts'])\n",
    "        oof_preds.append(checkPoint['val_preds'])\n",
    "        oof_targets.append(checkPoint['val_targets'])\n",
    "        oof_groups.append(checkPoint['val_groups'])\n",
    "        oof_folds.append([fold] * numRows)\n",
    "    \n",
    "    oof_ts = np.concatenate(oof_ts,axis=0)\n",
    "    oof_preds = np.concatenate(oof_preds,axis=0)\n",
    "    oof_targets = np.concatenate(oof_targets,axis=0)\n",
    "    oof_groups = np.concatenate(oof_groups,axis=0)\n",
    "    oof_folds = np.concatenate(oof_folds,axis=0)\n",
    "    \n",
    "    #print(oof_ts.shape, oof_preds.shape, oof_targets.shape, oof_groups.shape, oof_folds.shape)\n",
    "    oof_df = pd.DataFrame({'timestamp' : oof_ts, 'x_preds': oof_preds[:,0], 'y_preds': oof_preds[:,1],\n",
    "                       'floor_preds': oof_preds[:,2], 'x_tgt': oof_targets[:,0], 'y_tgt': oof_targets[:,1],\n",
    "                       'floor_tgt': oof_targets[:,2], 'path' : oof_groups, 'fold' : oof_folds\n",
    "                      })\n",
    "    print(f\"OOF prediction for {buildingName} site generated\")\n",
    "    return oof_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-dylan",
   "metadata": {
    "papermill": {
     "duration": 0.066146,
     "end_time": "2021-05-02T08:27:26.127091",
     "exception": false,
     "start_time": "2021-05-02T08:27:26.060945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test set prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-strengthening",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:26.283402Z",
     "iopub.status.busy": "2021-05-02T08:27:26.282708Z",
     "iopub.status.idle": "2021-05-02T08:27:26.307738Z",
     "shell.execute_reply": "2021-05-02T08:27:26.308586Z"
    },
    "papermill": {
     "duration": 0.113737,
     "end_time": "2021-05-02T08:27:26.308859",
     "exception": false,
     "start_time": "2021-05-02T08:27:26.195122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateWiFiSubmission(modelToFit, modelSaveDir:str, buildingName:str, modelName:str):\n",
    "    modelPaths = sorted(glob.glob(f\"{modelSaveDir}/{buildingName}_{modelName}_fold*.pth\"))\n",
    "    buildingTestData = f\"{wifiFeaturesDir_test}/{buildingName}_test.pickle\"\n",
    "    with open(buildingTestData, 'rb') as inputFile:\n",
    "        testData = pickle.load(inputFile)    \n",
    "    \n",
    "    test_ts = []\n",
    "    test_fold = []\n",
    "    test_preds = []\n",
    "    test_groups = []\n",
    "\n",
    "    for fold in range(CFG.N_FOLDS):\n",
    "        ## print(f\"Fold {fold} processing\")\n",
    "        #print(f\"Before stdscaler : testX mean = {testX.mean()}, testData std = {testX.std()}\")\n",
    "        testGroups = testData.iloc[:,-1].values    \n",
    "        testTimestamps = testData.iloc[:,0].values        \n",
    "        testX = getBuildingFeatures(testData, 'test')  \n",
    "        \n",
    "        checkPoint = torch.load(modelPaths[fold], map_location=torch.device(device))\n",
    "        model = modelToFit(n_input=testX.shape[1], n_output=3)\n",
    "        model.to(device);\n",
    "        model.load_state_dict(checkPoint['model'])\n",
    "\n",
    "        # set model to Validate mode\n",
    "        model.eval()\n",
    "        ## test Dataset and data loaders\n",
    "        testDataset = wiFiFeaturesDataset_test(testTimestamps, testX, testGroups)\n",
    "        testDataloader = getDataLoader(testDataset, datasetType= 'test')\n",
    "\n",
    "        dataLoaderIterator = iter(testDataloader)\n",
    "        for idx in range(len(testDataloader)):\n",
    "            try:\n",
    "                ts, inputs, testGroups = next(dataLoaderIterator)\n",
    "            except StopIteration:\n",
    "                dataLoaderIterator = iter(testDataloader)\n",
    "                ts, inputs, testGroups = next(dataLoaderIterator)\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            # forward prediction\n",
    "            with torch.no_grad():    \n",
    "                y_preds = model(inputs)\n",
    "\n",
    "            # store predictions and targets to compute metrics later\n",
    "            test_ts.append(ts)\n",
    "            test_preds.append(y_preds)\n",
    "            test_groups.append(testGroups)\n",
    "        \n",
    "        test_fold.append([fold] * len(testX))\n",
    "        del testDataloader\n",
    "        ## torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    # concatenate to get as 1 2d array \n",
    "    test_preds = torch.cat(test_preds, 0).cpu().data.numpy() \n",
    "    test_ts = np.concatenate(test_ts, axis=0)\n",
    "    test_fold = np.concatenate(test_fold, axis=0)\n",
    "    test_groups = np.concatenate(test_groups, axis=0)\n",
    "    subm_wifi_df = pd.DataFrame({'timestamp' : test_ts, 'x_preds': test_preds[:,0], 'y_preds': test_preds[:,1],\n",
    "                                 'floor_preds': test_preds[:,2], 'path' : test_groups, 'fold' : test_fold})\n",
    "    subm_wifi_df.to_pickle(f\"{buildingName}_wifi_subm.pickle\")  \n",
    "    print(f\"Test data prediction for {buildingName} site generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-entrepreneur",
   "metadata": {
    "papermill": {
     "duration": 0.063076,
     "end_time": "2021-05-02T08:27:26.451325",
     "exception": false,
     "start_time": "2021-05-02T08:27:26.388249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compute Device as CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-finnish",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:26.716020Z",
     "iopub.status.busy": "2021-05-02T08:27:26.715177Z",
     "iopub.status.idle": "2021-05-02T08:27:26.723222Z",
     "shell.execute_reply": "2021-05-02T08:27:26.724470Z"
    },
    "papermill": {
     "duration": 0.211543,
     "end_time": "2021-05-02T08:27:26.724754",
     "exception": false,
     "start_time": "2021-05-02T08:27:26.513211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Device as cpu or tpu\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-frank",
   "metadata": {
    "papermill": {
     "duration": 0.040782,
     "end_time": "2021-05-02T08:27:26.807120",
     "exception": false,
     "start_time": "2021-05-02T08:27:26.766338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-turkey",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:26.897498Z",
     "iopub.status.busy": "2021-05-02T08:27:26.896496Z",
     "iopub.status.idle": "2021-05-02T08:27:26.900412Z",
     "shell.execute_reply": "2021-05-02T08:27:26.899866Z"
    },
    "papermill": {
     "duration": 0.051783,
     "end_time": "2021-05-02T08:27:26.900609",
     "exception": false,
     "start_time": "2021-05-02T08:27:26.848826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for cv\n",
    "folds = GroupKFold(n_splits=CFG.N_FOLDS)\n",
    "\n",
    "# scaler to handle AMP\n",
    "scaler = GradScaler()   \n",
    "\n",
    "criterion = competitionMetric\n",
    "modelToFit = CNNWiFiFeaturesModel   ## CNNWiFiFeaturesModel_variant2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-flower",
   "metadata": {
    "papermill": {
     "duration": 0.040927,
     "end_time": "2021-05-02T08:27:26.982969",
     "exception": false,
     "start_time": "2021-05-02T08:27:26.942042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training & Validation main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-olive",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-02T08:27:27.080440Z",
     "iopub.status.busy": "2021-05-02T08:27:27.079764Z",
     "iopub.status.idle": "2021-05-02T08:41:56.517640Z",
     "shell.execute_reply": "2021-05-02T08:41:56.516899Z"
    },
    "papermill": {
     "duration": 869.490987,
     "end_time": "2021-05-02T08:41:56.517824",
     "exception": false,
     "start_time": "2021-05-02T08:27:27.026837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if CFG.TRAIN == True:\n",
    "    \n",
    "    buildingPathList_train = sorted(glob.glob(f\"{wifiFeaturesDir_train}/*.pickle\"))\n",
    "    buildingPathList_train = buildingPathList_train[CFG.BUILDING_SITES_RANGE[0]: CFG.BUILDING_SITES_RANGE[1]]\n",
    "    print(f\"{len(buildingPathList_train)} sites are to be trained\")\n",
    "    ## print(buildingPathList_train)\n",
    "\n",
    "    for buildingPath_train in buildingPathList_train:\n",
    "        \n",
    "        print('----------------------------------')\n",
    "        ## get building name\n",
    "        buildingName = getBuildingName(buildingPath_train)\n",
    "        \n",
    "        ## train and validate for building data\n",
    "        buildingTrainResults = trainValidateOneBuilding(buildingPath_train, modelToFit)\n",
    "        bestResults = getFoldBestResultsDf(buildingTrainResults)\n",
    "        \n",
    "        ## generate OOF prediction for building-model combination\n",
    "        buildingOOF = generateOOF(modelOutputDir, buildingName, CFG.MODEL_NAME)\n",
    "        \n",
    "        ## prediction for test data too\n",
    "        generateWiFiSubmission(modelToFit, modelOutputDir, buildingName, CFG.MODEL_NAME)\n",
    "\n",
    "        ## save results to file\n",
    "        buildingOOF.to_pickle(f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_OOF.pickle\")\n",
    "        bestResults.to_pickle(f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_bestResults.pickle\")\n",
    "        buildingTrainResults.to_pickle(f\"{modelOutputDir}/{buildingName}_{CFG.MODEL_NAME}_trainResults.pickle\")\n",
    "        \n",
    "        ## plot building results\n",
    "        plotTrainingResults(buildingTrainResults, buildingName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-entrepreneur",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.046591,
     "end_time": "2021-05-02T08:41:56.612271",
     "exception": false,
     "start_time": "2021-05-02T08:41:56.565680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "buildingPathList_train = sorted(glob.glob(f\"{wifiFeaturesDir_train}/*.pickle\"))\n",
    "buildingPathList_test = sorted(glob.glob(f\"{wifiFeaturesDir_test}/*.pickle\"))\n",
    "\n",
    "for idx in range(len(buildingPathList_train)):\n",
    "    print('-----------------------------')\n",
    "    trainFilePath = buildingPathList_train[idx]\n",
    "    testFilePath  = buildingPathList_test[idx]\n",
    "    print(f\"{idx}. {getBuildingName(trainFilePath)}\")\n",
    "    with open(trainFilePath, 'rb') as inputTrainFile:\n",
    "        trainData = pickle.load(inputTrainFile)\n",
    "    with open(testFilePath, 'rb') as inputTestFile:\n",
    "        testData = pickle.load(inputTestFile)\n",
    "    \n",
    "    with pd.option_context('display.max_rows', 1, 'display.max_columns', 12,\n",
    "                           'display.width', 500, 'display.precision', 3,\n",
    "                           'display.colheader_justify', 'left'):\n",
    "        display(trainData)\n",
    "        display(testData)\n",
    "```        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-flight",
   "metadata": {
    "papermill": {
     "duration": 0.047473,
     "end_time": "2021-05-02T08:41:56.708077",
     "exception": false,
     "start_time": "2021-05-02T08:41:56.660604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-weight",
   "metadata": {
    "papermill": {
     "duration": 0.046332,
     "end_time": "2021-05-02T08:41:56.801708",
     "exception": false,
     "start_time": "2021-05-02T08:41:56.755376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 905.227467,
   "end_time": "2021-05-02T08:41:59.222068",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-02T08:26:53.994601",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
